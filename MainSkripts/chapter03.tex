\documentclass[11pt,a4paper,twoside]{book}
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R


\input{header.sty}   % packages, layout and standard macros



\begin{document}
% LaTeX file for Chapter 03






\chapter{Examples}

The following chapter presents the Bayesian LMG implementation on two examples. Simulated data is used for the first example. Empirical data is used for the second example.

\section{Simulated Data}

We assume a simple model for the first example: 

\begin{align*} 
&Y_{i} \sim \mathcal{N}(\beta_{0}+x_{1} \beta_{1}+x_{2} \beta_{2}+x_{3} \beta_{3}+x_{4} \beta_{4}, \, \sigma^2),& \\ & \beta_{1} = 0.5, , \beta_{2} = 1, \, \beta_{3} = 2 , \, \beta_{4}=0, \, \sigma^2 = 1 & \\ & \X_{1}, \X_{2},\X_{3},\X_{4} \sim \mathcal{N}(0, 1) 
\end{align*} 

The values of the four predictors are sampled from a standard normal distribution. These values are then multiplied by the regression coefficients. A standard normal distributed error is added to obtain the dependent variable. Fifty observations were sampled.

The following Code was used to simulate the data :

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x1} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{); x2} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{x3} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{); x4} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{b1} \hlkwb{<-} \hlnum{0.5}\hlstd{; b2} \hlkwb{<-} \hlnum{1}\hlstd{; b3} \hlkwb{<-} \hlnum{2}\hlstd{; b4} \hlkwb{<-} \hlnum{0.8}
\hlcom{#b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1}

\hlstd{y} \hlkwb{<-} \hlstd{b1}\hlopt{*}\hlstd{x1} \hlopt{+} \hlstd{x2}\hlopt{*}\hlstd{b2} \hlopt{+} \hlstd{b3}\hlopt{*}\hlstd{x3} \hlopt{+} \hlstd{b4}\hlopt{*}\hlstd{x4} \hlopt{+} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}

\hlstd{df} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{y} \hlstd{= y,} \hlkwc{x1} \hlstd{= x1,} \hlkwc{x2} \hlstd{= x2,} \hlkwc{x3} \hlstd{= x3,} \hlkwc{x4} \hlstd{= x4)}
\end{alltt}
\end{kframe}
\end{knitrout}


The model is fitted using the \texttt{rstanarm} package \citep{rstanarm} with the default priors for the regression and $\sigma^2$ parameters. These default priors are called weakly informative priors, because they take into account the order of magnitude of the variables by using variance of the observed data. Information about these priors can be found in \cite{stanM2017}. A burn-in period of 1000, a sample size of 20000 and a thinning of 20 were chosen, resulting in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:simdata.postsample2}. 

For each posterior sample of the parameters the $\Rtwo$ value is calculated. The $\Rtwo$ of the submodels is then calculated by the conditional variance formula for each posterior sample. The resulting $\Rtwo$ values of the first few posterior samples is shown in Table~\ref{tab:simdata.postsample3}.  The thinning makes sense in this case to reduce the computational burden and still obtain an appropriate posterior of the $\Rtwo$ values \citep{Link2012}. 

The \texttt{hier.part} package is used to calculate the LMG value for each posterior sample. The LMG posteriors are shown in Table\ref{tbl:nonstochEx1} . The independent component (I) represents the LMG value. The joint contribution (J) represents the difference from the independent component to the explained variance of the predictor-only model (T). Assuming stochastic or non-stochastic regressors has an influence on the uncertainty. First, non-stochastic regressors are assumed. The resulting LMG values and joint contributions with a 95\% credible interval are shown in Table~\ref{tbl:nonstochEx1}. An option to display the resulting LMG distribution is shown in Figure ~ \ref{fig:simdata.LMG.plot}.  Using the default weakly informative priors, the LMG distributions obtained from the Bayesian framework are very similar to the bootstrap confidence intervals assuming non-stochastic predictors of the LMG estimates obtained from the \texttt{relaimpo} package, as shown in Table~\ref{tbl:nonstochEx1relamip}. 





\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{table}

\caption{\label{tab:simdata.postsample2}Samples from the posterior distributions of the regression parameters}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
  & x1 & x2 & x3 & x4 & sigma\\
\midrule
sample 1 & 0.555 & 1.023 & 1.99 & 0.835 & 1.103\\
sample 2 & 0.283 & 1.153 & 2.19 & 0.679 & 1.143\\
sample 3 & 0.571 & 1.131 & 2.03 & 0.877 & 1.026\\
sample 4 & 0.455 & 1.231 & 2.00 & 0.925 & 1.266\\
sample 5 & 0.600 & 0.908 & 1.90 & 0.700 & 0.991\\
sample 6 & 0.730 & 1.018 & 2.03 & 0.401 & 1.266\\
sample 7 & 0.363 & 1.029 & 1.84 & 0.619 & 1.145\\
sample 8 & 0.296 & 0.805 & 1.77 & 0.599 & 1.290\\
sample 9 & 0.177 & 0.812 & 1.61 & 0.376 & 1.406\\
sample 10 & 0.694 & 1.018 & 1.72 & 1.028 & 1.192\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{table}

\caption{\label{tab:simdata.postsample3}$ \Rtwo$ for all submodels for the first six posterior samples}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & sample 1 & sample 2 & sample 3 & sample 4 & sample 5 & sample 6\\
\midrule
none & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
x1 & 0.055 & 0.023 & 0.057 & 0.041 & 0.067 & 0.081\\
x2 & 0.055 & 0.059 & 0.069 & 0.080 & 0.048 & 0.055\\
x3 & 0.639 & 0.687 & 0.640 & 0.580 & 0.667 & 0.637\\
x4 & 0.068 & 0.040 & 0.072 & 0.076 & 0.053 & 0.014\\
x1 x2 & 0.099 & 0.075 & 0.114 & 0.109 & 0.104 & 0.122\\
x1 x3 & 0.681 & 0.702 & 0.684 & 0.610 & 0.719 & 0.702\\
x1 x4 & 0.125 & 0.064 & 0.131 & 0.118 & 0.123 & 0.096\\
x2 x3 & 0.750 & 0.806 & 0.770 & 0.722 & 0.769 & 0.747\\
x2 x4 & 0.126 & 0.102 & 0.144 & 0.159 & 0.104 & 0.071\\
x3 x4 & 0.705 & 0.725 & 0.709 & 0.653 & 0.718 & 0.650\\
x1 x2 x3 & 0.778 & 0.812 & 0.798 & 0.738 & 0.805 & 0.794\\
x1 x2 x4 & 0.171 & 0.118 & 0.190 & 0.190 & 0.161 & 0.139\\
x1 x3 x4 & 0.748 & 0.741 & 0.755 & 0.685 & 0.772 & 0.716\\
x2 x3 x4 & 0.819 & 0.847 & 0.843 & 0.799 & 0.823 & 0.762\\
all & 0.848 & 0.854 & 0.873 & 0.817 & 0.861 & 0.809\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}





\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}
\includegraphics[width=\maxwidth]{figure/ch03_figsimdata_LMG_plot-1} \caption[Posterior distribution of LMG values]{Posterior distribution of LMG values.}\label{fig:simdata.LMG.plot}
\end{figure}


\end{knitrout}

\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.039 (0.007, 0.094)  & 0.012 (0.004, 0.019)   & 0.051 (0.012, 0.113)  \\ 
x2 & 0.083 (0.03, 0.164)  & -0.022 (-0.032, -0.013)   & 0.06 (0.015, 0.133)  \\ 
x3 & 0.618 (0.497, 0.714)  & -0.02 (-0.031, -0.008)   & 0.598 (0.472, 0.697)  \\ 
x4 & 0.064 (0.014, 0.134)  & -0.001 (-0.002, \ensuremath{-3.301\times 10^{-4}})   & 0.063 (0.014, 0.133)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1}
\end{table}




\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
x1 & 0.092 (0.046, 0.157)  & 0.039 (0.007, 0.094)  \\ 
x2 & 0.109 (0.062, 0.177)  & 0.083 (0.03, 0.164)   \\ 
x3 & 0.636 (0.548, 0.723)  & 0.618 (0.497, 0.714)  \\ 
x4 & 0.03 (0.007, 0.072) & 0.064 (0.014, 0.134)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relamip}
\end{table}

\FloatBarrier


In this example we know that the predictor values were sampled from a normal distribution. It would therefore be more reasonable to assume stochastic predictors. Under the assumption of weak exogeinity and conditional independence, the posterior distributions of the regression parameters $\bbeta$ are valid for non-stochastic and stochastic predictors. However, the uncertainty about the LMG values needs to include the uncertainty about the covariance matrix. If we know the distribution of the predictors we can incorporate this information and obtain the posterior distribution of the covariance matrix. The package \texttt{Jags} was used for inference about the covariance matrix in a Bayesian way. As an alternative non-parametric bootstrap was used for inference about the covariance matrix. Both approaches resulted in very similar LMG values. Using the default priors further resulted in very similar LMG values as the non-parametric bootstrap option of the relaimpo package. Table~\ref{tbl:nonstochEx1relaimpstoch} shows the LMG values of these approaches. Using the bootstrap samples of the covariance matrix or samples from the posterior covariance matrix resulted in very similar LMG values. Bootstrap seems to be a valuable option for stochastic predictors when the distribution of the predictors is unknown. Even when the distribution is known, the difference seems to be tiny. A benefit of going the full Bayesian way is that we can also include prior knowledge about the covariance matrix. For stochastic predictors, compared to non-stochastic predictors, the uncertainty about the covariance matrix is reflected in the larger credible intervals . Even when we would knew the exact regression parameters, there is a lot of uncertainty in the LMG values caused by the uncertainty about the covariance matrix. 








\begin{table}[h]
\caption{LMG values assuming stochastic predictors with 95\% CI.}
\centering
\begin{tabular}{clll}
   \toprule
  & \multicolumn{1}{c}{\textbf{Relaimpo}} & \multicolumn{2}{c}{\textbf{Bayesian framework}} \\ \cmidrule(r){2-2}\cmidrule(l){3-4}
 \textbf{Variable} &  & \multicolumn{1}{c}{nonparameteric bootstrap}& \multicolumn{1}{c}{covariance inference} \\
 \midrule
x1 & 0.092 (0.02, 0.221)  & 0.047 (0.005, 0.144) &  0.078 (0.018, 0.17) \\ 
x2 & 0.109 (0.026, 0.241)  & 0.15 (0.055, 0.279)  & 0.156 (0.052, 0.273) \\ 
x3 & 0.636 (0.452, 0.757)  & 0.554 (0.395, 0.71)  & 0.561 (0.442, 0.66) \\ 
x4 & 0.03 (0.008, 0.11) & 0.034 (0.008, 0.097) & 0.033 (0.011, 0.103) \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relaimpstoch}
\end{table}







\clearpage

\section{Empirical Data}
In the following section, the Bayesian LMG implementation is applied on an empirical dataset containing test scores of pupils (N = 301) from a study by \cite{Holzinger1939} available in the R package \texttt{MBESS} \citep{MBESS}. This dataset was used in \cite{Nimon2008} to present commonality analysis, which is another variance decomposition technique. Scores from a paragraph comprehension test (paragrap) was predicted by four verbal tests:  general-information (general),  sentence-comprehension (sentence) ,  word-classification (wordc), and  word-meaning (wordm) (Table~\ref{table:hs.data}). 

The aim of the regression analysis  was to determine the strength of association between verbal ability and paragraph comprehension.  
An overview of the data is shown in Figure~\ref{fig:empi.lmg.plot}. The regression results of the paragraph comprehension scores on the verbal test scores are shown Table~\ref{tbl:empirical.reg}). A novice researcher may wrongly conclude, that there is little association between the \textit{non-significant} predictors (general information and word classification) and paragraph comprehension. Given the other predictors are already included in the model, the predictors seem not to provide us much information about the expected paragraph comprehension ability. However, we can not conclude out of this regression table, that the association between any of these \textit{non-significant} predictors and the dependent variable is unimportant. We see in Figure~\ref{fig:empi.lmg.plot} that the correlations between the predictors is rather high. The LMG metric may therefore provide us some new information about the importance of each predictor. 

The Bayesian regression model was fitted in \texttt{rstanarm}. The default priors were used for the regression coefficients and the $\sigma^2$ parameter. A burn-in period of 20000, a sample size of 20000 and a thinning of 20 resulted in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:empirical.data.postsample2}. The resulting $\Rtwo$ of of these posterior samples is shown in Table~\ref{tab:empirical.data.postsample3}. The LMG values were calculated by using \texttt{hier.part}. The  independent component (I), joint contribution (J) and total explained variance in a predictor-only model (T) are shown in Table~\ref{tbl:empirical.ijt}. Sentence-comprehension and word-meaning seem to be the most important predictors by the LMG metric. However, none of the predictors seem to be unimportant.  The joint contributions of each predictor is quite large.

For comparison purposes, the LMG metric was additionally calculated with the \texttt{relaimpo} package using parametric bootstrapping. The confidence intervals of  \texttt{relaimpo} are almost identical to the credible intervals of the Bayesian framework (Table~\ref{tbl:empirical.relaimp.comp}). Assuming stochastic or non-stochastic predictors would also result in almost identical uncertainty estimates with such a large sample size (Table\ref{tbl:empirical.relaimp.comp.stoch}). 







\begin{table}
\centering
\caption{Variable description}
\begin{tabular}{l l}
  \toprule			
  Variable Name & Description  \\   \midrule  
  paragrap & scores on paragraph comprehension test  \\
  general & scores on general information test \\
  sentence & scores on sentence completion test\\
  wordc & scores on word classification test \\
  wordm & scores on word meaning test \\
  \bottomrule  
\end{tabular}
\label{table:hs.data}
\end{table}

\begin{figure}
\includegraphics[width=\maxwidth]{figure/ch03_figunnamed-chunk-1-1} \caption[Test scores from Holzinger and Swineford's  (1939) Study]{Test scores from Holzinger and Swineford's  (1939) Study. N=301}\label{fig:unnamed-chunk-1}
\end{figure}






% latex table generated in R 3.4.3 by xtable 1.8-2 package
% Mon Aug  6 18:29:06 2018
\begin{table}[!h]
\centering
\caption{Regression of paragraph comprehension on verbal tests.} 
\label{tbl:empirical.reg}
\begingroup\footnotesize
\begin{tabular}{rrrr}
  \hline
 & Coefficient & 95\%-confidence interval & $p$-value \\ 
  \hline
Intercept & 0.071 & from -1.17 to 1.31 & 0.91 \\ 
  general & 0.03 & from -0.00 to 0.06 & 0.084 \\ 
  sentence & 0.26 & from 0.18 to 0.34 & $<$ 0.0001 \\ 
  wordc & 0.047 & from -0.01 to 0.11 & 0.14 \\ 
  wordm & 0.14 & from 0.08 to 0.19 & $<$ 0.0001 \\ 
   \hline
\end{tabular}
\endgroup
\end{table}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{table}

\caption{\label{tab:empirical.data.postsample2}Samples from the posterior distributions of the regression parameters}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
  & general & sentence & wordc & wordm & sigma\\
\midrule
sample 1 & 0.041 & 0.270 & 0.014 & 0.113 & 2.18\\
sample 2 & 0.036 & 0.243 & 0.010 & 0.166 & 2.09\\
sample 3 & 0.043 & 0.295 & -0.007 & 0.103 & 2.28\\
sample 4 & 0.024 & 0.240 & 0.051 & 0.192 & 2.19\\
sample 5 & 0.050 & 0.297 & 0.009 & 0.099 & 2.28\\
sample 6 & 0.027 & 0.259 & 0.040 & 0.157 & 2.09\\
sample 7 & 0.029 & 0.265 & 0.023 & 0.156 & 2.11\\
sample 8 & 0.017 & 0.259 & 0.026 & 0.168 & 2.18\\
sample 9 & 0.036 & 0.250 & 0.073 & 0.123 & 2.00\\
sample 10 & 0.004 & 0.269 & 0.059 & 0.163 & 2.10\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{table}

\caption{\label{tab:empirical.data.postsample3}$ \Rtwo$ for all submodels for the first six posterior samples}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & sample 1 & sample 2 & sample 3 & sample 4 & sample 5 & sample 6\\
\midrule
none & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
general & 0.426 & 0.455 & 0.407 & 0.449 & 0.433 & 0.448\\
sentence & 0.520 & 0.537 & 0.508 & 0.541 & 0.527 & 0.554\\
wordc & 0.299 & 0.312 & 0.271 & 0.348 & 0.298 & 0.343\\
wordm & 0.462 & 0.542 & 0.433 & 0.559 & 0.446 & 0.534\\
general sentence & 0.558 & 0.583 & 0.541 & 0.583 & 0.566 & 0.592\\
general wordc & 0.455 & 0.482 & 0.428 & 0.493 & 0.459 & 0.490\\
general wordm & 0.511 & 0.578 & 0.482 & 0.588 & 0.504 & 0.569\\
sentence wordc & 0.527 & 0.545 & 0.511 & 0.557 & 0.533 & 0.567\\
sentence wordm & 0.574 & 0.627 & 0.551 & 0.640 & 0.571 & 0.633\\
wordc wordm & 0.497 & 0.568 & 0.462 & 0.595 & 0.484 & 0.573\\
general sentence wordc & 0.559 & 0.583 & 0.541 & 0.587 & 0.566 & 0.595\\
general sentence wordm & 0.583 & 0.634 & 0.560 & 0.644 & 0.583 & 0.638\\
general wordc wordm & 0.523 & 0.587 & 0.490 & 0.606 & 0.516 & 0.588\\
sentence wordc wordm & 0.576 & 0.628 & 0.552 & 0.645 & 0.572 & 0.636\\
all & 0.584 & 0.634 & 0.560 & 0.647 & 0.583 & 0.640\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}
\includegraphics[width=\maxwidth]{figure/ch03_figempi_lmg_plot-1} \caption[LMG posterior distribution of different verbal ability tests]{LMG posterior distribution of different verbal ability tests}\label{fig:empi.lmg.plot}
\end{figure}


\end{knitrout}


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
general & 0.13 (0.105, 0.16)  & 0.299 (0.258, 0.333)   & 0.429 (0.364, 0.489)  \\ 
sentence & 0.203 (0.165, 0.245)  & 0.328 (0.293, 0.355)   & 0.532 (0.465, 0.588)  \\ 
wordc & 0.096 (0.073, 0.128)  & 0.24 (0.2, 0.275)   & 0.336 (0.273, 0.401)  \\ 
wordm & 0.176 (0.138, 0.214)  & 0.315 (0.278, 0.344)   & 0.492 (0.421, 0.55)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.ijt}
\end{table}


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
general & 0.131 (0.106, 0.161)  & 0.13 (0.105, 0.16)  \\ 
sentence & 0.206 (0.168, 0.251)  & 0.203 (0.165, 0.245)   \\ 
wordc & 0.096 (0.073, 0.125)  & 0.096 (0.073, 0.128)  \\ 
wordm & 0.178 (0.141, 0.219) & 0.176 (0.138, 0.214)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp}
\end{table}





\begin{table}[h]
\caption{Variance decomposition for Stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
general & 0.131 (0.106, 0.163)  &  0.127 (0.092, 0.16)  \\ 
sentence & 0.206 (0.171, 0.246)  &  0.204 (0.163, 0.249)   \\ 
wordc & 0.096 (0.075, 0.128)  &  0.097 (0.07, 0.128)  \\ 
wordm & 0.178 (0.144, 0.218) &  0.175 (0.136, 0.214) \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp.stoch}
\end{table}
\bibliographystyle{mywiley} 
\bibliography{biblio}
\end{document}
