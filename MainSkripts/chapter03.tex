\documentclass[11pt,a4paper,twoside]{book}
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R


\input{header.sty}   % packages, layout and standard macros



\begin{document}
% LaTeX file for Chapter 03












\chapter{Examples}

The following chapter presents the Bayesian LMG implementation by two examples. Simulated data is used for the first example. Empirical data is used for the second example.

\section{Simulated Data}

We assume a simple model for the first example: 
\begin{align*} 
&Y_{i} \sim \mathcal{N}(\beta_{0}+x_{1} \beta_{1}+x_{2} \beta_{2}+x_{3} \beta_{3}+x_{4} \beta_{4}, \, \sigma^2),& \\ & \beta_{1} = 0.5, , \beta_{2} = 1, \, \beta_{3} = 2 , \, \beta_{4}=0, \, \sigma^2 = 1 & \\ & \X_{1}, \X_{2},\X_{3},\X_{4} \sim \mathcal{N}(0, 1) 
\end{align*} 
The values of the four predictors are sampled from a standard normal distribution. These values are then multiplied by the regression coefficients. A standard normal distributed error is added to obtain the dependent variable. Fifty observations were sampled. The data generating R-code~\ref{r03:simdata.ex1} can be found in the Appendix.




The model is fitted using the \texttt{rstanarm} package \citep{rstanarm} with the default priors for the regression and $\sigma^2$ parameters. The exact command can be found in R-code~\ref{r03:model.ex1} These default priors are called 'weakly informative priors', because they take into account the order of magnitude of the variables by using the variance of the observed data. Information about these priors can be found in \cite{stanM2017}. A burn-in period of 20000, a sample size of 20000, and a thinning of 20 were chosen, resulting in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:simdata.postsample2}. 

For each posterior sample of the parameters, the $\Rtwo$ value was calculated. The $\Rtwo$ of the submodels was then calculated by the conditional variance formula for each posterior sample. The R-code is found in \ref{r03:LMG}. The resulting $\Rtwo$ values of the first few posterior samples are shown in Table~\ref{tab:simdata.postsample3}.  The thinning is reasonable in this case to reduce the computational burden and to still obtain an appropriate posterior of the $\Rtwo$ values \citep{Link2012}. 

The \texttt{hier.part} package was used to calculate the LMG value for each posterior sample. The LMG posteriors are shown in Table~\ref{tbl:nonstochEx1} . The independent component (I) represents the LMG value. The joint contribution (J) represents the difference from the independent component to the explained variance of the model containing only the predictor itself (T). Assuming stochastic or non-stochastic regressors has an influence on the uncertainty of the LMG values. 

At first, non-stochastic regressors were assumed. The resulting LMG values and joint contributions with a 95\% credible interval are shown in Table~\ref{tbl:nonstochEx1}. An option to display the resulting LMG distribution is shown in Figure ~ \ref{fig:simdata.LMG.plot}.  Using the default weakly informative priors, the LMG distributions obtained from the Bayesian framework were very similar to the bootstrap confidence intervals, assuming non-stochastic predictors of the LMG estimates obtained from the \texttt{relaimpo} package, as shown in Table~\ref{tbl:nonstochEx1relamip}. 





\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{table}

\caption{\label{tab:simdata.postsample2}Samples from the posterior distributions of the regression parameters}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
  & x1 & x2 & x3 & x4 & sigma\\
\midrule
sample 1 & 1.34 & 1.073 & 0.670 & 0.991 & 0.877\\
sample 2 & 1.07 & 1.396 & 0.937 & 0.999 & 1.055\\
sample 3 & 1.17 & 1.141 & 0.699 & 1.188 & 0.877\\
sample 4 & 1.29 & 1.064 & 0.640 & 1.136 & 1.012\\
sample 5 & 1.00 & 1.124 & 0.859 & 1.569 & 0.819\\
sample 6 & 0.99 & 1.102 & 0.599 & 1.127 & 1.035\\
sample 7 & 1.26 & 0.904 & 0.702 & 1.437 & 1.115\\
sample 8 & 1.22 & 1.128 & 0.552 & 1.421 & 0.924\\
sample 9 & 1.28 & 0.954 & 0.661 & 1.088 & 0.931\\
sample 10 & 1.09 & 1.057 & 0.865 & 1.475 & 0.984\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{table}

\caption{\label{tab:simdata.postsample3}$ \Rtwo$ for all submodels for the first six posterior samples}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & sample 1 & sample 2 & sample 3 & sample 4 & sample 5 & sample 6\\
\midrule
none & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
x1 & 0.351 & 0.213 & 0.253 & 0.296 & 0.155 & 0.196\\
x2 & 0.397 & 0.467 & 0.431 & 0.380 & 0.398 & 0.422\\
x3 & 0.081 & 0.117 & 0.078 & 0.067 & 0.100 & 0.057\\
x4 & 0.078 & 0.083 & 0.138 & 0.110 & 0.251 & 0.145\\
x1 x2 & 0.649 & 0.597 & 0.597 & 0.588 & 0.488 & 0.543\\
x1 x3 & 0.400 & 0.299 & 0.304 & 0.337 & 0.230 & 0.233\\
x1 x4 & 0.534 & 0.377 & 0.504 & 0.517 & 0.525 & 0.443\\
x2 x3 & 0.503 & 0.615 & 0.533 & 0.469 & 0.525 & 0.501\\
x2 x4 & 0.434 & 0.503 & 0.509 & 0.440 & 0.570 & 0.506\\
x3 x4 & 0.178 & 0.222 & 0.239 & 0.196 & 0.386 & 0.223\\
x1 x2 x3 & 0.721 & 0.716 & 0.672 & 0.648 & 0.593 & 0.601\\
x1 x2 x4 & 0.760 & 0.684 & 0.756 & 0.728 & 0.748 & 0.698\\
x1 x3 x4 & 0.601 & 0.484 & 0.575 & 0.574 & 0.630 & 0.497\\
x2 x3 x4 & 0.553 & 0.667 & 0.629 & 0.543 & 0.726 & 0.601\\
all & 0.844 & 0.817 & 0.846 & 0.802 & 0.875 & 0.770\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}







\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch03_figsimdata_LMG_plot-1} 

}

\caption[Posterior distribution of LMG values]{Posterior distribution of LMG values.}\label{fig:simdata.LMG.plot}
\end{figure}


\end{knitrout}

\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.235 (0.145, 0.337)  & 0.01 (-0.009, 0.029)   & 0.245 (0.15, 0.351)  \\ 
x2 & 0.31 (0.199, 0.411)  & 0.066 (0.051, 0.079)   & 0.376 (0.256, 0.482)  \\ 
x3 & 0.104 (0.041, 0.187)  & -0.008 (-0.014, -0.002)   & 0.096 (0.035, 0.178)  \\ 
x4 & 0.161 (0.085, 0.254)  & -0.027 (-0.042, -0.013)   & 0.133 (0.064, 0.228)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1}
\end{table}




\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
x1 & 0.241 (0.152, 0.339)  & 0.235 (0.145, 0.337)  \\ 
x2 & 0.318 (0.223, 0.431)  & 0.31 (0.199, 0.411)   \\ 
x3 & 0.107 (0.046, 0.185)  & 0.104 (0.041, 0.187)  \\ 
x4 & 0.166 (0.097, 0.262) & 0.161 (0.085, 0.254)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relamip}
\end{table}

\FloatBarrier


In this example, we know, that the predictor values were sampled from a normal distribution. It would therefore be more reasonable to assume stochastic predictors. Under the assumption of weak exogeinity and conditional independence, the posterior distributions of the regression parameters $\bbeta$ are valid for non-stochastic and stochastic predictors. However, the uncertainty about the LMG values needs to include the uncertainty about the covariance matrix. If we know the distribution of the predictors we can incorporate this information and obtain the posterior distribution of the covariance matrix. The package \texttt{Jags} was used for inference about the covariance matrix in a Bayesian way. As an alternative, non-parametric bootstrap was used for inference about the covariance matrix. 

Using the bootstrap samples of the covariance matrix or samples from the posterior covariance matrix produced  very similar LMG distributions. Bootstrap seems to be a valuable option for stochastic predictors when the distribution of the predictors is unknown. Even when the distribution is known, the difference seems to be tiny. A benefit of going the full Bayesian way is that we can also include prior knowledge about the covariance matrix. Using the default priors further produced very similar LMG distribution as using the non-parametric bootstrap option of the \texttt{relaimpo} package. Table~\ref{tbl:nonstochEx1relaimpstoch} shows the LMG values of these approaches. For stochastic predictors, in contrast to non-stochastic predictors, the uncertainty about the covariance matrix is reflected in the larger credible intervals. Even when the exact regression parameters were known, there would a lot of uncertainty in the LMG values caused by the uncertainty about the covariance matrix. 








\begin{table}[h]
\caption{LMG values assuming stochastic predictors with 95\% CI.}
\centering
\begin{tabular}{clll}
   \toprule
  & \multicolumn{1}{c}{\textbf{Relaimpo}} & \multicolumn{2}{c}{\textbf{Bayesian framework}} \\ \cmidrule(r){2-2}\cmidrule(l){3-4}
 \textbf{Variable} &  & \multicolumn{1}{c}{nonparameteric bootstrap}& \multicolumn{1}{c}{covariance inference} \\
 \midrule
x1 & 0.241 (0.125, 0.392)  & 0.242 (0.139, 0.356) &  0.248 (0.137, 0.408) \\ 
x2 & 0.318 (0.187, 0.429)  & 0.288 (0.164, 0.423)  & 0.302 (0.186, 0.416) \\ 
x3 & 0.107 (0.022, 0.249)  & 0.092 (0.031, 0.186)  & 0.099 (0.033, 0.192) \\ 
x4 & 0.166 (0.075, 0.28) & 0.176 (0.088, 0.301) & 0.161 (0.058, 0.269) \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relaimpstoch}
\end{table}







\clearpage

\section{Empirical Data}
In the following section, the Bayesian LMG implementation is applied on an empirical dataset containing test scores of pupils (N = 301) from a study by \cite{Holzinger1939} available in the R package \texttt{MBESS} \citep{MBESS}. This dataset was used in \cite{Nimon2008} to present commonality analysis, which is another variance decomposition technique. Scores from a paragraph comprehension test (paragrap) were predicted by four verbal tests:  general-information (general),  sentence-comprehension (sentence) ,  word-classification (wordc), and  word-meaning (wordm) (Table~\ref{table:hs.data}). 

The aim of the regression analysis was to determine the  association between verbal ability and paragraph comprehension.  
An overview of the data is shown in Figure~\ref{fig:empi.lmg.plot}. The regression results are shown in Table~\ref{tbl:empirical.reg}). A novice researcher may wrongly conclude, that there is little association between the "non-significant" predictors (general information and word-classification) and paragraph comprehension. Given the other predictors are already included in the model, the predictors seem not to provide  much information about the expected paragraph comprehension ability. However, it should not be concluded from this regression table, that the association between any of these "non-significant" predictors and the dependent variable is unimportant. As shown in Figure~\ref{fig:empi.lmg.plot}, the correlations between the predictors are rather high. The LMG metric may therefore provide new information about the importance of each predictor. 

The Bayesian regression model was fitted in \texttt{rstanarm}. The default priors were used for the regression coefficients and the $\sigma^2$ parameter. A burn-in period of 20000, a sample size of 20000, and a thinning of 20 resulted in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:empirical.data.postsample2}. The resulting $\Rtwo$ of these posterior samples are shown in Table~\ref{tab:empirical.data.postsample3}. The LMG values were calculated by using \texttt{hier.part}. The  independent component (I), joint contribution (J), and total explained variance in a one-predictor model (T) are shown in Table~\ref{tbl:empirical.ijt}. Sentence-comprehension and word-meaning seem to be the most important predictors by applying the LMG metric. However, none of the predictors seem to be unimportant.  The joint contributions of each predictor were quite large.

For comparison purposes, the LMG metric was additionally calculated with the \texttt{relaimpo} package using parametric bootstrapping. The confidence intervals of  \texttt{relaimpo} were almost identical to the credible intervals of the Bayesian framework (Table~\ref{tbl:empirical.relaimp.comp}). Assuming stochastic or non-stochastic predictors resulted also in almost identical uncertainty estimates with such a large sample size (Table\ref{tbl:empirical.relaimp.comp.stoch}). 







\begin{table}
\centering
\caption{Variable description}
\begin{tabular}{l l}
  \toprule			
  Variable Name & Description  \\   \midrule  
  paragrap & scores on paragraph comprehension test  \\
  general & scores on general information test \\
  sentence & scores on sentence completion test\\
  wordc & scores on word classification test \\
  wordm & scores on word meaning test \\
  \bottomrule  
\end{tabular}
\label{table:hs.data}
\end{table}

\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch03_figunnamed-chunk-1-1} 

}

\caption[Test scores from Holzinger and Swineford's  (1939) Study]{Test scores from Holzinger and Swineford's  (1939) Study. N=301}\label{fig:unnamed-chunk-1}
\end{figure}






% latex table generated in R 3.4.3 by xtable 1.8-2 package
% Thu Aug  9 19:21:45 2018
\begin{table}[!h]
\centering
\caption{Regression of paragraph comprehension on verbal tests.} 
\label{tbl:empirical.reg}
\begingroup\footnotesize
\begin{tabular}{rrrr}
  \hline
 & Coefficient & 95\%-confidence interval & $p$-value \\ 
  \hline
Intercept & 0.071 & from -1.17 to 1.31 & 0.91 \\ 
  general & 0.03 & from -0.00 to 0.06 & 0.084 \\ 
  sentence & 0.26 & from 0.18 to 0.34 & $<$ 0.0001 \\ 
  wordc & 0.047 & from -0.01 to 0.11 & 0.14 \\ 
  wordm & 0.14 & from 0.08 to 0.19 & $<$ 0.0001 \\ 
   \hline
\end{tabular}
\endgroup
\end{table}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{table}

\caption{\label{tab:empirical.data.postsample2}Samples from the posterior distributions of the regression parameters}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
  & general & sentence & wordc & wordm & sigma\\
\midrule
sample 1 & 0.035 & 0.251 & 0.047 & 0.133 & 2.06\\
sample 2 & 0.036 & 0.235 & 0.049 & 0.155 & 2.24\\
sample 3 & 0.050 & 0.268 & 0.036 & 0.150 & 2.15\\
sample 4 & 0.014 & 0.254 & 0.054 & 0.141 & 2.32\\
sample 5 & 0.026 & 0.318 & 0.022 & 0.130 & 2.25\\
sample 6 & 0.039 & 0.260 & 0.070 & 0.116 & 2.12\\
sample 7 & 0.063 & 0.239 & 0.039 & 0.091 & 2.27\\
sample 8 & 0.035 & 0.305 & 0.037 & 0.136 & 2.13\\
sample 9 & 0.051 & 0.284 & 0.009 & 0.137 & 2.19\\
sample 10 & 0.072 & 0.246 & 0.010 & 0.107 & 2.21\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{table}

\caption{\label{tab:empirical.data.postsample3}$ \Rtwo$ for all submodels for the first six posterior samples}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & sample 1 & sample 2 & sample 3 & sample 4 & sample 5 & sample 6\\
\midrule
none & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
general & 0.456 & 0.442 & 0.494 & 0.373 & 0.416 & 0.460\\
sentence & 0.551 & 0.518 & 0.571 & 0.488 & 0.552 & 0.555\\
wordc & 0.350 & 0.335 & 0.355 & 0.310 & 0.314 & 0.373\\
wordm & 0.512 & 0.510 & 0.544 & 0.452 & 0.479 & 0.493\\
general sentence & 0.593 & 0.563 & 0.624 & 0.512 & 0.578 & 0.598\\
general wordc & 0.499 & 0.481 & 0.531 & 0.419 & 0.453 & 0.513\\
general wordm & 0.558 & 0.549 & 0.597 & 0.479 & 0.517 & 0.547\\
sentence wordc & 0.567 & 0.534 & 0.584 & 0.501 & 0.559 & 0.576\\
sentence wordm & 0.619 & 0.597 & 0.648 & 0.548 & 0.604 & 0.612\\
wordc wordm & 0.559 & 0.550 & 0.586 & 0.494 & 0.517 & 0.555\\
general sentence wordc & 0.597 & 0.567 & 0.627 & 0.517 & 0.579 & 0.605\\
general sentence wordm & 0.628 & 0.606 & 0.661 & 0.550 & 0.607 & 0.623\\
general wordc wordm & 0.579 & 0.568 & 0.613 & 0.503 & 0.534 & 0.577\\
sentence wordc wordm & 0.625 & 0.603 & 0.653 & 0.553 & 0.605 & 0.623\\
all & 0.631 & 0.609 & 0.663 & 0.554 & 0.608 & 0.630\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch03_figempi_lmg_plot-1} 

}

\caption[LMG posterior distribution of different verbal ability tests]{LMG posterior distribution of different verbal ability tests}\label{fig:empi.lmg.plot}
\end{figure}


\end{knitrout}


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
general & 0.13 (0.104, 0.159)  & 0.299 (0.258, 0.332)   & 0.429 (0.363, 0.489)  \\ 
sentence & 0.203 (0.166, 0.246)  & 0.327 (0.291, 0.356)   & 0.532 (0.467, 0.589)  \\ 
wordc & 0.095 (0.074, 0.127)  & 0.239 (0.201, 0.277)   & 0.334 (0.275, 0.401)  \\ 
wordm & 0.177 (0.141, 0.215)  & 0.315 (0.279, 0.344)   & 0.491 (0.423, 0.551)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.ijt}
\end{table}


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
general & 0.131 (0.106, 0.162)  & 0.13 (0.104, 0.159)  \\ 
sentence & 0.206 (0.165, 0.248)  & 0.203 (0.166, 0.246)   \\ 
wordc & 0.096 (0.074, 0.127)  & 0.095 (0.074, 0.127)  \\ 
wordm & 0.178 (0.143, 0.217) & 0.177 (0.141, 0.215)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp}
\end{table}





\begin{table}[h]
\caption{Variance decomposition for Stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
general & 0.131 (0.106, 0.164)  &  0.127 (0.097, 0.16)  \\ 
sentence & 0.206 (0.17, 0.247)  &  0.202 (0.162, 0.246)   \\ 
wordc & 0.096 (0.075, 0.124)  &  0.094 (0.066, 0.128)  \\ 
wordm & 0.178 (0.144, 0.216) &  0.175 (0.138, 0.216) \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp.stoch}
\end{table}




\bibliographystyle{mywiley} 
\bibliography{biblio}
\end{document}
