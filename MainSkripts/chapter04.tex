\documentclass[11pt,a4paper,twoside]{book}
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R


\input{header.sty}   % packages, layout and standard macros



\begin{document}
% LaTeX file for Chapter 04











\chapter{Extension to longitudinal data}

Some extensions of the LMG formula beyond the simple linear regression model are shown in the following chapter. The focus is on repeated measures model. These models extend the simple linear regression by allowing intra-subject correlation between repeated measures.

The dependence of within-subject measurements can be modeled by including random effects (mixed model) or by assuming correlated errors within a subject (marginal model). A mixed model can be extended by including a random slope per subject, allowing for less restrictive longitudinal shapes. Different covariance matrices of the error terms allow for less restrictions in the marginal approach. An unstructured covariance matrix, where no restriction are imposed, allows for the most freedom in the error term. However, depending on the number of repeated measurements and the sample size, the covariance matrix can get too large to make reasonable inference about it \citep{Fitzmaurice2011}. 

The extension of the LMG formula in the Bayesian framework applied to longitudinal models is restricted to models where the conditional variance formula can  be applied to get the explained variance of the submodel from the regression parameters of the full-model. Therefore, the focus is on the fixed predictors, but not on the random effects. The conditional variance formula can be used in the marginal models, because only the fixed effects are modeled anyway. In the mixed model framework, the conditional variance formula is applicable to random intercept models. For random-slope models, there are at least some difficulties involved - if it is possible at all - to get the explained variance of the submodels. 

In this chapter, the Bayesian LMG Implementation is shown on a random intercept model and on a repeated measurement model with an unstructured covariance matrix.  

\section{Random intercept model}
The first example concerns a simple random intercept model with time-varying predictors.  Different $\Rtwo$ metrics exist for linear mixed models. The variance of a random intercept model with regression parameter $\bbeta$ can be written as

      \begin{align} 
        \var(y) = \sigma_{f}^2  + \sigma_{\alpha}^2 + \sigma_{\epsilon}^2, \label{eq:var.t.ri} 
        \end{align}

where $\sigma_{f}^2 = \var(\X \bbeta) = \bbeta^\top \bSigma_{\X \X}  \bbeta$ , $\sigma_{\alpha}^2 $ is the variance of the random intercept, and $\sigma_{\epsilon}^2$ represents the error variance \citep{Nakagawa2013}. 

An $\Rtwo$ that is guaranteed to be positive is defined in \cite{Nakagawa2013} as

   \begin{align} 
\Rtwo_{\text{LMM}} = \frac{\sigma_{f}^2}{\sigma_{f}^2 + \sigma_{\alpha}^2 + \sigma_{\epsilon}^2}.
\end{align}


Theoretically, it is possible that the $\Rtwo_{\text{LMM}}$ decreases when adding predictors \citep{Nakagawa2013}.  By adding predictors, $\sigma_{f}^2$  should always increase and  $\sigma_{\epsilon}^2$  should decrease. However, the $\sigma_{\alpha}^2$ may also increase minimally and the total $\Rtwo$ may then be minimally lower. The $\Rtwo$ can not decrease by using the conditional variance formula on the full-model to calculate the $\Rtwo$ of the submodels, because the total variance is fixed. Equal results should be obtained, by fitting a new model by maximum likelihood for each submodel and by comparing the explained variance of the fixed effects to the total variance of the full-model.  In the Bayesian framework, the conditional variance formula is necessary to account for the mutual interdependence of the submodels. The total variance of the full-model can be calculated as  $\var(y) = \var(\X \bbeta + \Z b) + \sigma^2$ or by using samples of $\sigma_{\alpha}^2$ as in definition \eqref{eq:var.t.ri}. The error term could again be sampled or calculated as in definition \eqref{eq:rtwoGelman}. In the following examples, definition \eqref{eq:var.t.ri} was used,  $\sigma_{\alpha}^2$ and $\sigma_{\epsilon}^2$ were sampled from their posterior distribution.

In repeated measures studies, the focus is often on within-subject changes. The between-subject variance, estimated with the random intercept term, is of minor importance. The important question is, how much variance the fixed predictors do explain, compared to the variance of the within subject error, which is

   \begin{align} 
\Rtwo_{\text{repeated}} = \frac{\sigma_{f}^2}{\sigma_{f}^2  + \sigma_{\epsilon}^2}, \label{eq:rtwo.repeated}
\end{align}

The square root of this term is known under the name \textit{correlation within subjects} in \cite{Bland1995}. Often, there are between-subject and within-subject predictors in a model. If the interest lies in the within-subject effects,  a model including only the between-subject predictors can be used as the null-model.

The following example shows a simple random intercept model with time-varying predictors. The main question is which within-subject predictors are the most important ones. The between-subject variance is of minor importance. 

The data are simulated from the following regression setting with $m = 4$ timepoints and $n = 20$ number of subjects (see Appendix~\ref{r04:simdatri} for R-code),

\begin{align*} 
Y_{i,j} \sim \mathcal{N}(\beta_{0}+x_{1_{i,j}} \beta_{1}+x_{2_{i,j}} \beta_{2}+x_{3_{i,j}} \beta_{3}+x_{4_{i,j}} \beta_{4} + \alpha_{i}, \, \sigma^2), \qquad &i = 1, \dots, n \\  &j = 1, \dots, m
\end{align*} 

where $\beta_{1} = 1 \,$,  $\beta_{2} = 1 \,$,   $\beta_{3} = 2 \,$  $\beta_{4}=2 \,$, $\sigma^2 = 1 \, $, $\alpha_{i} \sim \mathcal{N}(0, \sigma_{\alpha}^2) \,$, $\X \sim \mathcal{N}(\0, \bSigma)$.



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch04_fig_repeated_plot_ri-1} 

}

\caption[Individual trajectories of simulated random intercept model]{Individual trajectories of simulated random intercept model.}\label{fig:.repeated.plot.ri}
\end{figure}


\end{knitrout}


 The random intercept effect was of minor interest. The $\Rtwo$ of the models was calculated according to the formula of repeated measure correlation \eqref{eq:rtwo.repeated}. The R-code of the odel and the function can be found in the Appendix (\ref{r04:model.ri} and \ref{r04:LMG.ri}). Most of the within-subject variance was explained by the predictors (Table~\ref{tbl:repeatedcormod}). The credible intervals were very small. For information about the between-subject variance term, we can look at the posterior distribution of the random intercept variance term.

Next, the random intercept was directly included in the total variance calculation of the $\Rtwo$ values. There was a large between-subject variance in this simulated dataset (Table~\ref{tbl:repeatedcormod.tot}). Therefore, the LMG values including the between subject variance were much lower. The credible intervals were as well much larger, because the uncertainty about the between-subject variance was included. 

In my opinion, we can get more useful information from separating the between-subject variance from the within-subject variance components in this simple case. Note, that we assumed non-stochastic predictors. Otherwise, the credible intervals were larger. In general, it seems more reasonable to assume stochastic time-varying predictors. The variance could then be estimated by non-parametric bootstrap, resampling whole subjects (all repeated measurements of a subject).







\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.189 (0.187, 0.19)  & 0.467 (0.465, 0.468)   & 0.655 (0.652, 0.659)  \\ 
x2 & 0.19 (0.188, 0.191)  & 0.471 (0.469, 0.472)   & 0.661 (0.657, 0.664)  \\ 
x3 & 0.311 (0.31, 0.312)  & 0.633 (0.632, 0.633)   & 0.944 (0.942, 0.945)  \\ 
x4 & 0.31 (0.309, 0.311)  & 0.634 (0.634, 0.634)   & 0.944 (0.943, 0.946)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:repeatedcormod}
\end{table}




\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values,   J~=~joint~contribution, Total~=~total explained variance in one-predictor model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.037 (0.019, 0.068)  & 0.079 (-0.011, 0.185)   & 0.112 (0.023, 0.251)  \\ 
x2 & 0.066 (0.031, 0.108)  & 0.137 (0.055, 0.234)   & 0.204 (0.086, 0.341)  \\ 
x3 & 0.104 (0.051, 0.165)  & 0.182 (0.075, 0.309)   & 0.287 (0.127, 0.473)  \\ 
x4 & 0.114 (0.057, 0.178)  & 0.186 (0.077, 0.314)   & 0.301 (0.137, 0.491)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:repeatedcormod.tot}
\end{table}



\section{Marginal  model}

The next example is about a repeated measurement model with time-varying predictors and an unstructured error covariance matrix. The data are generated from the following model

\begin{align} 
&Y_{i} \sim \mathcal{N}(\X_{i} \bbeta, \bSigma), \qquad i = 1, \dots, n ,
\end{align} 

where $\bSigma$ represents an unstructured error covariance matrix, $\X_{i}$ represents the predictor matrix of size $m \times p$ of subject $i$.

The R-code of the data generation can be found in Appendix~\ref{r04:rmarg.data}. In the variance calculation, it is necessary to take into account that there is not just one $\sigma^2$ parameter, but a covariance matrix $\bSigma$. The diagonal elements of $\bSigma$ represent the variance of each timepoint. The sum of the diagonal elements of $\bSigma$ represents the variance for a whole subject. The mean can be taken of $\diag(\bSigma)$ to make the formula compatible with the $\bbeta^\top \bSigma_{\X \X}  \bbeta$ of \eqref{eq:rtwoused}, resulting in the total variance term

      \begin{align} 
        \var(\Y) = \bbeta^\top \bSigma_{\X \X}  \bbeta + \text{mean}(\diag(\bSigma)),
   \end{align}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch04_figsimdata_repeated_unstruct_plot-1} 

}

\caption[Individual trajectories of simulated data with unstructured error covariance matrix]{Individual trajectories of simulated data with unstructured error covariance matrix.}\label{fig:simdata.repeated.unstruct.plot}
\end{figure}


\end{knitrout}




The individual trajectories are shown in Figure~\ref{fig:simdata.repeated.unstruct.plot}. The R-code for the model and LMG implementation can be found in Appendix (\ref{r04:model.unstruct} and  \ref{r04:LMG.rmarg}). The resulting LMG values of the predictors are shown in Table~\ref{tbl:repeated.unstructured}.

\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.154 (0.127, 0.174)  & 0.409 (0.337, 0.456)   & 0.563 (0.462, 0.63)  \\ 
x2 & 0.155 (0.126, 0.178)  & 0.39 (0.322, 0.433)   & 0.544 (0.448, 0.609)  \\ 
x3 & 0.213 (0.177, 0.237)  & 0.481 (0.401, 0.532)   & 0.695 (0.582, 0.768)  \\ 
x4 & 0.206 (0.172, 0.228)  & 0.486 (0.404, 0.536)   & 0.692 (0.58, 0.764)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:repeated.unstructured}
\end{table}



\bibliographystyle{mywiley} 
\bibliography{biblio}
\end{document}
