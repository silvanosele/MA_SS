@article{Nimon2013,
abstract = {Multiple linear regression (MLR) remains a mainstay analysis in organizational research, yet inter-correlations between predictors (multicollinearity) undermine the interpretation of MLR weights in terms of predictor contributions to the criterion. Alternative indices include validity coefficients, structure coefficients, product measures, relative weights, all-possible-subsets regression, dominance weights, and commonality coefficients. This article reviews these indices, and uniquely, it offers freely available software that (a) computes and compares all of these indices with one another, (b) computes associated bootstrapped confidence intervals, and (c) does so for any number of pre-dictors so long as the correlation matrix is positive definite. Other available software is limited in all of these respects. We invite researchers to use this software to increase their insights when applying MLR to a data set. Avenues for future research and application are discussed. Keywords multiple regression, quantitative research, exploratory, research design A continued goal of organizational researchers conducting regression analysis is to make inferences about the relative importance of predictor variables (cf. Nimon, Gavrilova, {\&} Roberts, 2010; Zien-tek, Capraro, {\&} Capraro, 2008), yet it is all too common to rely heavily (if not solely) on the regression coefficients from the analysis which optimize sample-specific prediction (minimize sum of squared errors). Instead, other metrics that operationalize relative importance in ways that are consistent with such researchers' goals would seem more appropriate, and a range of metrics and approaches exists. In addition to regression weights and zero-order correlation coefficients that researchers likely report, MLR interpretation may be further informed by considering structure},
author = {Nimon, Kim F and Oswald, Frederick L},
doi = {10.1177/1094428113493929},
file = {:Users/silvano/Library/Application Support/Mendeley Desktop/Downloaded/Nimon, Oswald - 2013 - Understanding the Results of Multiple Linear Regression Beyond Standardized Regression Coefficients.pdf:pdf},
journal = {Organizational Research Methods},
number = {0},
pages = {1--25},
title = {{Understanding the Results of Multiple Linear Regression: Beyond Standardized Regression Coefficients}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.923.7363{\&}rep=rep1{\&}type=pdf},
volume = {00},
year = {2013}
}
@article{Ray-Mukherjee2014,
abstract = {1. In the face of natural complexities and multicollinearity, model selection and predictions using multiple regression may be ambiguous and risky. Confounding effects of predictors often cloud researchers' assessment and interpretation of the single best 'magic model'. The shortcomings of step-wise regression have been extensively described in statistical literature, yet it is still widely used in ecological literature. Similarly, hierarchical regression which is thought to be an improvement of the stepwise procedure, fails to address multicollinearity. 2. We propose that regression commonality analysis (CA), a technique more commonly used in psychology and education research will be helpful in interpreting the typical multiple regression analyses conducted on ecological data. 3. CA decomposes the variance of R 2 into unique and common (or shared) variance (or effects) of pre-dictors, and hence, it can significantly improve exploratory capabilities in studies where multiple regressions are widely used, particularly when predictors are correlated. CA can explicitly identify the magnitude and location of multicollinearity and suppression in a regression model. In this paper, using a simulated (from a correlation matrix) and an empirical dataset (human habitat selection, migration of Canadians across cities), we demonstrate how CA can be used with correlated predictors in multiple regression to improve our understanding and interpretation of data. We strongly encourage the use of CA in ecological research as a follow-on analysis from multiple regressions .},
author = {Ray-Mukherjee, Jayanti and Nimon, Kim and Mukherjee, Shomen and Morris, Douglas W and Slotow, Rob and Hamer, Michelle},
doi = {10.1111/2041-210X.12166},
file = {:Users/silvano/Library/Application Support/Mendeley Desktop/Downloaded/Ray-Mukherjee et al. - 2014 - Using commonality analysis in multiple regressions a tool to decompose regression effects in the face o(2).pdf:pdf},
keywords = {Key-words: stepwise regression,habitat selection,hierarchical regression,standardized partial regression coefficient,structure coefficients,suppressor variable},
title = {{Using commonality analysis in multiple regressions: a tool to decompose regression effects in the face of multicollinearity}},
url = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12166},
year = {2014}
}
@article{Gromping2007,
abstract = {Assigning shares of "relative importance" to each of a set of regressors is one of the key goals of researchers applying linear regression, particularly in sciences that work with observational data.. Although the topic is quite old, advances in computational capabilities have led to increased applications of computer-intensive methods like averaging over orderings that enable a reasonable decomposition of the model variance. This article serves two purposes: to reconcile the large and somewhat fragmented body of recent literature on relative importance and to investigate the theoretical and empirical properties of the key competitors for decomposition of model variance.},
author = {Gr{\"{o}}mping, Ulrike},
doi = {10.1198/000313007X188252},
file = {:Users/silvano/Dokumente/Masterarbeit/Literatur/Groemping2007.pdf:pdf},
isbn = {000313007X},
issn = {00031305},
journal = {American Statistician},
keywords = {Averaging over orderings,Linear model,Proportional marginal variance decomposition (PMVD,Sequential sums of squares},
number = {2},
pages = {139--147},
title = {{Estimators of relative importance in linear regression based on variance decomposition}},
volume = {61},
year = {2007}
}
@article{Gromping2015,
abstract = {Regression analysis is one of the most-used statistical methods. Often part of the research question is the identification of the most important regressors or an importance ranking of the regressors. Most regression models are not specifically suited for answering the variable importance question, so that many different proposals have been made. This article reviews in detail the various variable importance metrics for the linear model, particularly emphasizing variance decomposition metrics. All linear model metrics are illustrated by an example analysis. For nonlinear parametric models, several principles from linear models have been adapted, and machine-learning methods have their own set of variable importance methods. These are also briefly covered. Although there are many variable importance metrics, there is still no convincing theoretical basis for them, and they all have a heuristic touch. Nevertheless, some metrics are considered useful for a crude assessment in the absence of a good subject matter theory. WIREs Comput Stat 2015, 7:137–152. doi: 10.1002/wics.1346},
author = {Gr{\"{o}}mping, Ulrike},
doi = {10.1002/wics.1346},
file = {:Users/silvano/Dokumente/Masterarbeit/Literatur/Groemping-2015-Wiley{\_}Interdisciplinary{\_}Reviews{\%}3A{\_}Computational{\_}Statistics.pdf:pdf},
isbn = {1939-0068},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Correlated regressors,Hierarchical partitioning,Multiple regression,Relative importance},
number = {2},
pages = {137--152},
title = {{Variable importance in regression models}},
volume = {7},
year = {2015}
}
@article{Chevan1991,
author = {Chevan, Albert and Sutherland, Michael},
doi = {10.1080/00031305.1991.10475776},
issn = {0003-1305},
journal = {The American Statistician},
month = {may},
number = {2},
pages = {90--96},
title = {{Hierarchical Partitioning}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1991.10475776},
volume = {45},
year = {1991}
}
@article{Nakagawa2013,
abstract = {*$\backslash$nThe use of both linear and generalized linear mixed-effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (AIC), are usually presented as model comparison tools for mixed-effects models.$\backslash$n$\backslash$n$\backslash$n$\backslash$n*$\backslash$nThe presentation of ‘variance explained' (R2) as a relevant summarizing statistic of mixed-effects models, however, is rare, even though R2 is routinely reported for linear models (LMs) and also generalized linear models (GLMs). R2 has the extremely useful property of providing an absolute value for the goodness-of-fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained, R2 can also be a quantity of biological interest.$\backslash$n$\backslash$n$\backslash$n$\backslash$n*$\backslash$nOne reason for the under-appreciation of R2 for mixed-effects models lies in the fact that R2 can be defined in a number of ways. Furthermore, most definitions of R2 for mixed-effects have theoretical problems (e.g. decreased or negative R2 values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation).$\backslash$n$\backslash$n$\backslash$n$\backslash$n*$\backslash$nHere, we make a case for the importance of reporting R2 for mixed-effects models. We first provide the common definitions of R2 for LMs and GLMs and discuss the key problems associated with calculating R2 for mixed-effects models. We then recommend a general and simple method for calculating two types of R2 (marginal and conditional R2) for both LMMs and GLMMs, which are less susceptible to common problems.$\backslash$n$\backslash$n$\backslash$n$\backslash$n*$\backslash$nThis method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed-effects models. The proposed method has the potential to facilitate the presentation of R2 for a wide range of circumstances.},
archivePrefix = {arXiv},
arxivId = {2746},
author = {Nakagawa, Shinichi and Schielzeth, Holger},
doi = {10.1111/j.2041-210x.2012.00261.x},
editor = {O'Hara, Robert B.},
eprint = {2746},
file = {:Users/silvano/Library/Application Support/Mendeley Desktop/Downloaded/Nakagawa, Schielzeth - 2013 - A general and simple method for obtaining iRi sup2sup from generalized linear mixed-effects models.pdf:pdf},
isbn = {2041-210X},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Coefficient of determination,Goodness-of-fit,Heritability,Information criteria,Intra-class correlation,Linear models,Model fit,Repeatability,Variance explained},
month = {feb},
number = {2},
pages = {133--142},
pmid = {11165473},
publisher = {Wiley/Blackwell (10.1111)},
title = {{A general and simple method for obtaining R2 from generalized linear mixed-effects models}},
url = {http://doi.wiley.com/10.1111/j.2041-210x.2012.00261.x},
volume = {4},
year = {2013}
}
@article{Gelman2017,
abstract = {The usual definition of R 2 (variance of the predicted values divided by the variance of the data) has a problem for Bayesian fits, as the numerator can be larger than the denominator. We propose an alternative definition similar to one that has appeared in the survival analysis literature: the variance of the predicted values divided by the variance of predicted values plus the variance of the errors. This summary is computed automatically for linear and generalized linear regression models fit using rstanarm, our R package for fitting Bayesian applied regression models with Stan. 1. The problem Consider a regression model of outcomes y and predictors X with predicted values E(y|X, $\theta$), fit to data (X, y) n , n = 1, . . . , N . Ordinary least squares regression yields an estimated parameter vecto $\theta$ with predicted value y n = E(y|X n $\theta$) and residual variance V N n= y n , where we are using the notation, V N n=1 z n = 1 N − 1 N n=1 (z n − ¯ z) 2 , for any vector z. The proportion of variance explained, classical R 2 = V N n= y n V N n=1 y n , (1) is a commonly used measure of model fit, and there is a long literature on interpreting it, adjusting it for degrees of freedom used in fitting the model, and generalizing it to other settings such as hierarchical models; see Xu (2003) and Gelman and Pardoe (2006). Here we consider how to extend the concept of R 2 to apply to Bayesian model fitting. Our motivation is the rstanarm R package (Gabry and Goodrich, 2017) for fitting applied regression},
author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Ali, Imad},
file = {:Users/silvano/Library/Application Support/Mendeley Desktop/Downloaded/Gelman et al. - 2017 - R-squared for Bayesian regression models.pdf:pdf},
title = {{R-squared for Bayesian regression models *}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/bayes{\_}R2.pdf},
year = {2017}
}
@article{Nakagawa2017,
abstract = {The coefficient of determination R2 quantifies the proportion of variance explained by a statistical model and is an important summary statistic of biological interest. However, estimating R2 for generalized linear mixed models (GLMMs) remains challenging. We have previously introduced a version of R2 that we called [Formula: see text] for Poisson and binomial GLMMs, but not for other distributional families. Similarly, we earlier discussed how to estimate intra-class correlation coefficients (ICCs) using Poisson and binomial GLMMs. In this paper, we generalize our methods to all other non-Gaussian distributions, in particular to negative binomial and gamma distributions that are commonly used for modelling biological data. While expanding our approach, we highlight two useful concepts for biologists, Jensen's inequality and the delta method, both of which help us in understanding the properties of GLMMs. Jensen's inequality has important implications for biologically meaningful interpretation of GLMMs, whereas the delta method allows a general derivation of variance associated with non-Gaussian distributions. We also discuss some special considerations for binomial GLMMs with binary or proportion data. We illustrate the implementation of our extension by worked examples from the field of ecology and evolution in the R environment. However, our method can be used across disciplines and regardless of statistical environments.},
author = {Nakagawa, Shinichi and Johnson, Paul C D and Schielzeth, Holger},
doi = {10.1098/rsif.2017.0213},
file = {:Users/silvano/Library/Application Support/Mendeley Desktop/Downloaded/Nakagawa, Johnson, Schielzeth - 2017 - The coefficient of determinationR2and intra-class correlation coefficient from generalized linear.pdf:pdf},
issn = {1742-5662},
journal = {Journal of the Royal Society, Interface},
keywords = {goodness of fit,heritability,model fit,reliability analysis,repeatability,variance decomposition},
month = {sep},
number = {134},
pmid = {28904005},
publisher = {The Royal Society},
title = {{The coefficient of determination R2 and intra-class correlation coefficient from generalized linear mixed-effects models revisited and expanded.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28904005 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5636267},
volume = {14},
year = {2017}
}
