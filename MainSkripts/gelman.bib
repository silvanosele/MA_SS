@article{Gelman2017,
abstract = {The usual definition of R 2 (variance of the predicted values divided by the variance of the data) has a problem for Bayesian fits, as the numerator can be larger than the denominator. We propose an alternative definition similar to one that has appeared in the survival analysis literature: the variance of the predicted values divided by the variance of predicted values plus the variance of the errors. This summary is computed automatically for linear and generalized linear regression models fit using rstanarm, our R package for fitting Bayesian applied regression models with Stan. 1. The problem Consider a regression model of outcomes y and predictors X with predicted values E(y|X, $\theta$), fit to data (X, y) n , n = 1, . . . , N . Ordinary least squares regression yields an estimated parameter vecto $\theta$ with predicted value y n = E(y|X n $\theta$) and residual variance V N n= y n , where we are using the notation, V N n=1 z n = 1 N − 1 N n=1 (z n − ¯ z) 2 , for any vector z. The proportion of variance explained, classical R 2 = V N n= y n V N n=1 y n , (1) is a commonly used measure of model fit, and there is a long literature on interpreting it, adjusting it for degrees of freedom used in fitting the model, and generalizing it to other settings such as hierarchical models; see Xu (2003) and Gelman and Pardoe (2006). Here we consider how to extend the concept of R 2 to apply to Bayesian model fitting. Our motivation is the rstanarm R package (Gabry and Goodrich, 2017) for fitting applied regression},
author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Ali, Imad},
file = {:Users/silvano/Library/Application Support/Mendeley Desktop/Downloaded/Gelman et al. - 2017 - R-squared for Bayesian regression models(2).pdf:pdf},
title = {{R-squared for Bayesian regression models *}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/bayes{\_}R2.pdf},
year = {2017}
}
