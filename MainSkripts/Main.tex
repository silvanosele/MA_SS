\documentclass[11pt,a4paper,twoside]{book}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\input{header.sty}   % packages, layout and standard macros
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\input{title}

\graphicspath{{./figure/}}
\DeclareGraphicsExtensions{.pdf,.png}
\setcounter{tocdepth}{1}

\pagenumbering{roman}

\chapter*{Abstract}
\addtocontents{toc}{\protect \vspace*{13.mm}}
\addcontentsline{toc}{chapter}{\bfseries{Abstract}}

Quantifying the importance of predictors in regression has been an active area of research for a long time \citep{Gromping2015}. The variance decomposition metric \textit{LMG} provides useful information about possible associations between variables. The LMG metric is implemented in the R packages \texttt{hier.part} \citep{Walsh2015} and \texttt{relaimpo} \citep{Gromping2006}. Bayesian methods gained high popularity in many applied research areas in recent years.  

This master thesis shows how the LMG metric can be applied to a linear regression model that is fitted in the Bayesian framework. The LMG metric requires calculation of $\Rtwo$ for the possible submodels. The conditional variance formula can be applied to calculate the $\Rtwo$ values of the submodels from the posterior samples of the model containing all predictors. The mutual interdependence of the submodels is then respected for each posterior sample.

The implementation of the LMG metric in the Bayesian framework is presented on simulated and on empirical data. Using weakly informative priors resulted in very similar LMG values as the values obtained by using bootstrap in \texttt{relaimpo}. 

Some possible extension of the LMG metric to repeated measures studies are additionally presented. There are certain difficulties involved in quantifying the $\Rtwo$ in longitudinal data. However,  the extension seems to be reasonably possible for the simple random intercept model and marginal models.  


\tableofcontents
\setkeys{Gin}{width=.8\textwidth}

\chapter*{Preface}
\addtocontents{toc}{\protect \vspace*{13.mm}}
\addcontentsline{toc}{chapter}{\bfseries{Preface}}
\thispagestyle{plain}\markboth{Preface}{Preface}


\textit{Wenns scheisse l\"auft, l\"aufts scheisse!} Oliver Kahn

\bigskip

\begin{flushright}
Max Muster\\
June 2018
\end{flushright}

\addtocontents{toc}{\protect \vspace*{10mm}}

\cleardoublepage
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 



% LaTeX file for Chapter 01







\chapter{Introduction}

The objective of this master thesis is to implement the variable importance measure LMG (named after the authors Lindeman, Merenda, and Gold \citep{Gromping2007}) in linear models estimated with Bayesian methods. Bayesian methods have gained popularity because they allow to quantify the uncertainty about parameters and they allow to include prior information.

Regression models are popular in many applied research areas \citep{Nimon2013}. These models provide a tool to find an association between a response variable and a set of explanatory variables. The explanatory variables are also called predictors or covariates. Regression parameters provide information to what extent the response variable is expected to change when one predictor changes by one unit, given all other predictors in the model remain the same. Being aware of this last remark is very important for the correct interpretation of the regression parameters. It shows that the parameter value of a predictor is dependent on the other predictors in the model.

Because predictors are often correlated to some degree to each other, it is obviously not an easy task to find the most important predictors in a model. The first question is, what is meant by the importance of a predictor? There is no easy answer to this question and it is depending on the research issue. \cite{Gromping2015} concludes that there may never be a unique definition of variable importance. There exist different metrics to quantify the importance of predictors. These metrics focus on different aspects and with correlated predictors they lead to different conclusions.  A summary of variable importance metrics can be found in  \cite{Gromping2015}.  

A distinction should be made between the importance of predictors in regression models that are used to predict future data and in regression models, applied to find an association between predictors and the response variable. In the first case, the aim is only to reduce the error between the predicted values and the observable values. The underlying association between predictors is of minor importance. In the second case, the focus is on the strength of the relationship between the predictors and the response variable. A predictor may explain little of the response variable, given two other correlated predictors are already included in a regression model. However, this predictor that is unimportant from the regression output may be the main cause  of the other two predictorss. Therefore, it may be the most important predictor of this regression model \citep{Gromping2007}. 

The causal relationship between the variables is missing in standard regression models. Studying a predictor, given other variables are already included or using models that contain only  the predictor itself,  provide only some parts of the bigger picture about the predictor in a model. Which  are the most useful variable importance metrics is still an open debate. A convincing theoretical basis is still lacking for all of those metrics.  \cite{Gromping2015} recommends to use the existing best practices, until a more profound solution will be found. For variance (or generally goodness-of-fit) decomposition based importance, she recommends to use LMG enhanced with joint contributions or dominance analysis \citep{Gromping2015}.








% LaTeX file for Chapter 02








\chapter{Theoretical background} 

\section{LMG variable importance metric}

The focus of this master thesis is on the LMG variable importance metric. The LMG is a metric that is based on variance decomposition. The total $\Rtwo$ of a model is decomposed onto the predictors. Marginal and conditional information are incorporated \citep{Gromping2015} .  The  formulas of this section are taken from \cite{Gromping2015}, using the same mathematical notations. 

The following notations for the explained variance \eqref{eq:evar} and sequentially added variance \eqref{eq:svar} simplify the notation of the LMG formula. 

   \begin{align} 
     \EVAR(S) = \var(Y) - \var( Y \mid X_{j}, j \in S),   \label{eq:evar} 
   \end{align} 
   \begin{align} 
     \SVAR(M \mid S) = \EVAR(M \cup S) - \EVAR(S), \label{eq:svar} 
    \end{align} where $S$ and $M$ denote disjoint sets of predictors.
    
   The LMG formula is given below for the first predictor only. Because of exchangeable predictors, this is no loss of generality.  $\Rtwo(S)$ can be written as $\EVAR(S)/\var(Y).$ 

   \begin{align} 
     \LMG(1) &= \frac{1}{p!} \sum_{\pi permutation}^{} \SVAR(\{1\} \mid S_{1}(\pi)),   \label{eq:lmgw1}  \\
     &= \frac{1}{p!} \sum_{S \subseteq \{ 2, \dots, p \} }^{} n(S)! \, (p-n(S)-1)! \, \SVAR(\{1\} \mid S) \label{eq:lmgw2}  \\
     &=  \cfrac{1}{p} \sum_{i=0}^{p-1} \left( \substack {\sum\limits_{ S \subseteq \{ 2, \dots, p \}}\\n(S)=1}^{} \ \SVAR(\{1\} \mid S)\right)\bigg/ \binom{p-1}{i}  \label{eq:lmgw3}
   \end{align}
   where $S_{1}(\pi)$ is the set of predecessors of predictor 1.
   
   The different formula writings help to better understand what the calculation is about in the LMG metric. The $\Rtwo$ of the model including all predictors is decomposed. In the formula on the top \eqref{eq:lmgw1}, the LMG value of predictor 1 is represented as an unweighted average over all orderings of the sequential added variance contribution of predictor 1. The formula in the center \eqref{eq:lmgw2}, shows that the calculation can be  done more efficiently. The orderings with the same set of predecessors $S$ are combined into one summand. Instead of $p!$ summands only $2^{p-1}$ summands need to be calculated. The formula on the bottom \eqref{eq:lmgw3} shows that the LMG metric can also be seen as the unweighted average over average explained variance improvements when adding predictor 1 to a model of size $i$ without predictor 1 \citep{Gromping2015}. The LMG metric is implemented in the R package \texttt{relaimpo} \citep{Gromping2006}.
   
\cite{Chevan1991} propose that, instead of only using the variances, an appropriate goodness-of-fit metric can as well be used in the LMG formula. They name their proposal \textit{hierarchical partitioning}. The requirements are simply: an initial measure of fit when no predictor variable is present, a final measure of fit when $N$ predictor variables are present, all intermediate models when various combinations of predictor variables are present. 
  The LMG component of each variable is named \textit{independent component} (I). The sum of the independent components (I) results then in the overall goodness-of-fit metric. The difference between the goodness-of-fit when only the predictor itself is included in the model, compared to its independent component (I), is named the \textit{joint contribution} (J) \citep{Gromping2015}. Hierarchical partitioning is implemented in the \texttt{hier.part} package \citep{Walsh2015}. When  $\Rtwo$ is chosen as the goodness-of-fit measure, the LMG values are calculated. The hierarchical partitioning function of \texttt{hier.part} is used in this master thesis. The hierarchical partitioning function accepts a data frame with the $\Rtwo$ values of all submodels as input. Of note,  the partitioning function of \texttt{hier.part} is only guaranteed to work up to nine predictors and does not work at all for more than twelve predictors.
  
\section{Appropriate $\Rtwo$ definitions in the Bayesian framework}
The focus of this master thesis is on the standard linear model. For this model, the most widely used goodness-of-fit metric is $\Rtwo$.  Different formulas for $\Rtwo$ exist \cite{Kvalseth1985}, all leading to the same value when an intercept is included and the model is fitted by maximum likelihood. 

Two commonly used $\Rtwo$ definitions are:
   
      \begin{align} 
     R^2 &= 1 - \frac{\sum_{i=1}^{n}(y_{i} - \hat{y_{i}})^2}{\sum_{i=1}^{n}(y_{i}-\bar{y})^2}   \label{eq:rtwo1} \\
     R^2 &= \frac{\sum_{i=1}^{n}(\hat{y_{i}} - \bar{y})^2}{\sum_{i=1}^{n}(y_{i}-\bar{y})^2}, \qquad i = 1,\dots,n,   \label{eq:rtwo2} 
   \end{align} 
where $\hat{y_{i}} = = \E ({y \mid X_{i}, \hat{\theta}})$.  $\hat{\theta}$ is the maximum likelihood estimate of the regression coefficients.

When other estimation methods than maximum likelihood are used, equation~\eqref{eq:rtwo1} can be negative and equation~\eqref{eq:rtwo2}  can be bigger than 1. This is not uncommon in a Bayesian regression setting when samples of the posterior parameter distribution are employed. A model that explains more than 100\% of the variance is nonsense. A negative $\Rtwo$ is also difficult to interpret. A negative $\Rtwo$ may be interpreted as a fit that is worse than the mean of the data. This can make sense for predictive purposes, e.g. when new data from a test set is predicted by leave-one-out crossvalidation \citep{Alexander2015}.  For non predicting purposes, a negative $\Rtwo$ does not make sense. The aim of the LMG formula is to gain some more information about the possible association between variables. A predictor can not explain less than zero variance in the population. To respect the non-negative share property of the LMG formula, the $\Rtwo$ of submodels should not decrease when adding predictors. Both classical $\Rtwo$ definitions seem not to be well suited for the LMG metric in the Bayesian framework.

A more reasonable $\Rtwo$ definition for the LMG formula in the Bayesian framework can be found by noting that the variance of the linear model can also be written as 

      \begin{align} 
        \var(y) = \var(\X \bbeta) + \sigma^2 = \bbeta^\top \bSigma_{\X \X}  \bbeta + \sigma^2, \label{eq:vary} 
   \end{align}
where $\bbeta^\top = (\beta_{1} \dots \beta_{p})$ are the regression parameters without the intercept.
$\bSigma_{\X \X}$ is the covariance matrix of the predictors.

By using this variance definition \cite{Gelman2017} propose to use 

      \begin{align} 
       \Rtwo_{Gelman} = \frac{\var(\sum_{i=1}^{n}\hat{y}^s_{i})}{\var(\sum_{i=1}^{n}\hat{y}^s_{i})+\var(\sum_{i=1}^{n}e^s_{i})}, \qquad i = 1,\dots,n, \label{eq:rtwoGelman} 
   \end{align} 
where $\hat{y}^s_{i}  = \E \left({y \mid X_{i}, \theta^s}\right) $ and the vector of errors $e^s_{i} = y_{i} - \hat{y}^s_{i}$ and $\theta^s, s = 1,\dotsc, S$ are draws from the posterior parameter distribution. The $\Rtwo$ is then guaranteed to be between 0 and 1. The $\Rtwo$ can  be interpreted as a data-based estimate of the proportion of variance explained for new data under the assumption that the predictors are held fixed \citep{Gelman2017}.

In the Bayesian framework, the $\sigma^2$ parameter is explicitly modeled in the standard linear regression setting. Therefore, it is possible to sample the $\sigma^2$ parameter from its posterior distribution instead of defining the error as in definition \eqref{eq:rtwoGelman}, which would lead to the following definition:

    \begin{align} 
        \Rtwo &= \frac{\var(\sum_{i=1}^{n}\hat{y}^s_{i})}{\var(\sum_{i=1}^{n}\hat{y}^s_{i})+\sigma_{s}^2} \nonumber \\ &= \frac{\bbeta_{s}^\top \bSigma_{\X \X}  \bbeta_{s}}{\bbeta_{s}^\top \bSigma_{\X \X}  \bbeta_{s}+\sigma_{s}^2}, \qquad i = 1,\dots,n, \label{eq:rtwoused} 
   \end{align} 
where $\hat{y}^s_{i}  = E \left({y \mid X_{i}, \theta^s}\right) $,  and $\theta^s, s = 1,\dotsc, S$ are draws from the posterior parameter distribution.


The predictors in definition \eqref{eq:rtwoGelman} and definition \eqref{eq:rtwoused} could also be taken as random \citep{Gelman2017}. The predictors are then called stochastic predictors. Using the sample covariance estimate provides then just an estimate of the true covariance structure. With stochastic predictors, there is an additional uncertainty in the $\Rtwo$ formula that can have a large influence on the $\Rtwo$ and the LMG values.

In practice, definition \eqref{eq:rtwoused} and definition \eqref{eq:rtwoGelman} should lead to  similar values in the standard linear model. In my opinion, it is more reasonable to take the full Bayesian route by sampling  $\sigma^2$ of its posterior distribution. This approach provides the opportunity to include prior information about $\sigma^2$ directly into to $\Rtwo$ calculations. The LMG calculations in the examples of this master thesis will therefore be based on definition \eqref{eq:rtwoused}.  A benefit of definition \eqref{eq:rtwoGelman} is that it also works for generalized linear models where we often have no separate variance parameter.

The denominator of $\Rtwo$ is no longer fixed in definition \eqref{eq:rtwoGelman} and in definition \eqref{eq:rtwoused}. We can therefore no longer interpret an increase in $\Rtwo$ as an improved fit to a fixed target \citep{Gelman2017}. The unfixed denominator seems to be problematic for the LMG formula in the Bayesian framework. However, in the linear model it is possible to calculate the $\Rtwo$ of all submodels from the parameters of the model inlcuding all predictors (full-model) and the covariance matrix of the predictors. Therefore,  all submodels of a posterior sample are compared to the same fixed value. A possible way to get the $\Rtwo$ of the submodels from the full-model is shown in the next section.

\section{Use of conditional variance formula to get $\Rtwo$ of submodels}


For two predictors, definition \eqref{eq:vary} simplifies to

      \begin{align} 
        \var(y) = \beta_{1}^2 \var(X_{1}) + 2  \beta_{1}  \beta_{2} \cov(X_{1}, X_{2}) + \beta_{2}^2 \var(X_{2}) + \sigma^2, \label{eq:varx1x2} 
   \end{align}
 
 When predictor $X_{1}$ is the only one in the model, the explained variance includes the variance of the predictor itself, the whole covariance term, and some of the contribution of the variance of $X_{2}$ in equation \eqref{eq:varx1x2} additionaly . In mathematical notation, that is
 
      \begin{align} 
        \SVAR({X_{1}} \mid \emptyset ) = \beta_{1}^2 \var(X_{1}) + 2  \beta_{1}  \beta_{2} \cov(X_{1}, X_{2}) + \beta_{2}^2 \var(X_{2}) \rho_{12}^2 \text{.} \nonumber 
   \end{align}
   
The contribution of the second regressor is then simply the difference to the total explained variance \citep{Gromping2007}. 

In the general case with $p$ regressors, the conditional variance formula \eqref{eq:condvar} can be used to calculate the $\Rtwo$ of all submodels. For example, the conditional variance formula can be used to specify the conditional distribution of a multivariate normal distribution $\Y$.

The elements of the vector $\Y$ are reordered as
\begin{align*}
\mathbf{Y} = \begin{pmatrix}
\mathbf{Y}_{1} \\ 
\mathbf{Y}_{2} \end{pmatrix}, \mathbf{Y}_{1} \in \IR^q, \mathbf{Y}_{2} \in \IR^{p-q} .
\end{align*}

The joint distribution is a multivariate normal distribution with elements
\begin{align*}
\begin{pmatrix}
\mathbf{Y}_{1} \\ 
\mathbf{Y}_{2} \end{pmatrix} \sim \mathcal{N}
\Bigg(\begin{pmatrix}
\boldsymbol{\mu}_{1} \\ 
\boldsymbol{\mu}_{2} 
\end{pmatrix},
\begin{pmatrix}
\bSigma_{11} & \bSigma_{12} \\
\bSigma_{21} & \bSigma_{22} \\
\end{pmatrix}\Bigg),
\ \bSigma_{21} = \bSigma_{12}^{T},
\end{align*}
the conditional distribution is normally distributed again with mean 
\begin{align*}
\E (\mathbf{Y}_{1} | \mathbf{y}_{2} ) = \boldsymbol{\mu}_{1}\ +\ \bSigma_{12} \bSigma_{22}^{-1}(\mathbf{Y}_{2}\ -\ \boldsymbol{\mu}_{2}),
\end{align*}
and the conditional variance is
\begin{align}
\var ( \mathbf{Y}_{1} | \mathbf{y}_{2} ) = \bSigma_{11}\ -\ \bSigma_{12} \bSigma_{22}^{-1}\bSigma_{21} \text{.} \label{eq:condvar} 
\end{align}

The aim is to calculate $\Rtwo$ of a submodel containining the predictors $\X_{q...p}$, and the regression coefficients $\bbeta^\top = (\beta_{1}, \dotsc, \beta_{p})$ without the intercept. The regression coefficients are further separated in $\bbeta^\top_{1  ,\dotsc,  q-1} = (\beta_{1} ,\dotsc, \beta_{q-1})$ and $\bbeta^\top_{q ,\dotsc, p} = (\beta_{q} ,\dotsc, \beta_{p})$. 

As in the multivariate normal distribution example above, the covariance matrix of $p$ predictors is written as 

      \begin{align*} 
\cov(\X) =	\bSigma_{\X \X} = \begin{pmatrix}
\bSigma_{11} & \bSigma_{12} \\
\bSigma_{21} & \bSigma_{22}  \\
\end{pmatrix}^{p \times p}, 
   \end{align*}
   
         \begin{align*} 
   \text{where} \qquad \bSigma_{11} &= \cov(\X_{1,\dotsc, q-1}, \X_{1 ,\dotsc,q-1}), \\ \bSigma_{12} &= \cov(\X_{1,\dotsc, q-1}, \X_{q ,\dotsc, p}),\\ \bSigma_{22} &= \cov(\X_{q ,\dotsc, p}, \X_{q \dots p}) \text{.} \nonumber
      \end{align*}
      
 The conditional variance of the predictors $ \X_{1 ,\dotsc, q-1} $ given the predictors  $ \X_{q ,\dotsc, p} $ is then
 
          \begin{align*} 
 \cov(\X_{1,\dotsc, q-1} \mid \x_{q ,\dotsc, p}) = \bSigma_{11}\ -\ \bSigma_{12} \bSigma_{22}^{-1}\bSigma_{21} .
       \end{align*}
       
       The total explained variance of the full-model containing $\X_{1 \dots p}$ omits simply the $\sigma^2$ parameter in \eqref{eq:vary} , which is

      \begin{align*} 
        \EVAR(\X_{1 ,\dotsc, p}) = \bbeta^\top \bSigma_{\X \X}  \bbeta. 
   \end{align*}

The explained variance of a submodel can be calculated by subtracting the explained variance of the not-in-the-model-included-predictors that is not explained by in-the-model-included-predictors from the total explained variance. The variance that is not explained by in-the-model-included-predictors is given by the variance of the not-in-the-model-included predictors conditional on the in-the-model-included-predictors. The explained variance of a submodel containing predictors $\X_{q ,\dotsc, p}$ can therefore be written as

       \begin{align} 
       \EVAR(\X_{q \dots p}) =  \EVAR(\X_{1  ,\dotsc, p}) - \bbeta^\top_{1 ,\dotsc, q-1} \cov(\X_{1,\dotsc, {q-1}} \mid \x_{q \dots p}) \bbeta_{1 ,\dotsc, {q-1}} . \label{eq:varsub} 
   \end{align}

To gain the the $\Rtwo$ value of the submodel, it is necessary to divide the explained variance by the total variance, which is
       \begin{align*} 
\EVAR(\X_{q ,\dotsc, p}) / \var(\Y),   
\end{align*}
where $\var(\Y)$ is definied as  $\bbeta^\top \bSigma_{\X \X}  \bbeta + \sigma^2$.

A posterior density distribution is obtained for the regression parameters in the Bayesian regression setting. The LMG formula requires calculation of the $\Rtwo$ values for all $2^p-1$ submodels. Samples from the joint posterior paramters of the full-model are used to calculate the explained variance of the  submodels. For each sample, the  conditional variance formula is used to obtain the $\Rtwo$ of the $2^p-1$ submodels. The non-negative property and the dependence of the parameters from the submodels to each other is then respected for each sample. 

Instead of using the conditional mean formula to get the $\Rtwo$ of the submodels,  it would be possible to fit a separate Bayesian model for each submodel. An $\Rtwo$ distribution can easily be built for each submodel by using definition \eqref{eq:rtwoGelman} or definition \eqref{eq:rtwoused}. However, the problem is how to calculate the LMG values out of these $\Rtwo$ distributions. If we just sample independently from the $\Rtwo$ distributions, the dependence of the paramter values of the submodels to each other is ignored. We would have many possibly true parameter values of a predictor in the same LMG comparison. It would then also be possible that the $\Rtwo$ decreases when adding predictors.  Another drawback is that it would be much more time-consuming to fit a separate Bayesian model for each submodel. Using the conditional variance formula on the full-model allows to calculate LMG values in the Bayesian framework in a reasonable time exposure. Depending on the number of predictors and the number of posterior samples, the calculations still take some time in the Bayesian framework. For stochastic predictors, the computation time is multiplied by the number of covariance samples.


\section{Bayesian Regression}
The following section provides a brief introduction to Bayesian regression. It further shows that assuming stochastic or non-stochastic predictors results in the same posteriors for the regression parameters under some assumptions.  It is summarized from the book \textit{Bayesian Analysis for the Social Sciences} \citep{Jackman2009}. 

In regression analysis, we are interested in the dependence of $\y$ on $\X$. The conditional mean of a continuous response variable $\y = (y_{1}, \dots, y_{n})^\top$ is related to a $n \times k$ predictor matrix $\X$ via a linear model, 

       \begin{align*} 
\E(\y \mid \X , \bbeta) = \X \bbeta ,
   \end{align*}
where $\bbeta$ is a $k \times 1$ vector of unknown regression coefficients.

Under some assumptions about the density, conditional independence and homoskedastic variances, the regression setting can be written as

       \begin{align*} 
\y \mid \X , \bbeta, \sigma^2 \sim \mathcal{N}(\X \bbeta , \sigma^2 \I_{n}) \text{.}
   \end{align*}

Under the assumption of weak exogeneity and conditional independence, the joint density of the data can be written as

       \begin{align*} 
p(\y, \X \mid \btheta) = p(\y \mid \X, \btheta_{y \mid x}) \, p(\X \mid \btheta_{x}),
   \end{align*}
where $\btheta = (\btheta_{y \mid x}, \btheta_{x})^\top$. 

The weak exogeneity assumption implicates that the whole information about $\y_{i}$ is contained in $x_{i}$ and $\btheta_{y \mid x}$. Knowledge of the parameters $\btheta_{x_{i}}$ provides no additional information about $\y_{i}$.
The interest of regression is mostly on the posterior parameters $\btheta_{y \mid x}$. These posterior densities are proportional to the likelihood of the data  multiplied by the prior density. The joint density $p(\y, \X \mid  \btheta)$ is used to learn about the posterior parameters, via Bayes Rule

       \begin{align*} 
p(\btheta \mid \y, \X) \propto p(\y, \X \mid  \btheta) \, p(\btheta).
   \end{align*}
   
   The dependence of $\y$ on $\X$ is captured in the parameters $\btheta_{y \mid x} = (\beta, \sigma^2)$. Under the assumption of independent prior densities about $\btheta_{y \mid x}$ and $\btheta_{x}$ the posterior distribution of the parameters can be written as
   
          \begin{align} 
p(\bbeta, \sigma^2, \btheta_{x} \mid \y, \X) = \frac{p(\y \mid \X, \bbeta, \sigma^2) \, p(\bbeta, \sigma^2)}{p(\y \mid \X)} \times \frac{p(\X \mid \btheta_{x}) \, p(\btheta_{x})}{p(\X)}.  \label{eq:bayesf} 
   \end{align}
   
  The factorization in equation \ref{eq:bayesf} shows, that under the above mentioned assumptions, the posterior inference about the parameters $\btheta_{y \mid x} = (\beta, \sigma^2)$ is independent from the inference about $\btheta_{x}$ given data $\X$. This also means that the assumptions about $\X$ being non-stochastic or stochastic result in the same posterior density of  $\btheta_{y \mid x}$. In the case of non-stochastic regressors, $p(\X)$ and $\btheta_{x}$ drop out of the calculations. For stochastic predictors, it means, that given $\X$, nothing more can be gained about $\theta_{y \mid x} = (\bbeta, \sigma^2)$ from knowing $\btheta_{x}$. 
  
  The focus of regression is on $\btheta_{y \mid x} = (\bbeta, \sigma^2)$, for which it does not matter whether we assume fixed or stochastic predictors under the above mentioned assumptions. The variance of the predictors is also incorporated in the LMG formula. The LMG formula may be especially interesting for continuous predictors, which often are of stochastic nature. \cite{Gromping2006} recommends in most cases to use the non fixed regressor option when calculating bootstrap confidence intervals. Therefore, the information about $\btheta_{x}$ would  also be relevant for stochastic regressors.  As seen in equation \eqref{eq:bayesf}, inference about $\btheta_{x}$  is independent from inference about $\btheta_{y \mid x}$. If there are stochastic predictors and we use the sample estimate of the covariance matrix, we do not incorporate the uncertainty of the estimate.  Because the explained variance is calculated by $\bbeta^\top \bSigma_{\X \X}  \bbeta$, inference about  $\btheta_{x}$  seems to be equally important as inference about $\btheta_{y \mid x}$ for stochastic predictors. If the distribution of the $p(\X)$ is known, the $\btheta_{x}$ could be estimated. However, the computation time is then much higher, because the whole LMG calculation need to done for each posterior covariance sample of the predictors. Depending on the number of predictors this would be very time-consuming. In most cases, the problem is that the distribution of the $\X$ is unknown. As a practical solution,  nonparametric bootstrapping of the covariance matrix could be used to include the uncertainty of the stochastic predictors in the LMG calculations. Again, it would be necessary to do the LMG calculations for each bootstrap sample of the covariance matrix. There exist also different covariance estimators. The shrinkage method may be an interesting estimator with some nice properties \citep{Schafer2005}. 
  
  
  
 



% LaTeX file for Chapter 03












\chapter{Examples}

The following chapter presents the Bayesian LMG implementation by two examples. Simulated data is used for the first example. Empirical data is used for the second example.

\section{Simulated data}

We assume a simple model for the first example: 
\begin{align*} 
&Y_{i} \sim \mathcal{N}(\beta_{0}+x_{1} \beta_{1}+x_{2} \beta_{2}+x_{3} \beta_{3}+x_{4} \beta_{4}, \, \sigma^2),& \\ & \beta_{1} = 0.5, , \beta_{2} = 1, \, \beta_{3} = 2 , \, \beta_{4}=0, \, \sigma^2 = 1 & \\ & \X_{1}, \X_{2},\X_{3},\X_{4} \sim \mathcal{N}(0, 1) 
\end{align*} 
The values of the four predictors are sampled from a standard normal distribution. These values are then multiplied by the regression coefficients. A standard normal distributed error is added to obtain the dependent variable. Fifty observations were sampled. The data generating R-code can be found in the Appendix~\ref{r03:simdata.ex1}.




The model is fitted using the \texttt{rstanarm} package \citep{rstanarm} with the default priors for the regression and $\sigma^2$ parameters. The exact command can be found in R-code~\ref{r03:model.ex1} These default priors are called 'weakly informative priors', because they take into account the order of magnitude of the variables by using the variance of the observed data. Information about these priors can be found in \cite{stanM2017}. A burn-in period of 20000, a sample size of 20000, and a thinning of 20 were chosen, resulting in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:simdata.postsample2}. 

For each posterior sample of the parameters, the $\Rtwo$ value was calculated. The $\Rtwo$ of the submodels was then calculated by the conditional variance formula for each posterior sample. The R-code is found in Appendix \ref{r03:LMG}. The resulting $\Rtwo$ values of the first few posterior samples are shown in Table~\ref{tab:simdata.postsample3}.  The thinning is reasonable in this case to reduce the computational burden and to still obtain an appropriate posterior of the $\Rtwo$ values \citep{Link2012}. 

The \texttt{hier.part} package was used to calculate the LMG value for each posterior sample. The LMG posteriors are shown in Table~\ref{tbl:nonstochEx1} . The independent component (I) represents the LMG value. The joint contribution (J) represents the difference from the independent component to the explained variance of the model containing only the predictor itself (T). Assuming stochastic or non-stochastic regressors has an influence on the uncertainty of the LMG values. 

At first, non-stochastic regressors were assumed. The resulting LMG values and joint contributions with a 95\% credible interval are shown in Table~\ref{tbl:nonstochEx1}. An option to display the resulting LMG distribution is shown in Figure~\ref{fig:simdata.LMG.plot}.  Using the default weakly informative priors, the LMG distributions obtained from the Bayesian framework were very similar to the bootstrap confidence intervals, assuming non-stochastic predictors of the LMG estimates obtained from the \texttt{relaimpo} package, as shown in Table~\ref{tbl:nonstochEx1relamip}. 





\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{table}

\caption{\label{tab:simdata.postsample2}Samples from the posterior distributions of the regression parameters.}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
  & x1 & x2 & x3 & x4 & sigma\\
\midrule
sample 1 & 0.873 & 1.126 & 0.952 & 1.049 & 0.994\\
sample 2 & 0.971 & 1.066 & 0.743 & 1.194 & 1.050\\
sample 3 & 1.109 & 0.953 & 0.840 & 1.124 & 0.915\\
sample 4 & 1.190 & 1.117 & 1.016 & 0.896 & 1.114\\
sample 5 & 1.017 & 0.937 & 0.971 & 1.155 & 0.829\\
sample 6 & 0.797 & 0.792 & 1.069 & 1.045 & 0.927\\
sample 7 & 0.945 & 0.894 & 1.081 & 1.150 & 0.998\\
sample 8 & 0.783 & 1.121 & 0.792 & 1.192 & 0.774\\
sample 9 & 0.887 & 1.134 & 1.109 & 1.162 & 0.879\\
sample 10 & 0.780 & 0.821 & 0.852 & 1.006 & 0.913\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{table}

\caption{\label{tab:simdata.postsample3}$ \Rtwo$ for all submodels for the first six posterior samples.}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & sample 1 & sample 2 & sample 3 & sample 4 & sample 5 & sample 6\\
\midrule
none & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
x1 & 0.317 & 0.340 & 0.423 & 0.418 & 0.396 & 0.324\\
x2 & 0.191 & 0.161 & 0.131 & 0.162 & 0.129 & 0.106\\
x3 & 0.182 & 0.110 & 0.151 & 0.203 & 0.191 & 0.247\\
x4 & 0.235 & 0.306 & 0.280 & 0.158 & 0.290 & 0.256\\
x1 x2 & 0.497 & 0.491 & 0.544 & 0.568 & 0.515 & 0.422\\
x1 x3 & 0.418 & 0.387 & 0.491 & 0.523 & 0.495 & 0.475\\
x1 x4 & 0.483 & 0.564 & 0.616 & 0.511 & 0.600 & 0.507\\
x2 x3 & 0.372 & 0.271 & 0.282 & 0.364 & 0.320 & 0.353\\
x2 x4 & 0.437 & 0.479 & 0.422 & 0.328 & 0.429 & 0.371\\
x3 x4 & 0.468 & 0.462 & 0.483 & 0.405 & 0.539 & 0.567\\
x1 x2 x3 & 0.599 & 0.538 & 0.612 & 0.674 & 0.615 & 0.574\\
x1 x2 x4 & 0.673 & 0.725 & 0.746 & 0.669 & 0.728 & 0.613\\
x1 x3 x4 & 0.628 & 0.647 & 0.723 & 0.650 & 0.747 & 0.715\\
x2 x3 x4 & 0.671 & 0.635 & 0.625 & 0.576 & 0.679 & 0.682\\
all & 0.821 & 0.810 & 0.855 & 0.810 & 0.878 & 0.823\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}







\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch03_figsimdata_LMG_plot-1} 

}

\caption[LMG distributions of the predictors]{LMG distributions of the predictors.}\label{fig:simdata.LMG.plot}
\end{figure}


\end{knitrout}

\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor model.}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.253 (0.163, 0.353)  & 0.087 (0.071, 0.099)   & 0.34 (0.235, 0.445)  \\ 
x2 & 0.152 (0.084, 0.239)  & \ensuremath{-5.633\times 10^{-4}} (-0.002, 0.001)   & 0.152 (0.083, 0.24)  \\ 
x3 & 0.165 (0.091, 0.251)  & 0.016 (0.005, 0.028)   & 0.182 (0.102, 0.272)  \\ 
x4 & 0.258 (0.163, 0.359)  & 0.008 (-0.003, 0.018)   & 0.266 (0.169, 0.368)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1}
\end{table}




\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor only model.}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
x1 & 0.258 (0.172, 0.353)  & 0.253 (0.163, 0.353)  \\ 
x2 & 0.157 (0.09, 0.236)  & 0.152 (0.084, 0.239)   \\ 
x3 & 0.169 (0.098, 0.254)  & 0.165 (0.091, 0.251)  \\ 
x4 & 0.266 (0.175, 0.368) & 0.258 (0.163, 0.359)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relamip}
\end{table}

\FloatBarrier


In this example, we know, that the predictor values were sampled from a normal distribution. It would therefore be more reasonable to assume stochastic predictors. Under the assumption of weak exogeinity and conditional independence, the posterior distributions of the regression parameters $\bbeta$ are valid for non-stochastic and stochastic predictors. However, the uncertainty about the LMG values needs to include the uncertainty about the covariance matrix. If we know the distribution of the predictors we can incorporate this information and obtain the posterior distribution of the covariance matrix. The package \texttt{Jags} was used for inference about the covariance matrix in a Bayesian way. The R-code of the covariance inference can be found in Appendix~\ref{r03:covminf} and the R-code to get the $\Rtwo$ values can be found in Appendix~\ref{r03:LMG.covm}. As an alternative, non-parametric bootstrap was used for inference about the covariance matrix. The R-code of the adapted LMG function can be found in Appendix~\ref{r03:LMG.boot}.

Using the bootstrap samples of the covariance matrix or samples from the posterior covariance matrix produced  very similar LMG distributions. Bootstrap seems to be a valuable option for stochastic predictors when the distribution of the predictors is unknown. Even when the distribution is known, the difference seems to be tiny. A benefit of going the full Bayesian way is that we can also include prior knowledge about the covariance matrix. Using the default priors further produced very similar LMG distribution as using the non-parametric bootstrap option of the \texttt{relaimpo} package. Table~\ref{tbl:nonstochEx1relaimpstoch} shows the LMG values of these approaches. For stochastic predictors, in contrast to non-stochastic predictors, the uncertainty about the covariance matrix is reflected in the larger credible intervals. Even when the exact regression parameters were known, there would a lot of uncertainty in the LMG values caused by the uncertainty about the covariance matrix. 








\begin{table}[h]
\caption{LMG values assuming stochastic predictors with 95\% CI.}
\centering
\begin{tabular}{clll}
   \toprule
  & \multicolumn{1}{c}{\textbf{Relaimpo}} & \multicolumn{2}{c}{\textbf{Bayesian framework}} \\ \cmidrule(r){2-2}\cmidrule(l){3-4}
 \textbf{Variable} &  & \multicolumn{1}{c}{nonparameteric bootstrap}& \multicolumn{1}{c}{covariance inference} \\
 \midrule
x1 & 0.258 (0.14, 0.391)  & 0.25 (0.124, 0.391) &  0.234 (0.126, 0.344) \\ 
x2 & 0.157 (0.059, 0.271)  & 0.138 (0.053, 0.241)  & 0.163 (0.062, 0.265) \\ 
x3 & 0.169 (0.078, 0.281)  & 0.152 (0.043, 0.265)  & 0.159 (0.068, 0.269) \\ 
x4 & 0.266 (0.143, 0.413) & 0.257 (0.157, 0.373) & 0.276 (0.158, 0.417) \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relaimpstoch}
\end{table}







\clearpage

\section{Empirical data}
In the following section, the Bayesian LMG implementation is applied on an empirical dataset containing test scores of pupils (N = 301) from a study by \cite{Holzinger1939} available in the R package \texttt{MBESS} \citep{MBESS}. This dataset was used in \cite{Nimon2008} to present commonality analysis, which is another variance decomposition technique. Scores from a paragraph comprehension test (paragrap) were predicted by four verbal tests:  general-information (general),  sentence-comprehension (sentence) ,  word-classification (wordc), and  word-meaning (wordm) (Table~\ref{table:hs.data}). 

The aim of the regression analysis was to determine the  association between verbal ability and paragraph comprehension.  
An overview of the data is shown in Figure~\ref{fig:empi.lmg.plot}. The regression results are shown in Table~\ref{tbl:empirical.reg}). A novice researcher may wrongly conclude, that there is little association between the "non-significant" predictors (general information and word-classification) and paragraph comprehension. Given the other predictors are already included in the model, the predictors seem not to provide  much information about the expected paragraph comprehension ability. However, it should not be concluded from this regression table, that the association between any of these "non-significant" predictors and the dependent variable is unimportant. As shown in Figure~\ref{fig:empi.lmg.plot}, the correlations between the predictors are rather high. The LMG metric may therefore provide new information about the importance of each predictor. 

The Bayesian regression model was fitted in \texttt{rstanarm}. The default priors were used for the regression coefficients and the $\sigma^2$ parameter. A burn-in period of 20000, a sample size of 20000, and a thinning of 20 resulted in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:empirical.data.postsample2}. The resulting $\Rtwo$ of these posterior samples are shown in Table~\ref{tab:empirical.data.postsample3}. The LMG values were calculated by using \texttt{hier.part}. The  independent component (I), joint contribution (J), and total explained variance in a one-predictor model (T) are shown in Table~\ref{tbl:empirical.ijt}. Sentence-comprehension and word-meaning seem to be the most important predictors by applying the LMG metric. However, none of the predictors seem to be unimportant.  The joint contributions of each predictor were quite large.

For comparison purposes, the LMG metric was additionally calculated with the \texttt{relaimpo} package using parametric bootstrapping. The confidence intervals of  \texttt{relaimpo} were almost identical to the credible intervals of the Bayesian framework (Table~\ref{tbl:empirical.relaimp.comp}). Assuming stochastic or non-stochastic predictors resulted also in almost identical uncertainty estimates with such a large sample size (Table\ref{tbl:empirical.relaimp.comp.stoch}). 







\begin{table}
\centering
\caption{Variable description of empirical data set}
\begin{tabular}{l l}
  \toprule			
  Variable Name & Description  \\   \midrule  
  paragrap & scores on paragraph comprehension test  \\
  general & scores on general information test \\
  sentence & scores on sentence completion test\\
  wordc & scores on word classification test \\
  wordm & scores on word meaning test \\
  \bottomrule  
\end{tabular}
\label{table:hs.data}
\end{table}

\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch03_figunnamed-chunk-1-1} 

}

\caption[Test scores from Holzinger and Swineford's  (1939) Study]{Test scores from Holzinger and Swineford's  (1939) Study. N=301.}\label{fig:unnamed-chunk-1}
\end{figure}






% latex table generated in R 3.4.3 by xtable 1.8-2 package
% Fri Aug 10 23:26:21 2018
\begin{table}[!h]
\centering
\caption{Regression of paragraph comprehension on verbal tests.} 
\label{tbl:empirical.reg}
\begingroup\footnotesize
\begin{tabular}{rrrr}
  \hline
 & Coefficient & 95\%-confidence interval & $p$-value \\ 
  \hline
Intercept & 0.071 & from -1.17 to 1.31 & 0.91 \\ 
  general & 0.03 & from -0.00 to 0.06 & 0.084 \\ 
  sentence & 0.26 & from 0.18 to 0.34 & $<$ 0.0001 \\ 
  wordc & 0.047 & from -0.01 to 0.11 & 0.14 \\ 
  wordm & 0.14 & from 0.08 to 0.19 & $<$ 0.0001 \\ 
   \hline
\end{tabular}
\endgroup
\end{table}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{table}

\caption{\label{tab:empirical.data.postsample2}Samples from the posterior distributions of the regression parameters.}
\centering
\begin{tabular}[t]{lrrrrr}
\toprule
  & general & sentence & wordc & wordm & sigma\\
\midrule
sample 1 & 0.017 & 0.275 & 0.072 & 0.136 & 2.15\\
sample 2 & 0.050 & 0.260 & 0.042 & 0.100 & 2.05\\
sample 3 & 0.024 & 0.291 & 0.083 & 0.135 & 2.28\\
sample 4 & -0.003 & 0.259 & 0.017 & 0.195 & 2.31\\
sample 5 & -0.006 & 0.344 & 0.043 & 0.120 & 2.26\\
sample 6 & 0.043 & 0.201 & 0.059 & 0.163 & 2.23\\
sample 7 & 0.057 & 0.230 & 0.015 & 0.138 & 2.25\\
sample 8 & 0.048 & 0.287 & 0.092 & 0.105 & 2.16\\
sample 9 & 0.028 & 0.268 & 0.058 & 0.123 & 2.19\\
sample 10 & 0.030 & 0.362 & 0.019 & 0.080 & 2.05\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{table}

\caption{\label{tab:empirical.data.postsample3}$ \Rtwo$ for all submodels for the first six posterior samples.}
\centering
\begin{tabular}[t]{lrrrrrr}
\toprule
  & sample 1 & sample 2 & sample 3 & sample 4 & sample 5 & sample 6\\
\midrule
none & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
general & 0.421 & 0.473 & 0.431 & 0.353 & 0.347 & 0.456\\
sentence & 0.553 & 0.555 & 0.556 & 0.483 & 0.540 & 0.504\\
wordc & 0.364 & 0.350 & 0.373 & 0.273 & 0.309 & 0.344\\
wordm & 0.496 & 0.484 & 0.492 & 0.497 & 0.434 & 0.521\\
general sentence & 0.580 & 0.604 & 0.587 & 0.502 & 0.547 & 0.561\\
general wordc & 0.481 & 0.512 & 0.493 & 0.388 & 0.401 & 0.497\\
general wordm & 0.531 & 0.549 & 0.532 & 0.508 & 0.456 & 0.564\\
sentence wordc & 0.572 & 0.570 & 0.578 & 0.489 & 0.546 & 0.525\\
sentence wordm & 0.612 & 0.608 & 0.612 & 0.570 & 0.575 & 0.596\\
wordc wordm & 0.553 & 0.537 & 0.554 & 0.516 & 0.479 & 0.563\\
general sentence wordc & 0.589 & 0.607 & 0.596 & 0.503 & 0.551 & 0.567\\
general sentence wordm & 0.616 & 0.624 & 0.617 & 0.570 & 0.575 & 0.608\\
general wordc wordm & 0.564 & 0.570 & 0.567 & 0.519 & 0.484 & 0.582\\
sentence wordc wordm & 0.621 & 0.614 & 0.623 & 0.570 & 0.577 & 0.604\\
all & 0.622 & 0.626 & 0.625 & 0.570 & 0.577 & 0.612\\
\bottomrule
\end{tabular}
\end{table}


\end{knitrout}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch03_figempi_lmg_plot-1} 

}

\caption[LMG distributions of the different verbal ability predictors]{LMG distributions of the different verbal ability predictors.}\label{fig:empi.lmg.plot}
\end{figure}


\end{knitrout}


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor only model.}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
general & 0.13 (0.104, 0.163)  & 0.298 (0.258, 0.336)   & 0.428 (0.362, 0.495)  \\ 
sentence & 0.205 (0.167, 0.245)  & 0.326 (0.294, 0.359)   & 0.532 (0.468, 0.593)  \\ 
wordc & 0.095 (0.072, 0.122)  & 0.239 (0.197, 0.273)   & 0.335 (0.269, 0.394)  \\ 
wordm & 0.176 (0.14, 0.213)  & 0.314 (0.278, 0.346)   & 0.491 (0.423, 0.553)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.ijt}
\end{table}


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
general & 0.131 (0.107, 0.16)  & 0.13 (0.104, 0.163)  \\ 
sentence & 0.206 (0.17, 0.251)  & 0.205 (0.167, 0.245)   \\ 
wordc & 0.096 (0.073, 0.127)  & 0.095 (0.072, 0.122)  \\ 
wordm & 0.178 (0.142, 0.218) & 0.176 (0.14, 0.213)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp}
\end{table}





\begin{table}[h]
\caption{Variance decomposition for stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor only model.}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
general & 0.131 (0.108, 0.164)  &  0.132 (0.095, 0.166)  \\ 
sentence & 0.206 (0.167, 0.248)  &  0.203 (0.164, 0.244)   \\ 
wordc & 0.096 (0.073, 0.128)  &  0.099 (0.068, 0.129)  \\ 
wordm & 0.178 (0.145, 0.218) &  0.176 (0.139, 0.214) \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp.stoch}
\end{table}






% LaTeX file for Chapter 04











\chapter{Extension to longitudinal data}

Some extensions of the LMG formula beyond the simple linear regression model are shown in the following chapter. The focus is on repeated measures model. These models extend the simple linear regression by allowing intra-subject correlation between repeated measures.

The dependence of within-subject measurements can be modeled by including random effects (mixed model) or by assuming correlated errors within a subject (marginal model). A mixed model can be extended by including a random slope per subject, allowing for less restrictive longitudinal shapes. Different covariance matrices of the error terms allow for less restrictions in the marginal approach. An unstructured covariance matrix, where no restriction are imposed, allows for the most freedom in the error term. However, depending on the number of repeated measurements and the sample size, the covariance matrix can get too large to make reasonable inference about it \citep{Fitzmaurice2011}. 

The extension of the LMG formula in the Bayesian framework applied to longitudinal models is restricted to models where the conditional variance formula can  be applied to get the explained variance of the submodel from the regression parameters of the full-model. Therefore, the focus is on the fixed predictors, but not on the random effects. The conditional variance formula can be used in the marginal models, because only the fixed effects are modeled anyway. In the mixed model framework, the conditional variance formula is applicable to random intercept models. For random-slope models, there are at least some difficulties involved - if it is possible at all - to get the explained variance of the submodels. 

In this chapter, the Bayesian LMG Implementation is shown on a random intercept model and on a repeated measurement model with an unstructured covariance matrix.  

\section{Random intercept model}
The first example concerns a simple random intercept model with time-varying predictors.  Different $\Rtwo$ metrics exist for linear mixed models. The variance of a random intercept model with regression parameter $\bbeta$ can be written as

      \begin{align} 
        \var(y) = \sigma_{f}^2  + \sigma_{\alpha}^2 + \sigma_{\epsilon}^2, \label{eq:var.t.ri} 
        \end{align}

where $\sigma_{f}^2 = \var(\X \bbeta) = \bbeta^\top \bSigma_{\X \X}  \bbeta$ , $\sigma_{\alpha}^2 $ is the variance of the random intercept, and $\sigma_{\epsilon}^2$ represents the error variance \citep{Nakagawa2013}. 

An $\Rtwo$ that is guaranteed to be positive is defined in \cite{Nakagawa2013} as

   \begin{align} 
\Rtwo_{\text{LMM}} = \frac{\sigma_{f}^2}{\sigma_{f}^2 + \sigma_{\alpha}^2 + \sigma_{\epsilon}^2}.
\end{align}


Theoretically, it is possible that the $\Rtwo_{\text{LMM}}$ decreases when adding predictors \citep{Nakagawa2013}.  By adding predictors, $\sigma_{f}^2$  should always increase and  $\sigma_{\epsilon}^2$  should decrease. However, the $\sigma_{\alpha}^2$ may also increase minimally and the total $\Rtwo$ may then be minimally lower. The $\Rtwo$ can not decrease by using the conditional variance formula on the full-model to calculate the $\Rtwo$ of the submodels, because the total variance is fixed. Equal results should be obtained, by fitting a new model by maximum likelihood for each submodel and by comparing the explained variance of the fixed effects to the total variance of the full-model.  In the Bayesian framework, the conditional variance formula is necessary to account for the mutual interdependence of the submodels. The total variance of the full-model can be calculated as  $\var(y) = \var(\X \bbeta + \Z b) + \sigma^2$ or by using samples of $\sigma_{\alpha}^2$ as in definition \eqref{eq:var.t.ri}. The error term could again be sampled or calculated as in definition \eqref{eq:rtwoGelman}. In the following examples, definition \eqref{eq:var.t.ri} was used,  $\sigma_{\alpha}^2$ and $\sigma_{\epsilon}^2$ were sampled from their posterior distribution.

In repeated measures studies, the focus is often on within-subject changes. The between-subject variance, estimated with the random intercept term, is of minor importance. The important question is, how much variance the fixed predictors do explain, compared to the variance of the within subject error, which is

   \begin{align} 
\Rtwo_{\text{repeated}} = \frac{\sigma_{f}^2}{\sigma_{f}^2  + \sigma_{\epsilon}^2}, \label{eq:rtwo.repeated}
\end{align}

The square root of this term is known under the name \textit{correlation within subjects} in \cite{Bland1995}. Often, there are between-subject and within-subject predictors in a model. If the interest lies in the within-subject effects,  a model including only the between-subject predictors can be used as the null-model.

The following example shows a simple random intercept model with time-varying predictors. The main question is which within-subject predictors are the most important ones. The between-subject variance is of minor importance. 

The data are simulated from the following regression setting with $m = 4$ timepoints and $n = 20$ number of subjects (see Appendix~\ref{r04:simdatri} for R-code),

\begin{align*} 
Y_{i,j} \sim \mathcal{N}(\beta_{0}+x_{1_{i,j}} \beta_{1}+x_{2_{i,j}} \beta_{2}+x_{3_{i,j}} \beta_{3}+x_{4_{i,j}} \beta_{4} + \alpha_{i}, \, \sigma^2), \qquad &i = 1, \dots, n \\  &j = 1, \dots, m
\end{align*} 

where $\beta_{1} = 1 \,$,  $\beta_{2} = 1 \,$,   $\beta_{3} = 2 \,$  $\beta_{4}=2 \,$, $\sigma^2 = 1 \, $, $\alpha_{i} \sim \mathcal{N}(0, \sigma_{\alpha}^2) \,$, $\X \sim \mathcal{N}(\0, \bSigma)$.



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch04_fig_repeated_plot_ri-1} 

}

\caption[Individual trajectories of simulated random intercept model]{Individual trajectories of simulated random intercept model.}\label{fig:.repeated.plot.ri}
\end{figure}


\end{knitrout}


 The random intercept effect was of minor interest. The $\Rtwo$ of the models was calculated according to the formula of repeated measure correlation \eqref{eq:rtwo.repeated}. The R-code of the odel and the function can be found in the Appendix (\ref{r04:model.ri} and \ref{r04:LMG.ri}). Most of the within-subject variance was explained by the predictors (Table~\ref{tbl:repeatedcormod}). The credible intervals were very small. For information about the between-subject variance term, we can look at the posterior distribution of the random intercept variance term.

Next, the random intercept was directly included in the total variance calculation of the $\Rtwo$ values. There was a large between-subject variance in this simulated dataset (Table~\ref{tbl:repeatedcormod.tot}). Therefore, the LMG values including the between subject variance were much lower. The credible intervals were as well much larger, because the uncertainty about the between-subject variance was included. 

In my opinion, we can get more useful information from separating the between-subject variance from the within-subject variance components in this simple case. Note, that we assumed non-stochastic predictors. Otherwise, the credible intervals were larger. In general, it seems more reasonable to assume stochastic time-varying predictors. The variance could then be estimated by non-parametric bootstrap, resampling whole subjects (all repeated measurements of a subject).







\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.197 (0.195, 0.199)  & 0.489 (0.487, 0.491)   & 0.686 (0.682, 0.69)  \\ 
x2 & 0.208 (0.206, 0.21)  & 0.515 (0.513, 0.517)   & 0.723 (0.719, 0.727)  \\ 
x3 & 0.305 (0.303, 0.306)  & 0.647 (0.647, 0.647)   & 0.952 (0.95, 0.954)  \\ 
x4 & 0.29 (0.288, 0.291)  & 0.623 (0.623, 0.624)   & 0.913 (0.911, 0.915)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:repeatedcormod}
\end{table}




\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values,   J~=~joint~contribution, Total~=~total explained variance in one-predictor model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.042 (0.012, 0.107)  & 0.112 (0.028, 0.233)   & 0.154 (0.04, 0.337)  \\ 
x2 & 0.056 (0.022, 0.096)  & 0.129 (0.047, 0.233)   & 0.184 (0.069, 0.329)  \\ 
x3 & 0.074 (0.031, 0.127)  & 0.158 (0.058, 0.286)   & 0.232 (0.089, 0.412)  \\ 
x4 & 0.075 (0.032, 0.126)  & 0.154 (0.057, 0.274)   & 0.23 (0.091, 0.398)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:repeatedcormod.tot}
\end{table}



\section{Marginal  model}

The next example is about a repeated measurement model with time-varying predictors and an unstructured error covariance matrix. The data are generated from the following model

\begin{align} 
&Y_{i} \sim \mathcal{N}(\X_{i} \bbeta, \bSigma), \qquad i = 1, \dots, n ,
\end{align} 

where $\bSigma$ represents an unstructured error covariance matrix, $\X_{i}$ represents the predictor matrix of size $m \times p$ of subject $i$.

The R-code of the data generation can be found in Appendix~\ref{r04:rmarg.data}. In the variance calculation, it is necessary to take into account that there is not just one $\sigma^2$ parameter, but a covariance matrix $\bSigma$. The diagonal elements of $\bSigma$ represent the variance of each timepoint. The sum of the diagonal elements of $\bSigma$ represents the variance for a whole subject. The mean can be taken of $\diag(\bSigma)$ to make the formula compatible with the $\bbeta^\top \bSigma_{\X \X}  \bbeta$ of \eqref{eq:rtwoused}, resulting in the total variance term

      \begin{align} 
        \var(\Y) = \bbeta^\top \bSigma_{\X \X}  \bbeta + \text{mean}(\diag(\bSigma)),
   \end{align}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\textwidth-3cm]{figure/ch04_figsimdata_repeated_unstruct_plot-1} 

}

\caption[Individual trajectories of simulated data with unstructured error covariance matrix]{Individual trajectories of simulated data with unstructured error covariance matrix.}\label{fig:simdata.repeated.unstruct.plot}
\end{figure}


\end{knitrout}




The individual trajectories are shown in Figure~\ref{fig:simdata.repeated.unstruct.plot}. The R-code for the model and LMG implementation can be found in Appendix (\ref{r04:model.unstruct} and  \ref{r04:LMG.rmarg}). The resulting LMG values of the predictors are shown in Table~\ref{tbl:repeated.unstructured}.

\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I~=~LMG~values, J~=~joint~contribution, Total~=~total explained variance in one-predictor model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
x1 & 0.11 (0.082, 0.131)  & 0.284 (0.215, 0.329)   & 0.394 (0.298, 0.459)  \\ 
x2 & 0.115 (0.086, 0.137)  & 0.301 (0.227, 0.348)   & 0.417 (0.312, 0.484)  \\ 
x3 & 0.196 (0.149, 0.229)  & 0.396 (0.305, 0.453)   & 0.592 (0.451, 0.679)  \\ 
x4 & 0.197 (0.149, 0.23)  & 0.399 (0.307, 0.456)   & 0.596 (0.455, 0.683)  \\ 
   \bottomrule
\end{tabular}
\label{tbl:repeated.unstructured}
\end{table}





% LaTeX file for Chapter 05






\chapter{Discussion and Conclusion}

\section{Other variable importance metrics in the Bayesian framework}

There exists different variable importance metrics \citep{Gromping2015}. The focus of this master thesis was on the LMG formula.  A lot of variable importance metrics are based on the $\Rtwo$ of the full model compared to the submodels. Instead of the LMG formula, another variable importance metric could as well be used on the $\Rtwo$ of all  submodels for each posterior sample. Commonality analysis \citep{Nimon2008} and dominance analysis \citep{Gromping2015} seem to be interesting variance decomposition metrics. Both metrics provide some extensions to the LMG framework. Moreover, different options exist for the description of the LMG distribution. For example, the \texttt{relaimpo} package provides more bootstrap options, like pairwise differences, that could easily be transferred to the Bayesian framework. 
 
\section{Conclusion}

The Bayesian framework provides the option to include the uncertainty about parameters.  Using  the conditional variance formula allows to calculate the $\Rtwo$ of all the submodels from the posterior parameter distributions of the full model. Instead of fitting $2^{p}-1$ models, only the full-model needs to be fitted. The mutual interdependence of the submodels is then automatically respected. The $\Rtwo$ of the submodels do not decrease when adding predictors. The important property of non-negativity shares is then respected in the Bayesian framework.

A disadvantage about calculating the $\Rtwo$ of all the submodels with the conditional variance formula seems to be the restriction to the linear model. Although, this may be a topic of further research. Another disadvantage of the Bayesian framework, compared to the classical LMG implementation, are higher computational costs. The calculations are still possible in a reasonable time period for non-stochastic predictors. Parallel computing could be used to speed up the calculations. For stochastic predictors, the computation time are much higher than in the classical framework.

Assuming non-stochastic or stochastic predictors can have a big impact in small samples on the uncertainty of the explained variance and on the LMG values. Although the posterior regression parameter distributions are the same in both cases (under the assumptions described in chapter 2), the explained variance of a model is directly dependent on the covariance matrix. Inference about the covariance of the predictors $\X$ is therefore an important part when stochastic predictors are assumed. However, this does not seem to be an easy problem in general. Non-parametric bootstrap of the covariance matrix provides a practical solution in the Bayesian framework. 

 When the sample size is large enough, the classical and the Bayesian framework should lead to very similar values. The Bayesian framework allows including prior information, that may especially be relevant for small sample sizes. The credible interval of the Bayesian framework, in contrast to the confidence intervals, may further be easier to interpret in the mathematically correct way.
 
A lot of studies are concerned with within-subject changes. The extension of the LMG formula to those kinds of problems is not straightforward. It is depending on the complexity of the data. However, the extension seems to be easily possible when the focus is on the fixed effects for the simple random intercept model. In the Bayesian framework and in the maximum likelihood framework, it seems reasonable to compare only the explained variance of the fixed effects of the submodels against the total variance, containing $\sigma_{f}^2 + \sigma_{\alpha}^2 + \sigma_{\epsilon}^2$. Otherwise, there may be problems with the non-negativity property of the shares. 









\cleardoublepage

\appendix


\chapter{Additional notes and R-codes}

The error term in the definition of the $\Rtwo$ proposed by \cite{Gelman2017} is defined as $\var(\sum_{i=1}^{n}e^s_{n})$. I think we could also use $ \sum(y - \hat{y}^s)^2/(n-1) $ as an estimate of the error. By using the maximum likelihood estimate,  $\var(y_{i} - \hat{y_{i}}) = \sum (y_{i} - \hat{y_{i}})^2/ (n-1) $,  because the mean of the residuals is 0. When  samples of the posterior parameters are used, the mean of the residuals is not exactly zero. $\var(y_{i} - \hat{y_{i}}) = \sum (y_{i} - \hat{y_{i}})^2/ (n-1) $ is than a little bit larger than $\var(y_{i} - \hat{y_{i}}). $ In practice, the values should only differ by a very small amount. We do not expect the errors to have a systematic bias. However, the residuals are just a sample of the error. The mean of the residuals must not be exactly 0 when the samples of the posteriors are used for the regression coefficients.   



\section{Code used in chapter 3}
\vspace{15mm}

\begin{codeenv}

\captionof{rcode}{Data generation of example 1 chapter 3}\label{r03:simdata.ex1}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x1} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{x2} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{x3} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{x4} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlcom{# b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0.8}
\hlstd{b1} \hlkwb{<-} \hlnum{1}
\hlstd{b2} \hlkwb{<-} \hlnum{1}
\hlstd{b3} \hlkwb{<-} \hlnum{1}
\hlstd{b4} \hlkwb{<-} \hlnum{1}

\hlstd{y} \hlkwb{<-} \hlstd{b1} \hlopt{*} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{*} \hlstd{b2} \hlopt{+} \hlstd{b3} \hlopt{*} \hlstd{x3} \hlopt{+} \hlstd{b4} \hlopt{*} \hlstd{x4} \hlopt{+} \hlkwd{rnorm}\hlstd{(}\hlnum{50}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}

\hlstd{df} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{y} \hlstd{= y,} \hlkwc{x1} \hlstd{= x1,} \hlkwc{x2} \hlstd{= x2,} \hlkwc{x3} \hlstd{= x3,} \hlkwc{x4} \hlstd{= x4)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{codeenv}

\vspace{15mm}

\begin{codeenv}
\caption{Bayesian regression model for example 1 chapter 3}\label{r03:model.ex1}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{post2} \hlkwb{<-} \hlkwd{stan_glm}\hlstd{(y} \hlopt{~} \hlnum{1} \hlopt{+} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3} \hlopt{+} \hlstd{x4,} \hlkwc{data} \hlstd{= df,} \hlkwc{chains} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{cores} \hlstd{=} \hlnum{1}\hlstd{,}
    \hlkwc{iter} \hlstd{=} \hlnum{40000}\hlstd{,} \hlkwc{thin} \hlstd{=} \hlnum{20}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}

\vspace{15mm}


\begin{codeenv}
\caption{Function to get $\Rtwo$ from posterior samples of the full model}\label{r03:LMG}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{rtwos} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{X}\hlstd{,} \hlkwc{post.sample}\hlstd{) \{}
    \hlcom{# X: Predictor data as data frame post.sample: posterior sample as matrix}
    \hlcom{# (M[sample_i,]), last position should be sigma paramater.}
    \hlstd{X} \hlkwb{<-} \hlkwd{cov}\hlstd{(X)}  \hlcom{#covariance matrix}
    \hlcom{# Prepare data frame and rownames by using the combn() function}
    \hlstd{lst} \hlkwb{<-} \hlkwd{list}\hlstd{()}
    \hlstd{pcan} \hlkwb{<-} \hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlstd{n} \hlkwb{<-} \hlstd{(}\hlnum{2}\hlopt{^}\hlstd{pcan)} \hlopt{-} \hlnum{1}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{pcan) \{}
        \hlstd{lst[[i]]} \hlkwb{<-} \hlkwd{combn}\hlstd{(pcan, i)}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{character}\hlstd{(}\hlkwc{length} \hlstd{=} \hlnum{0}\hlstd{)}
    \hlstd{v} \hlkwb{<-} \hlkwd{rownames}\hlstd{(X)}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lst)) \{}
        \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
            \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste0}\hlstd{(v[}\hlopt{-}\hlstd{cur])}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste}\hlstd{(name,} \hlkwc{collapse} \hlstd{=} \hlstr{" "}\hlstd{)}
            \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(var.names, name)}
        \hlstd{\}}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rev}\hlstd{(var.names),} \hlstr{"all"}\hlstd{)}
    \hlstd{var.names[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlstr{"none"}
    \hlstd{size} \hlkwb{<-} \hlkwd{nrow}\hlstd{(post.sample)}  \hlcom{# how many samples}
    \hlstd{sig.posi} \hlkwb{<-} \hlkwd{ncol}\hlstd{(post.sample)}
    \hlstd{df.Rtwos} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{, n} \hlopt{+} \hlnum{1}\hlstd{, size))}
    \hlkwd{row.names}\hlstd{(df.Rtwos)} \hlkwb{<-} \hlstd{var.names}
    \hlcom{# fill in R^2 matrix, use posterior samples and calculate submodels}
    \hlcom{# according to the conditional variance formula.}
    \hlstd{v} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlkwa{for} \hlstd{(s} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{size) \{}
        \hlstd{sample.s} \hlkwb{<-} \hlstd{post.sample[s,} \hlopt{-}\hlstd{sig.posi]}
        \hlstd{Vtot} \hlkwb{<-} \hlstd{sample.s} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlstd{sample.s}  \hlcom{#total explained variance}
        \hlstd{count} \hlkwb{=} \hlstd{n}
        \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{(}\hlkwd{length}\hlstd{(lst)} \hlopt{-} \hlnum{1}\hlstd{)) \{}
            \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
                \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
                \hlstd{set} \hlkwb{<-} \hlstd{v[}\hlopt{-}\hlstd{cur]}
                \hlstd{matr} \hlkwb{<-} \hlstd{X[cur, cur]} \hlopt{-} \hlstd{X[cur, set]} \hlopt{%*%} \hlkwd{solve}\hlstd{(X[set, set])} \hlopt{%*%}
                  \hlstd{X[set, cur]}  \hlcom{#conditional variance formula}
                \hlstd{var.explain} \hlkwb{<-} \hlstd{sample.s[cur]} \hlopt{%*%} \hlstd{matr} \hlopt{%*%} \hlstd{sample.s[cur]}
                \hlstd{df.Rtwos[count, s]} \hlkwb{<-} \hlstd{Vtot} \hlopt{-} \hlstd{var.explain}
                \hlstd{count} \hlkwb{=} \hlstd{count} \hlopt{-} \hlnum{1}
            \hlstd{\}}
        \hlstd{\}}
        \hlstd{df.Rtwos[n} \hlopt{+} \hlnum{1}\hlstd{, s]} \hlkwb{<-} \hlstd{Vtot}
        \hlstd{df.Rtwos[, s]} \hlkwb{<-} \hlstd{df.Rtwos[, s]}\hlopt{/}\hlkwd{c}\hlstd{(Vtot} \hlopt{+} \hlstd{post.sample[s, sig.posi]}\hlopt{^}\hlnum{2}\hlstd{)}
    \hlstd{\}}
    \hlkwd{return}\hlstd{(df.Rtwos)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}

\vspace{15mm}

\begin{codeenv}
\caption{$\Rtwo$ function for stochastic predictors using bootstrap}\label{r03:LMG.boot}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# helper function to calculate covariance bootstrap sample}

\hlstd{bootcov} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{df}\hlstd{,} \hlkwc{boot.n}\hlstd{) \{}
    \hlstd{len} \hlkwb{<-} \hlkwd{nrow}\hlstd{(df)}
    \hlstd{cov.m} \hlkwb{<-} \hlkwd{cov}\hlstd{(df)}
    \hlstd{l} \hlkwb{<-} \hlkwd{dim}\hlstd{(cov.m)[}\hlnum{1}\hlstd{]}
    \hlstd{M.boot} \hlkwb{<-} \hlkwd{array}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{c}\hlstd{(l, l, boot.n))}
    \hlstd{M.boot[, ,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{cov}\hlstd{(df)}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{2}\hlopt{:}\hlstd{boot.n) \{}
        \hlstd{dfs} \hlkwb{<-} \hlstd{df[}\hlkwd{sample}\hlstd{(}\hlnum{1}\hlopt{:}\hlstd{len,} \hlkwc{replace} \hlstd{= T), ]}
        \hlstd{M.boot[, , i]} \hlkwb{<-} \hlkwd{cov}\hlstd{(dfs)}
    \hlstd{\}}

    \hlkwd{return}\hlstd{(M.boot)}
\hlstd{\}}


\hlcom{# Function that includes uncertainty about stochastic predictors by}
\hlcom{# bootstrapping using bootcov boot.n is the number of bootstrap samples}
\hlcom{# takes boot.n times longer to calculate.}

\hlstd{rtwos.boot} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{df}\hlstd{,} \hlkwc{post.sample}\hlstd{,} \hlkwc{boot.n}\hlstd{) \{}
    \hlcom{# df: Predictor data as data frame post.sample: posterior sample as matrix}
    \hlcom{# (M[sample_i,]), last position should be sigma paramater.}
    \hlstd{X} \hlkwb{<-} \hlkwd{cov}\hlstd{(df)}  \hlcom{#covariance matrix}
    \hlcom{# Prepare data frame and rownames by using the combn() function}
    \hlstd{lst} \hlkwb{<-} \hlkwd{list}\hlstd{()}
    \hlstd{pcan} \hlkwb{<-} \hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlstd{n} \hlkwb{<-} \hlstd{(}\hlnum{2}\hlopt{^}\hlstd{pcan)} \hlopt{-} \hlnum{1}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{pcan) \{}
        \hlstd{lst[[i]]} \hlkwb{<-} \hlkwd{combn}\hlstd{(pcan, i)}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{character}\hlstd{(}\hlkwc{length} \hlstd{=} \hlnum{0}\hlstd{)}
    \hlstd{v} \hlkwb{<-} \hlkwd{rownames}\hlstd{(X)}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lst)) \{}
        \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
            \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste0}\hlstd{(v[}\hlopt{-}\hlstd{cur])}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste}\hlstd{(name,} \hlkwc{collapse} \hlstd{=} \hlstr{" "}\hlstd{)}
            \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(var.names, name)}
        \hlstd{\}}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rev}\hlstd{(var.names),} \hlstr{"all"}\hlstd{)}
    \hlstd{var.names[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlstr{"none"}
    \hlstd{size} \hlkwb{<-} \hlkwd{nrow}\hlstd{(post.sample)}  \hlcom{# how many samples}
    \hlstd{sig.posi} \hlkwb{<-} \hlkwd{ncol}\hlstd{(post.sample)}
    \hlstd{df.Rtwos} \hlkwb{<-} \hlkwd{array}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{c}\hlstd{(n} \hlopt{+} \hlnum{1}\hlstd{, size, boot.n))}
    \hlkwd{dimnames}\hlstd{(df.Rtwos)[[}\hlnum{1}\hlstd{]]} \hlkwb{<-} \hlstd{var.names}
    \hlstd{boot.M} \hlkwb{<-} \hlkwd{bootcov}\hlstd{(df, boot.n)}
    \hlstd{v} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlkwa{for} \hlstd{(b} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{boot.n) \{}
        \hlstd{X} \hlkwb{<-} \hlstd{boot.M[, , b]}
        \hlcom{########### fill in R^2 matrix, use posterior samples and calculate submodels}
        \hlcom{########### according to the conditional variance formula.}
        \hlkwa{for} \hlstd{(s} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{size) \{}
            \hlstd{sample.s} \hlkwb{<-} \hlstd{post.sample[s,} \hlopt{-}\hlstd{sig.posi]}
            \hlstd{Vtot} \hlkwb{<-} \hlstd{sample.s} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlstd{sample.s}  \hlcom{#total explained variance}
            \hlstd{count} \hlkwb{=} \hlstd{n}
            \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{(}\hlkwd{length}\hlstd{(lst)} \hlopt{-} \hlnum{1}\hlstd{)) \{}
                \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
                  \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
                  \hlstd{set} \hlkwb{<-} \hlstd{v[}\hlopt{-}\hlstd{cur]}
                  \hlstd{matr} \hlkwb{<-} \hlstd{X[cur, cur]} \hlopt{-} \hlstd{X[cur, set]} \hlopt{%*%} \hlkwd{solve}\hlstd{(X[set, set])} \hlopt{%*%}
                    \hlstd{X[set, cur]}  \hlcom{#conditional variance }
                  \hlstd{var.explain} \hlkwb{<-} \hlstd{sample.s[cur]} \hlopt{%*%} \hlstd{matr} \hlopt{%*%} \hlstd{sample.s[cur]}
                  \hlstd{df.Rtwos[count, s, b]} \hlkwb{<-} \hlstd{Vtot} \hlopt{-} \hlstd{var.explain}
                  \hlstd{count} \hlkwb{=} \hlstd{count} \hlopt{-} \hlnum{1}
                \hlstd{\}}
            \hlstd{\}}
            \hlstd{df.Rtwos[n} \hlopt{+} \hlnum{1}\hlstd{, s, b]} \hlkwb{<-} \hlstd{Vtot}
            \hlstd{df.Rtwos[, s, b]} \hlkwb{<-} \hlstd{df.Rtwos[, s, b]}\hlopt{/}\hlkwd{c}\hlstd{(Vtot} \hlopt{+} \hlstd{post.sample[s, sig.posi]}\hlopt{^}\hlnum{2}\hlstd{)}
        \hlstd{\}}
    \hlstd{\}}
    \hlkwd{return}\hlstd{(df.Rtwos)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}

\vspace{15mm}


\begin{codeenv}

\caption{$\Rtwo$ function for stochastic predictors using covariance samples}\label{r03:LMG.covm}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Function to include samples of the predictor covariance matrix boot.n =}
\hlcom{# number of samples covm = posterior sample of covariance matrix}

\hlstd{rtwos.covm} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{df}\hlstd{,} \hlkwc{post.sample}\hlstd{,} \hlkwc{covm}\hlstd{,} \hlkwc{boot.n}\hlstd{) \{}
    \hlcom{# df: Predictor data as data frame post.sample: posterior sample as matrix}
    \hlcom{# (M[sample_i,]), last position should be sigma paramater.}
    \hlstd{X} \hlkwb{<-} \hlkwd{cov}\hlstd{(df)}  \hlcom{#covariance matrix}
    \hlcom{# Prepare data frame and rownames by using the combn() function}
    \hlstd{lst} \hlkwb{<-} \hlkwd{list}\hlstd{()}
    \hlstd{pcan} \hlkwb{<-} \hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlstd{n} \hlkwb{<-} \hlstd{(}\hlnum{2}\hlopt{^}\hlstd{pcan)} \hlopt{-} \hlnum{1}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{pcan) \{}
        \hlstd{lst[[i]]} \hlkwb{<-} \hlkwd{combn}\hlstd{(pcan, i)}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{character}\hlstd{(}\hlkwc{length} \hlstd{=} \hlnum{0}\hlstd{)}
    \hlstd{v} \hlkwb{<-} \hlkwd{rownames}\hlstd{(X)}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lst)) \{}
        \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
            \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste0}\hlstd{(v[}\hlopt{-}\hlstd{cur])}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste}\hlstd{(name,} \hlkwc{collapse} \hlstd{=} \hlstr{" "}\hlstd{)}
            \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(var.names, name)}
        \hlstd{\}}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rev}\hlstd{(var.names),} \hlstr{"all"}\hlstd{)}
    \hlstd{var.names[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlstr{"none"}
    \hlstd{size} \hlkwb{<-} \hlkwd{nrow}\hlstd{(post.sample)}  \hlcom{# how many samples}
    \hlstd{sig.posi} \hlkwb{<-} \hlkwd{ncol}\hlstd{(post.sample)}
    \hlstd{df.Rtwos} \hlkwb{<-} \hlkwd{array}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{c}\hlstd{(n} \hlopt{+} \hlnum{1}\hlstd{, size, boot.n))}
    \hlkwd{dimnames}\hlstd{(df.Rtwos)[[}\hlnum{1}\hlstd{]]} \hlkwb{<-} \hlstd{var.names}
    \hlstd{v} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlkwa{for} \hlstd{(b} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{boot.n) \{}
        \hlstd{X} \hlkwb{<-} \hlstd{covm[, , b]}
        \hlcom{########### fill in R^2 matrix, use posterior samples and calculate submodels}
        \hlcom{########### according to the conditional expectation formula.}
        \hlkwa{for} \hlstd{(s} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{size) \{}
            \hlstd{sample.s} \hlkwb{<-} \hlstd{post.sample[s,} \hlopt{-}\hlstd{sig.posi]}
            \hlstd{Vtot} \hlkwb{<-} \hlstd{sample.s} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlstd{sample.s}  \hlcom{#total explained variance}
            \hlstd{count} \hlkwb{=} \hlstd{n}
            \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{(}\hlkwd{length}\hlstd{(lst)} \hlopt{-} \hlnum{1}\hlstd{)) \{}
                \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
                  \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
                  \hlstd{set} \hlkwb{<-} \hlstd{v[}\hlopt{-}\hlstd{cur]}
                  \hlstd{matr} \hlkwb{<-} \hlstd{X[cur, cur]} \hlopt{-} \hlstd{X[cur, set]} \hlopt{%*%} \hlkwd{solve}\hlstd{(X[set, set])} \hlopt{%*%}
                    \hlstd{X[set, cur]}  \hlcom{#conditional variance }
                  \hlstd{var.explain} \hlkwb{<-} \hlstd{sample.s[cur]} \hlopt{%*%} \hlstd{matr} \hlopt{%*%} \hlstd{sample.s[cur]}
                  \hlstd{df.Rtwos[count, s, b]} \hlkwb{<-} \hlstd{Vtot} \hlopt{-} \hlstd{var.explain}
                  \hlstd{count} \hlkwb{=} \hlstd{count} \hlopt{-} \hlnum{1}
                \hlstd{\}}
            \hlstd{\}}
            \hlstd{df.Rtwos[n} \hlopt{+} \hlnum{1}\hlstd{, s, b]} \hlkwb{<-} \hlstd{Vtot}
            \hlstd{df.Rtwos[, s, b]} \hlkwb{<-} \hlstd{df.Rtwos[, s, b]}\hlopt{/}\hlkwd{c}\hlstd{(Vtot} \hlopt{+} \hlstd{post.sample[s, sig.posi]}\hlopt{^}\hlnum{2}\hlstd{)}
        \hlstd{\}}
    \hlstd{\}}
    \hlkwd{return}\hlstd{(df.Rtwos)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}

\vspace{15mm}

\begin{codeenv}

\caption{Jags code to make inference about covariance matrix}\label{r03:covminf}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{zy} \hlkwb{=} \hlstd{df[,}\hlnum{2}\hlopt{:}\hlnum{5}\hlstd{]}
\hlcom{#----------------------------------------------------------------------------}
\hlcom{# In the following example we know that the $\textbackslash{}X$ are coming from a normal distribution. The covariance matrix of the distribution is estimated in aBayesian way. The package Jags is used.}
\hlcom{#----------------------------------------------------------------------------}
\hlcom{# Assemble data for sending to JAGS:}
\hlstd{dataList} \hlkwb{=} \hlkwd{list}\hlstd{(}
        \hlkwc{zy} \hlstd{= zy ,}
        \hlkwc{Ntotal} \hlstd{=}  \hlkwd{nrow}\hlstd{(zy) ,}
        \hlkwc{Nvar} \hlstd{=} \hlkwd{ncol}\hlstd{(zy) ,}
        \hlkwc{zRscal} \hlstd{=} \hlkwd{ncol}\hlstd{(zy) ,}  \hlcom{# for dwish prior}
        \hlkwc{zRmat} \hlstd{=} \hlkwd{diag}\hlstd{(}\hlkwc{x}\hlstd{=}\hlnum{1}\hlstd{,}\hlkwc{nrow}\hlstd{=}\hlkwd{ncol}\hlstd{(zy))}
\hlstd{)}

\hlcom{# Define the model:}
\hlstd{modelString} \hlkwb{=} \hlstr{"
model \{
for ( i in 1:Ntotal ) \{
zy[i,1:Nvar] ~ dmnorm( zMu[1:Nvar] , zInvCovMat[1:Nvar,1:Nvar] ) 
\}
for ( varIdx in 1:Nvar ) \{ zMu[varIdx] ~ dnorm( 0 , 1/2^2 ) \}
zInvCovMat ~ dwish( zRmat[1:Nvar,1:Nvar] , zRscal )
# Convert invCovMat to sd and correlation:
zCovMat <- inverse( zInvCovMat )

\}
"} \hlcom{# close quote for modelString}
\hlkwd{writeLines}\hlstd{( modelString ,} \hlkwc{con}\hlstd{=}\hlstr{"Jags-MultivariateNormal-model.txt"} \hlstd{)}

\hlcom{# Run the chains:}
\hlstd{nChain} \hlkwb{=} \hlnum{3}
\hlstd{nAdapt} \hlkwb{=} \hlnum{500}
\hlstd{nBurnIn} \hlkwb{=} \hlnum{500}
\hlstd{nThin} \hlkwb{=} \hlnum{10}
\hlstd{nStepToSave} \hlkwb{=} \hlnum{20000}
\hlkwd{require}\hlstd{(rjags)}
\hlstd{jagsModel} \hlkwb{=} \hlkwd{jags.model}\hlstd{(} \hlkwc{file}\hlstd{=}\hlstr{"Jags-MultivariateNormal-model.txt"} \hlstd{,}
                                                                                                \hlkwc{data}\hlstd{=dataList ,} \hlkwc{n.chains}\hlstd{=nChain ,} \hlkwc{n.adapt}\hlstd{=nAdapt )}
\hlkwd{update}\hlstd{( jagsModel ,} \hlkwc{n.iter}\hlstd{=nBurnIn )}
\hlstd{codaSamples} \hlkwb{=} \hlkwd{coda.samples}\hlstd{( jagsModel ,}
                                                                                                                \hlkwc{variable.names}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{'zCovMat'}\hlstd{) ,}
                                                                                                                \hlkwc{n.iter}\hlstd{=nStepToSave}\hlopt{/}\hlstd{nChain}\hlopt{*}\hlstd{nThin ,} \hlkwc{thin}\hlstd{=nThin )}
\hlcom{# Convergence diagnostics:}
\hlstd{parameterNames} \hlkwb{=} \hlkwd{varnames}\hlstd{(codaSamples)} \hlcom{# get all parameter names}
\hlcom{# Examine the posterior distribution:}
\hlstd{mcmcMat} \hlkwb{=} \hlkwd{as.matrix}\hlstd{(codaSamples)}
\hlstd{chainLength} \hlkwb{=} \hlkwd{nrow}\hlstd{(mcmcMat)}
\hlstd{covMat} \hlkwb{<-} \hlkwd{array}\hlstd{(}\hlnum{NA}\hlstd{,} \hlkwd{c}\hlstd{(}\hlnum{4}\hlstd{,}\hlnum{4}\hlstd{,chainLength))}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{chainLength)\{}
\hlstd{covMat[}\hlnum{1}\hlopt{:}\hlnum{4}\hlstd{,}\hlnum{1}\hlopt{:}\hlnum{4}\hlstd{,i]}\hlkwb{<-}\hlkwd{matrix}\hlstd{(mcmcMat[i,],} \hlnum{4}\hlstd{,} \hlnum{4}\hlstd{)}
\hlstd{\}}
\hlstd{covMat} \hlkwb{<-} \hlstd{covMat[}\hlnum{1}\hlopt{:}\hlnum{4}\hlstd{,}\hlnum{1}\hlopt{:}\hlnum{4}\hlstd{,}\hlkwd{sample}\hlstd{(}\hlnum{1}\hlopt{:}\hlnum{20000}\hlstd{,} \hlkwc{replace}\hlstd{=F)]}
\hlstd{df.rtwos} \hlkwb{<-}\hlkwd{rtwos.covm}\hlstd{(df[,}\hlnum{2}\hlopt{:}\hlnum{5}\hlstd{], post.sample, covMat,} \hlnum{10}\hlstd{)}
\hlstd{n.boot} \hlkwb{=} \hlnum{10}
\hlstd{LMG.Vals.I.covm}\hlkwb{<-}\hlkwd{array}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{c}\hlstd{(}\hlnum{4}\hlstd{,}\hlkwd{dim}\hlstd{(df.rtwos)[}\hlnum{2}\hlstd{], n.boot))}
\hlstd{LMG.Vals.J.covm}\hlkwb{<-}\hlkwd{array}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{c}\hlstd{(}\hlnum{4}\hlstd{,}\hlkwd{dim}\hlstd{(df.rtwos.boot)[}\hlnum{2}\hlstd{], n.boot))}
\hlstd{LMG.Vals.T.covm}\hlkwb{<-}\hlkwd{array}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{c}\hlstd{(}\hlnum{4}\hlstd{,}\hlkwd{dim}\hlstd{(df.rtwos.boot)[}\hlnum{2}\hlstd{], n.boot))}
\hlkwa{for} \hlstd{(b} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n.boot)\{}

        \hlkwa{for}\hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{dim}\hlstd{(df.rtwos)[}\hlnum{2}\hlstd{])\{}

                \hlstd{gofn}\hlkwb{<-}\hlstd{df.rtwos[,i,b]}

                \hlstd{obj.Gelman}\hlkwb{<-}\hlkwd{partition}\hlstd{(gofn,} \hlkwc{pcan} \hlstd{=} \hlnum{4}\hlstd{,} \hlkwc{var.names} \hlstd{=} \hlkwd{names}\hlstd{(df[,}\hlnum{2}\hlopt{:}\hlnum{5}\hlstd{]))}

                \hlstd{LMG.Vals.I.covm[,i, b]}\hlkwb{=}\hlstd{obj.Gelman}\hlopt{$}\hlstd{IJ[,}\hlnum{1}\hlstd{]}
                \hlstd{LMG.Vals.J.covm[,i, b]}\hlkwb{=}\hlstd{obj.Gelman}\hlopt{$}\hlstd{IJ[,}\hlnum{2}\hlstd{]}
                \hlstd{LMG.Vals.T.covm[,i, b]}\hlkwb{=}\hlstd{obj.Gelman}\hlopt{$}\hlstd{IJ[,}\hlnum{3}\hlstd{]}
        \hlstd{\}}
\hlstd{\}}

\hlkwd{quantile}\hlstd{(}\hlkwd{c}\hlstd{(LMG.Vals.I.covm[}\hlnum{1}\hlstd{,}\hlnum{1}\hlopt{:}\hlnum{1000}\hlstd{,]),} \hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.5}\hlstd{,} \hlnum{0.975}\hlstd{))}
\hlkwd{quantile}\hlstd{(}\hlkwd{c}\hlstd{(LMG.Vals.I.covm[}\hlnum{2}\hlstd{,}\hlnum{1}\hlopt{:}\hlnum{1000}\hlstd{,]),} \hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.5}\hlstd{,} \hlnum{0.975}\hlstd{))}
\hlkwd{quantile}\hlstd{(}\hlkwd{c}\hlstd{(LMG.Vals.I.covm[}\hlnum{3}\hlstd{,}\hlnum{1}\hlopt{:}\hlnum{1000}\hlstd{,]),} \hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.5}\hlstd{,} \hlnum{0.975}\hlstd{))}
\hlkwd{quantile}\hlstd{(}\hlkwd{c}\hlstd{(LMG.Vals.I.covm[}\hlnum{4}\hlstd{,}\hlnum{1}\hlopt{:}\hlnum{1000}\hlstd{,]),} \hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.5}\hlstd{,} \hlnum{0.975}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

	\end{codeenv}

\clearpage 

\section{Code used in chapter 4}
\vspace{15mm}

\begin{codeenv}

\caption{Data generating code random intercept model}\label{r04:simdatri}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sub} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlnum{20}
\hlstd{subi} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{20}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{4}\hlstd{)}
\hlstd{subi} \hlkwb{<-} \hlkwd{rep}\hlstd{(subi,} \hlnum{4}\hlstd{)}
\hlstd{t} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{3}\hlstd{)}
\hlstd{t} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{3}\hlstd{,} \hlnum{20}\hlstd{))}

\hlstd{mu} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{4}\hlstd{)}
\hlstd{sig} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0.4}\hlstd{,} \hlnum{4}\hlstd{,} \hlnum{4}\hlstd{)}
\hlkwd{diag}\hlstd{(sig)} \hlkwb{<-} \hlnum{1}
\hlstd{sig[}\hlnum{3}\hlstd{,} \hlnum{4}\hlstd{]} \hlkwb{<-} \hlnum{0.9}
\hlstd{sig[}\hlnum{4}\hlstd{,} \hlnum{3}\hlstd{]} \hlkwb{<-} \hlnum{0.9}
\hlstd{sig[}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlnum{0.3}
\hlstd{sig[}\hlnum{2}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{0.3}

\hlstd{rawvars} \hlkwb{<-} \hlkwd{mvrnorm}\hlstd{(}\hlkwc{n} \hlstd{=} \hlnum{80}\hlstd{,} \hlkwc{mu} \hlstd{= mu,} \hlkwc{Sigma} \hlstd{= sig)}

\hlstd{x1} \hlkwb{<-} \hlstd{t} \hlopt{+} \hlstd{rawvars[,} \hlnum{1}\hlstd{]}
\hlstd{x2} \hlkwb{<-} \hlstd{t} \hlopt{+} \hlstd{rawvars[,} \hlnum{2}\hlstd{]}
\hlstd{x3} \hlkwb{<-} \hlstd{t} \hlopt{+} \hlstd{rawvars[,} \hlnum{3}\hlstd{]}
\hlstd{x4} \hlkwb{<-} \hlstd{t} \hlopt{+} \hlstd{rawvars[,} \hlnum{4}\hlstd{]}

\hlstd{b1} \hlkwb{<-} \hlstd{b2} \hlkwb{<-} \hlnum{1}
\hlstd{b3} \hlkwb{<-} \hlstd{b4} \hlkwb{<-} \hlnum{2}

\hlstd{y} \hlkwb{<-} \hlstd{x1} \hlopt{*} \hlstd{b1} \hlopt{+} \hlstd{x2} \hlopt{*} \hlstd{b2} \hlopt{+} \hlstd{x3} \hlopt{*} \hlstd{b3} \hlopt{+} \hlstd{x4} \hlopt{*} \hlstd{b4} \hlopt{+} \hlstd{subi} \hlopt{+} \hlkwd{rnorm}\hlstd{(}\hlnum{80}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0.1}\hlstd{)}
\hlstd{df} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{y} \hlstd{= y,} \hlkwc{x1} \hlstd{= x1,} \hlkwc{x2} \hlstd{= x2,} \hlkwc{x3} \hlstd{= x3,} \hlkwc{x4} \hlstd{= x4,} \hlkwc{sub} \hlstd{=} \hlkwd{rep}\hlstd{(sub,}
    \hlnum{4}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

	\end{codeenv}
	
	\vspace{15mm}
	
			\begin{codeenv}
		
\caption{Marginal model LMG implementation}\label{r04:model.ri}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fit} \hlkwb{<-} \hlkwd{stan_glmer}\hlstd{(y} \hlopt{~} \hlstd{x1} \hlopt{+} \hlstd{x2} \hlopt{+} \hlstd{x3} \hlopt{+} \hlstd{x4} \hlopt{+} \hlstd{(}\hlnum{1} \hlopt{|} \hlstd{sub),} \hlkwc{data} \hlstd{= df,} \hlkwc{chains} \hlstd{=} \hlnum{4}\hlstd{,}
    \hlkwc{cores} \hlstd{=} \hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}
	
				\vspace{15mm}
	
	\begin{codeenv}
\caption{Rndom intercept LMG implementation}\label{r04:LMG.ri}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# function to calculate R2 data frame from random intercept model rtwos for}
\hlcom{# repeated measurement how much variance is explained by the within subject}
\hlcom{# predictors (repeated measuremnt correlations, Bland-Altman 1995)}
\hlcom{# var(fixed) / (var(fixed)+residual)}
\hlstd{rtwos.ri.r} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{X}\hlstd{,} \hlkwc{post.sample}\hlstd{) \{}
    \hlcom{# X: Predictor data as data frame post.sample: posterior sample as matrix}
    \hlcom{# (M[sample_i,]), second last position should be sigma paramater, last}
    \hlcom{# position random intercept sd.}
    \hlstd{X} \hlkwb{<-} \hlkwd{cov}\hlstd{(X)}  \hlcom{#covariance matrix}
    \hlcom{# Prepare data frame and rownames by using the combn() function}
    \hlstd{lst} \hlkwb{<-} \hlkwd{list}\hlstd{()}
    \hlstd{pcan} \hlkwb{<-} \hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlstd{n} \hlkwb{<-} \hlstd{(}\hlnum{2}\hlopt{^}\hlstd{pcan)} \hlopt{-} \hlnum{1}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{pcan) \{}
        \hlstd{lst[[i]]} \hlkwb{<-} \hlkwd{combn}\hlstd{(pcan, i)}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{character}\hlstd{(}\hlkwc{length} \hlstd{=} \hlnum{0}\hlstd{)}
    \hlstd{v} \hlkwb{<-} \hlkwd{rownames}\hlstd{(X)}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lst)) \{}
        \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
            \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste0}\hlstd{(v[}\hlopt{-}\hlstd{cur])}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste}\hlstd{(name,} \hlkwc{collapse} \hlstd{=} \hlstr{" "}\hlstd{)}
            \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(var.names, name)}
        \hlstd{\}}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rev}\hlstd{(var.names),} \hlstr{"all"}\hlstd{)}
    \hlstd{var.names[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlstr{"none"}
    \hlstd{size} \hlkwb{<-} \hlkwd{nrow}\hlstd{(post.sample)}  \hlcom{# how many samples}
    \hlstd{sig.posi} \hlkwb{<-} \hlkwd{ncol}\hlstd{(post.sample)} \hlopt{-} \hlnum{1}  \hlcom{# second last position sigma}
    \hlstd{df.Rtwos} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{, n} \hlopt{+} \hlnum{1}\hlstd{, size))}
    \hlkwd{row.names}\hlstd{(df.Rtwos)} \hlkwb{<-} \hlstd{var.names}
    \hlcom{########### fill in R^2 matrix, use posterior samples and calculate submodels}
    \hlcom{########### according to the conditional variance formula.}
    \hlstd{v} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlkwa{for} \hlstd{(s} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{size) \{}
        \hlstd{sample.s} \hlkwb{<-} \hlstd{post.sample[s,} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(X)]}
        \hlstd{Vtot} \hlkwb{<-} \hlstd{sample.s} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlstd{sample.s}  \hlcom{#total explained variance}
        \hlstd{count} \hlkwb{=} \hlstd{n}
        \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{(}\hlkwd{length}\hlstd{(lst)} \hlopt{-} \hlnum{1}\hlstd{)) \{}
            \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
                \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
                \hlstd{set} \hlkwb{<-} \hlstd{v[}\hlopt{-}\hlstd{cur]}
                \hlstd{matr} \hlkwb{<-} \hlstd{X[cur, cur]} \hlopt{-} \hlstd{X[cur, set]} \hlopt{%*%} \hlkwd{solve}\hlstd{(X[set, set])} \hlopt{%*%}
                  \hlstd{X[set, cur]}  \hlcom{#conditional variance formula}
                \hlstd{var.explain} \hlkwb{<-} \hlstd{sample.s[cur]} \hlopt{%*%} \hlstd{matr} \hlopt{%*%} \hlstd{sample.s[cur]}  \hlcom{#multiply by parameter sample}
                \hlstd{df.Rtwos[count, s]} \hlkwb{<-} \hlstd{Vtot} \hlopt{-} \hlstd{var.explain}
                \hlstd{count} \hlkwb{=} \hlstd{count} \hlopt{-} \hlnum{1}
            \hlstd{\}}
        \hlstd{\}}
        \hlstd{df.Rtwos[n} \hlopt{+} \hlnum{1}\hlstd{, s]} \hlkwb{<-} \hlstd{Vtot}
        \hlstd{df.Rtwos[, s]} \hlkwb{<-} \hlstd{df.Rtwos[, s]}\hlopt{/}\hlkwd{c}\hlstd{(Vtot} \hlopt{+} \hlstd{post.sample[s, sig.posi]}\hlopt{^}\hlnum{2}\hlstd{)}  \hlcom{#total variance + sigma^2 (post sample[s])}
    \hlstd{\}}
    \hlkwd{return}\hlstd{(df.Rtwos)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}
	
	\vspace{15mm}
	
			\begin{codeenv}
		
\caption{Marginal model data}\label{r04:rmarg.data}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sub} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlnum{20}
\hlstd{subi} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{20}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlstd{subi} \hlkwb{<-} \hlkwd{rep}\hlstd{(subi,} \hlnum{4}\hlstd{)}
\hlstd{mu} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{4}\hlstd{)}
\hlstd{sig} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0.4}\hlstd{,} \hlnum{4}\hlstd{,} \hlnum{4}\hlstd{)}
\hlkwd{diag}\hlstd{(sig)} \hlkwb{<-} \hlnum{1}
\hlstd{sig[}\hlnum{3}\hlstd{,} \hlnum{4}\hlstd{]} \hlkwb{<-} \hlnum{0.9}
\hlstd{sig[}\hlnum{4}\hlstd{,} \hlnum{3}\hlstd{]} \hlkwb{<-} \hlnum{0.9}
\hlstd{sig[}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlnum{0.3}
\hlstd{sig[}\hlnum{2}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{0.3}
\hlstd{rawvars} \hlkwb{<-} \hlkwd{mvrnorm}\hlstd{(}\hlkwc{n} \hlstd{=} \hlnum{80}\hlstd{,} \hlkwc{mu} \hlstd{= mu,} \hlkwc{Sigma} \hlstd{= sig)}
\hlkwd{cov}\hlstd{(rawvars)}
\hlstd{t} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{3}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{4}\hlstd{,} \hlnum{20}\hlstd{))}
\hlstd{x1} \hlkwb{<-} \hlstd{t} \hlopt{+} \hlstd{rawvars[,} \hlnum{1}\hlstd{]}
\hlstd{x2} \hlkwb{<-} \hlstd{t} \hlopt{+} \hlstd{rawvars[,} \hlnum{2}\hlstd{]}
\hlstd{x3} \hlkwb{<-} \hlstd{t} \hlopt{+} \hlstd{rawvars[,} \hlnum{3}\hlstd{]}
\hlstd{x4} \hlkwb{<-} \hlstd{t} \hlopt{+} \hlstd{rawvars[,} \hlnum{4}\hlstd{]}
\hlstd{Sig} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{3}\hlstd{,} \hlnum{4}\hlstd{,} \hlnum{4}\hlstd{)}
\hlkwd{diag}\hlstd{(Sig)} \hlkwb{<-} \hlnum{10}
\hlstd{u} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{4}\hlstd{)}
\hlstd{Sig[}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{5}
\hlstd{Sig[}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlnum{7}
\hlstd{Sig[}\hlnum{3}\hlstd{,} \hlnum{4}\hlstd{]} \hlkwb{<-} \hlnum{8}
\hlstd{Sig[}\hlnum{4}\hlstd{,} \hlnum{3}\hlstd{]} \hlkwb{<-} \hlnum{8}
\hlstd{Sig[}\hlnum{1}\hlstd{,} \hlnum{2}\hlstd{]} \hlkwb{<-} \hlnum{4}
\hlstd{Sig[}\hlnum{2}\hlstd{,} \hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{4}
\hlstd{error} \hlkwb{<-} \hlkwd{mvrnorm}\hlstd{(}\hlnum{20}\hlstd{, u, Sig)}
\hlstd{y} \hlkwb{<-} \hlstd{x1} \hlopt{*} \hlstd{b1} \hlopt{+} \hlstd{x2} \hlopt{*} \hlstd{b2} \hlopt{+} \hlstd{x3} \hlopt{*} \hlstd{b3} \hlopt{+} \hlstd{x4} \hlopt{*} \hlstd{b4} \hlopt{+} \hlkwd{c}\hlstd{(error)}
\hlstd{t} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{3}\hlstd{,} \hlnum{20}\hlstd{),} \hlkwd{rep}\hlstd{(}\hlnum{4}\hlstd{,} \hlnum{20}\hlstd{))}
\hlstd{df} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{y} \hlstd{= y,} \hlkwc{x1} \hlstd{= x1,} \hlkwc{x2} \hlstd{= x2,} \hlkwc{x3} \hlstd{= x3,} \hlkwc{x4} \hlstd{= x4,} \hlkwc{sub} \hlstd{=} \hlkwd{rep}\hlstd{(sub,}
    \hlnum{4}\hlstd{),} \hlkwc{t} \hlstd{= t)}

\hlcom{# Prepare for Bayesian framework}

\hlstd{Y} \hlkwb{<-} \hlkwd{matrix}\hlstd{(df[,} \hlstr{"y"}\hlstd{],} \hlnum{20}\hlstd{,} \hlnum{4}\hlstd{,} \hlkwc{byrow} \hlstd{= F)}
\hlstd{x1} \hlkwb{<-} \hlkwd{matrix}\hlstd{(df[,} \hlstr{"x1"}\hlstd{],} \hlnum{20}\hlstd{,} \hlnum{4}\hlstd{,} \hlkwc{byrow} \hlstd{= F)}
\hlstd{x2} \hlkwb{<-} \hlkwd{matrix}\hlstd{(df[,} \hlstr{"x2"}\hlstd{],} \hlnum{20}\hlstd{,} \hlnum{4}\hlstd{,} \hlkwc{byrow} \hlstd{= F)}
\hlstd{x3} \hlkwb{<-} \hlkwd{matrix}\hlstd{(df[,} \hlstr{"x3"}\hlstd{],} \hlnum{20}\hlstd{,} \hlnum{4}\hlstd{,} \hlkwc{byrow} \hlstd{= F)}
\hlstd{x4} \hlkwb{<-} \hlkwd{matrix}\hlstd{(df[,} \hlstr{"x4"}\hlstd{],} \hlnum{20}\hlstd{,} \hlnum{4}\hlstd{,} \hlkwc{byrow} \hlstd{= F)}

\hlstd{N} \hlkwb{=} \hlnum{20}  \hlcom{#subjects}
\hlstd{M} \hlkwb{=} \hlnum{4}  \hlcom{# repeated measures}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}
	
		\vspace{15mm}

		\begin{codeenv}
		
\caption{Marginal model LMG implementation}\label{r04:LMG.rmarg}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{rtwos.marg} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{X}\hlstd{,} \hlkwc{post.sample}\hlstd{,} \hlkwc{m}\hlstd{) \{}
    \hlcom{# X: Predictor data as data frame post.sample: posterior sample as matrix}
    \hlcom{# (M[sample_i,]), the last positions are filled with diag(SIGMA) inferences}
    \hlcom{# (number of repeated measures)}
    \hlstd{X} \hlkwb{<-} \hlkwd{cov}\hlstd{(X)}  \hlcom{#covariance matrix}
    \hlcom{# Prepare data frame and rownames by using the combn() function}
    \hlstd{lst} \hlkwb{<-} \hlkwd{list}\hlstd{()}
    \hlstd{pcan} \hlkwb{<-} \hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlstd{n} \hlkwb{<-} \hlstd{(}\hlnum{2}\hlopt{^}\hlstd{pcan)} \hlopt{-} \hlnum{1}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{pcan) \{}
        \hlstd{lst[[i]]} \hlkwb{<-} \hlkwd{combn}\hlstd{(pcan, i)}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{character}\hlstd{(}\hlkwc{length} \hlstd{=} \hlnum{0}\hlstd{)}
    \hlstd{v} \hlkwb{<-} \hlkwd{rownames}\hlstd{(X)}
    \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(lst)) \{}
        \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
            \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste0}\hlstd{(v[}\hlopt{-}\hlstd{cur])}
            \hlstd{name} \hlkwb{<-} \hlkwd{paste}\hlstd{(name,} \hlkwc{collapse} \hlstd{=} \hlstr{" "}\hlstd{)}
            \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(var.names, name)}
        \hlstd{\}}
    \hlstd{\}}
    \hlstd{var.names} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rev}\hlstd{(var.names),} \hlstr{"all"}\hlstd{)}
    \hlstd{var.names[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlstr{"none"}
    \hlstd{size} \hlkwb{<-} \hlkwd{nrow}\hlstd{(post.sample)}  \hlcom{# how many samples}
    \hlstd{sig.posi} \hlkwb{<-} \hlstd{((}\hlkwd{ncol}\hlstd{(post.sample)} \hlopt{-} \hlstd{(m} \hlopt{-} \hlnum{1}\hlstd{))}\hlopt{:}\hlkwd{ncol}\hlstd{(post.sample))}  \hlcom{# second last position sigma, last position random intercept variance}
    \hlstd{df.Rtwos} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{, n} \hlopt{+} \hlnum{1}\hlstd{, size))}
    \hlkwd{row.names}\hlstd{(df.Rtwos)} \hlkwb{<-} \hlstd{var.names}
    \hlcom{########### fill in R^2 matrix, use posterior samples and calculate submodels}
    \hlcom{########### according to the conditional variance formula.}
    \hlstd{v} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlkwd{dim}\hlstd{(X)[}\hlnum{2}\hlstd{]}
    \hlkwa{for} \hlstd{(s} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{size) \{}
        \hlstd{sample.s} \hlkwb{<-} \hlstd{post.sample[s,} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(X)]}
        \hlstd{Vtot} \hlkwb{<-} \hlstd{sample.s} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlstd{sample.s}  \hlcom{#total explained variance}
        \hlstd{count} \hlkwb{=} \hlstd{n}
        \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{(}\hlkwd{length}\hlstd{(lst)} \hlopt{-} \hlnum{1}\hlstd{)) \{}
            \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{ncol}\hlstd{(lst[[i]])) \{}
                \hlstd{cur} \hlkwb{<-} \hlstd{lst[[i]][, j]}
                \hlstd{set} \hlkwb{<-} \hlstd{v[}\hlopt{-}\hlstd{cur]}
                \hlstd{matr} \hlkwb{<-} \hlstd{X[cur, cur]} \hlopt{-} \hlstd{X[cur, set]} \hlopt{%*%} \hlkwd{solve}\hlstd{(X[set, set])} \hlopt{%*%}
                  \hlstd{X[set, cur]}  \hlcom{#conditional variance formula}
                \hlstd{var.explain} \hlkwb{<-} \hlstd{sample.s[cur]} \hlopt{%*%} \hlstd{matr} \hlopt{%*%} \hlstd{sample.s[cur]}
                \hlstd{df.Rtwos[count, s]} \hlkwb{<-} \hlstd{Vtot} \hlopt{-} \hlstd{var.explain}
                \hlstd{count} \hlkwb{=} \hlstd{count} \hlopt{-} \hlnum{1}
            \hlstd{\}}
        \hlstd{\}}
        \hlstd{df.Rtwos[n} \hlopt{+} \hlnum{1}\hlstd{, s]} \hlkwb{<-} \hlstd{Vtot}
        \hlstd{df.Rtwos[, s]} \hlkwb{<-} \hlstd{df.Rtwos[, s]}\hlopt{/}\hlkwd{c}\hlstd{(Vtot} \hlopt{+} \hlkwd{sum}\hlstd{(post.sample[s, sig.posi]))}
        \hlcom{# total explained variance + diag(SIGMA)}
    \hlstd{\}}
    \hlkwd{return}\hlstd{(df.Rtwos)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}

			\vspace{15mm}


		\begin{codeenv}
		
\caption{Marginal model LMG implementation}\label{r04:model.unstruct}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.98, 0.98, 0.98}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#--------------------------------------------}
\hlstd{modelString} \hlkwb{<-} \hlstr{"model\{

# Likelihood
for(i in 1:N)\{
Y[i,1:M] ~ dmnorm(mu[i,1:M],Omega[1:M,1:M])
for(j in 1:M)\{
mu[i,j] <- beta0 + beta1*x1[i,j]+ beta2*x2[i,j]+ beta3*x3[i,j] + beta4*x4[i,j]  
\}\}

# Priors

Omega[1:M, 1:M] ~dwish(zRmat[1:M,1:M] , zRscal)
Sigma[1:M, 1:M] <- inverse(Omega)

beta0      ~ dnorm(0,0.001)
beta1      ~ dnorm(0,0.001)
beta2      ~ dnorm(0,0.001)
beta3      ~ dnorm(0,0.001)
beta4      ~ dnorm(0,0.001)

\}"}

\hlkwd{writeLines}\hlstd{(modelString,} \hlkwc{con} \hlstd{=} \hlstr{"Jags-MultivariateNormal-model.txt"}\hlstd{)}

\hlstd{model} \hlkwb{<-} \hlkwd{jags.model}\hlstd{(}\hlkwd{textConnection}\hlstd{(modelString),} \hlkwc{data} \hlstd{=} \hlkwd{list}\hlstd{(}\hlkwc{Y} \hlstd{= Y,} \hlkwc{N} \hlstd{= N,}
    \hlkwc{M} \hlstd{= M,} \hlkwc{x1} \hlstd{= x1,} \hlkwc{x2} \hlstd{= x2,} \hlkwc{x3} \hlstd{= x3,} \hlkwc{x4} \hlstd{= x4,} \hlkwc{zRscal} \hlstd{=} \hlkwd{ncol}\hlstd{(Y),} \hlkwc{zRmat} \hlstd{=} \hlkwd{diag}\hlstd{(}\hlkwc{x} \hlstd{=} \hlnum{1}\hlstd{,}
        \hlkwc{nrow} \hlstd{=} \hlkwd{ncol}\hlstd{(Y))),} \hlkwc{n.chains} \hlstd{=} \hlnum{3}\hlstd{)}

\hlstd{samp} \hlkwb{<-} \hlkwd{coda.samples}\hlstd{(model,} \hlkwc{variable.names} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"beta1"}\hlstd{,} \hlstr{"beta2"}\hlstd{,} \hlstr{"beta3"}\hlstd{,} \hlstr{"beta4"}\hlstd{,}
    \hlstr{"Sigma"}\hlstd{),} \hlkwc{n.iter} \hlstd{=} \hlnum{20000}\hlstd{,} \hlkwc{progress.bar} \hlstd{=} \hlstr{"none"}\hlstd{)}

\hlcom{# summary(samp)}

\hlcom{# L}
 \hlstd{calculations}

\hlstd{samp} \hlkwb{<-} \hlkwd{coda.samples}\hlstd{(model,} \hlkwc{variable.names} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"beta1"}\hlstd{,} \hlstr{"beta2"}\hlstd{,} \hlstr{"beta3"}\hlstd{,} \hlstr{"beta4"}\hlstd{,}
    \hlstr{"Sigma[1,1]"}\hlstd{,} \hlstr{"Sigma[2,2]"}\hlstd{,} \hlstr{"Sigma[3,3]"}\hlstd{,} \hlstr{"Sigma[4,4]"}\hlstd{),} \hlkwc{n.iter} \hlstd{=} \hlnum{20000}\hlstd{,}
    \hlkwc{thin} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{progress.bar} \hlstd{=} \hlstr{"none"}\hlstd{)}

\hlstd{post.sample} \hlkwb{<-} \hlstd{samp[[}\hlnum{1}\hlstd{]][,} \hlkwd{c}\hlstd{(}\hlnum{5}\hlopt{:}\hlnum{8}\hlstd{,} \hlnum{1}\hlopt{:}\hlnum{4}\hlstd{)]}

\hlstd{df.rtwos} \hlkwb{<-} \hlkwd{rtwos.marg}\hlstd{(df[,} \hlnum{2}\hlopt{:}\hlnum{5}\hlstd{], post.sample,} \hlnum{4}\hlstd{)}  \hlcom{# 4 repeated measures}

\hlstd{L}
\hlstd{.Vals.I} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{4}\hlstd{,} \hlkwd{dim}\hlstd{(df.rtwos)[}\hlnum{2}\hlstd{])}
\hlstd{L}
\hlstd{.Vals.J} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{4}\hlstd{,} \hlkwd{dim}\hlstd{(df.rtwos)[}\hlnum{2}\hlstd{])}
\hlstd{L}
\hlstd{.Vals.T} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{4}\hlstd{,} \hlkwd{dim}\hlstd{(df.rtwos)[}\hlnum{2}\hlstd{])}

\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{dim}\hlstd{(df.rtwos)[}\hlnum{2}\hlstd{]) \{}
    \hlstd{gofn} \hlkwb{<-} \hlstd{df.rtwos[, i]}
    \hlstd{obj.Gelman} \hlkwb{<-} \hlkwd{partition}\hlstd{(gofn,} \hlkwc{pcan} \hlstd{=} \hlnum{4}\hlstd{,} \hlkwc{var.names} \hlstd{=} \hlkwd{names}\hlstd{(df[,} \hlnum{2}\hlopt{:}\hlnum{5}\hlstd{]))}
    \hlstd{L}
\hlstd{.Vals.I[, i]} \hlkwb{=} \hlstd{obj.Gelman}\hlopt{$}\hlstd{IJ[,} \hlnum{1}\hlstd{]}
    \hlstd{L}
\hlstd{.Vals.J[, i]} \hlkwb{=} \hlstd{obj.Gelman}\hlopt{$}\hlstd{IJ[,} \hlnum{2}\hlstd{]}
    \hlstd{L}
\hlstd{.Vals.T[, i]} \hlkwb{=} \hlstd{obj.Gelman}\hlopt{$}\hlstd{IJ[,} \hlnum{3}\hlstd{]}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
	\end{codeenv}
	
	


\phantomsection


\addtocontents{toc}{\protect \vspace*{10mm}}
\addcontentsline{toc}{chapter}{\bfseries Bibliography}




\bibliographystyle{mywiley} 
\bibliography{biblio}

\cleardoublepage

\end{document}



%%% Local Variables:
%%% ispell-local-dictionary: "en_US"
%%% End:
