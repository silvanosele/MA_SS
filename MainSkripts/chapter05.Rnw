% LaTeX file for Chapter 05

<<set-parent05, echo=FALSE, cache=FALSE>>=
set_parent('Main.Rnw')
@


<<'preamble05',include=FALSE>>=
library(knitr)
opts_chunk$set(
    fig.path='figure/ch05_fig', 
    self.contained=FALSE,
    cache=TRUE
) 
@

\chapter{Discussion and Conclusion}

\section{Other variable importance metrics in the Bayesian framework}

There exists different variable importance metrics \citep{Gromping2015}. The focus of this master thesis was on the LMG formula.  A lot of variable importance metrics are based on the $\Rtwo$ of the full model compared to the submodels. Instead of the LMG formula, another variable importance metric could as well be used on the $\Rtwo$ of all  submodels for each posterior sample. Commonality analysis \citep{Nimon2008} and dominance analysis \citep{Gromping2015} seem to be interesting variance decomposition metrics. Both metrics provide some extensions to the LMG framework. Moreover, different options exist for the description of the LMG distribution. For example, the \texttt{relaimpo} package provides more bootstrap options, like pairwise differences, that could easily be transferred to the Bayesian framework. 
 
\section{Conclusion}

The Bayesian framework provides the option to include the uncertainty about parameters.  Using  the conditional variance formula allows to calculate the $\Rtwo$ of all the submodels from the posterior parameter distributions of the full model. Instead of fitting $2^{p}-1$ models, only the full-model needs to be fitted. The mutual interdependence of the submodels is then automatically respected. The $\Rtwo$ of the submodels do not decrease when adding predictors. The important property of non-negativity shares is then respected in the Bayesian framework.

A disadvantage about calculating the $\Rtwo$ of all the submodels with the conditional variance formula seems to be the restriction to the linear model. Although, this may be a topic of further research. Another disadvantage of the Bayesian framework, compared to the classical LMG implementation, are higher computational costs. The calculations are still possible in a reasonable time period for non-stochastic predictors. Parallel computing could be used to speed up the calculations. For stochastic predictors, the computation time are much higher than in the classical framework.

Assuming non-stochastic or stochastic predictors can have a big impact in small samples on the uncertainty of the explained variance and on the LMG values. Although the posterior regression parameter distributions are the same in both cases (under the assumptions described in chapter 2), the explained variance of a model is directly dependent on the covariance matrix. Inference about the covariance of the predictors $\X$ is therefore an important part when stochastic predictors are assumed. However, this does not seem to be an easy problem in general. Non-parametric bootstrap of the covariance matrix provides a practical solution in the Bayesian framework. 

 When the sample size is large enough, the classical and the Bayesian framework should lead to very similar values. The Bayesian framework allows including prior information, that may especially be relevant for small sample sizes. The credible interval of the Bayesian framework, in contrast to the confidence intervals, may further be easier to interpret in the mathematically correct way.
 
A lot of studies are concerned with within-subject changes. The extension of the LMG formula to those kinds of problems is not straightforward. It is depending on the complexity of the data. However, the extension seems to be easily possible when the focus is on the fixed effects for the simple random intercept model. In the Bayesian framework and in the maximum likelihood framework, it seems reasonable to compare only the explained variance of the fixed effects of the submodels against the total variance, containing $\sigma_{f}^2 + \sigma_{\alpha}^2 + \sigma_{\epsilon}^2$. Otherwise, there may be problems with the non-negativity property of the shares. 







