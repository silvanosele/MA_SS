% LaTeX file for Chapter 03

<<set-parent03, echo=FALSE, cache=FALSE>>=
set_parent('Main.Rnw')
@

<<'preamble03',include=FALSE>>=
library(knitr); library(hier.part); library(rstanarm);  library(GGally); library(ggplot2); library(relaimpo); library(rjags); library(corpcor); library(brinla); library(stargazer); library(xtable); library(kableExtra); library(MBESS); library(MASS); library(biostatUZH)


opts_chunk$set(
    fig.path='figure/ch03_fig', 
    self.contained=FALSE,
    inlcude=FALSE,
    cache=FALSE
) 

options(digits =3)
#function to calculate R2 data frame
rtwos<-function(X, post.sample){
  # X: Predictor data as data frame
  # post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
  
  X <- cov(X) #covariance matrix
  
  #Prepare data frame and rownames by using the combn() function
  
  lst <- list()
  
  pcan <- dim(X)[2]
  n <- (2^pcan)-1
  
  for (i in 1:pcan){
    lst[[i]] <- combn(pcan,i)
  }
  
  var.names <- character(length = 0)
  
  v<- rownames(X)
  
  for(i in 1:length(lst))
  {
    
    for (j in 1:ncol(lst[[i]])){
      cur<- lst[[i]][,j]
      name <- paste0(v[-cur])
      name <- paste(name, collapse = " ")
      var.names <- c(var.names, name)
    }
  }
  
  var.names<-c(rev(var.names), 'all')
  
  var.names[1]<-'none'
  
  size <- nrow(post.sample)  # how many samples
  
  sig.posi <- ncol(post.sample)
  
  df.Rtwos<-data.frame(matrix(0,n+1,size))
  
  row.names(df.Rtwos) <- var.names
  
  ########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
  
  v <- 1:dim(X)[2]
  
  for (s in 1:size){
    
    sample.s <- post.sample[s,-sig.posi]
    
    Vtot <- sample.s%*%X%*%sample.s  #total explained variance
    
    count=n  
    
    for(i in 1:(length(lst)-1))
    {
      
      for (j in 1:ncol(lst[[i]])){
        cur <- lst[[i]][,j]
        set <- v[-cur]
        matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
        var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
        df.Rtwos[count,s] <- Vtot - var.explain  
        count=count-1
      }
    }
    
    df.Rtwos[n+1,s] <- Vtot
    
    df.Rtwos[,s] <- df.Rtwos[,s]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
  }
  
  return(df.Rtwos)
}



bootcov <- function(df, boot.n){
	len <- nrow(df)
	cov.m <- cov(df)
	l <- dim(cov.m)[1]	
	M.boot <- array(NA, c(l,l,boot.n))
	M.boot[,,1] <- cov(df)
	for (i in 2 :boot.n){
		dfs <- df[sample(1:len, replace=T),]
		M.boot[,,i] <- cov(dfs)
	}
	
	return(M.boot)
}


#Function that includes uncertainty about stochastic predictors by bootstrapping using bootcov
#n is the number of bootstrap samples it should calculate
# takes boot.n times longer to calculate. 
rtwos.boot<-function(df, post.sample, boot.n){
	# X: Predictor data as data frame
	# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
	
	X <- cov(df) #covariance matrix
	
	#Prepare data frame and rownames by using the combn() function
	
	lst <- list()
	
	pcan <- dim(X)[2]
	n <- (2^pcan)-1
	
	for (i in 1:pcan){
		lst[[i]] <- combn(pcan,i)
	}
	
	var.names <- character(length = 0)
	
	v<- rownames(X)
	
	for(i in 1:length(lst))
	{
		
		for (j in 1:ncol(lst[[i]])){
			cur<- lst[[i]][,j]
			name <- paste0(v[-cur])
			name <- paste(name, collapse = " ")
			var.names <- c(var.names, name)
		}
	}
	
	var.names<-c(rev(var.names), 'all')
	
	var.names[1]<-'none'
	
	size <- nrow(post.sample)  # how many samples
	
	sig.posi <- ncol(post.sample)
	
	df.Rtwos<-array(0,c(n+1,size, boot.n))
	
	dimnames(df.Rtwos)[[1]] <- var.names
	
	boot.M <- bootcov(df, boot.n)
	
	v <- 1:dim(X)[2]
	
	for (b in 1:boot.n){
		
		
		X <- boot.M[,,b]
	
	########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
	
	for (s in 1:size){
		
		sample.s <- post.sample[s,-sig.posi]
		
		Vtot <- sample.s%*%X%*%sample.s  #total explained variance
		
		count=n  
		
		for(i in 1:(length(lst)-1))
		{
			
			for (j in 1:ncol(lst[[i]])){
				cur <- lst[[i]][,j]
				set <- v[-cur]
				matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
				var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
				df.Rtwos[count,s,b] <- Vtot - var.explain  
				count=count-1
			}
		}
		
		df.Rtwos[n+1,s,b] <- Vtot
		
		df.Rtwos[,s,b] <- df.Rtwos[,s,b]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
	}
	
	}
	return(df.Rtwos)
}


#Function to include samples of the predictor covariance matrix.

rtwos.covm<-function(df, post.sample, covm, boot.n){
	# X: Predictor data as data frame
	# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
	
	X <- cov(df) #covariance matrix
	
	#Prepare data frame and rownames by using the combn() function
	
	lst <- list()
	
	pcan <- dim(X)[2]
	n <- (2^pcan)-1
	
	for (i in 1:pcan){
		lst[[i]] <- combn(pcan,i)
	}
	
	var.names <- character(length = 0)
	
	v<- rownames(X)
	
	for(i in 1:length(lst))
	{
		
		for (j in 1:ncol(lst[[i]])){
			cur<- lst[[i]][,j]
			name <- paste0(v[-cur])
			name <- paste(name, collapse = " ")
			var.names <- c(var.names, name)
		}
	}
	
	var.names<-c(rev(var.names), 'all')
	
	var.names[1]<-'none'
	
	size <- nrow(post.sample)  # how many samples
	
	sig.posi <- ncol(post.sample)
	
	df.Rtwos<-array(0,c(n+1,size, boot.n))
	
	dimnames(df.Rtwos)[[1]] <- var.names
	
	v <- 1:dim(X)[2]
	
	
	for (b in 1:boot.n){
		
		
		X <- covm[,,b]
		
		########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
		
		for (s in 1:size){
			
			sample.s <- post.sample[s,-sig.posi]
			
			Vtot <- sample.s%*%X%*%sample.s  #total explained variance
			
			count=n  
			
			for(i in 1:(length(lst)-1))
			{
				
				for (j in 1:ncol(lst[[i]])){
					cur <- lst[[i]][,j]
					set <- v[-cur]
					matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
					var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
					df.Rtwos[count,s,b] <- Vtot - var.explain  
					count=count-1
				}
			}
			
			df.Rtwos[n+1,s,b] <- Vtot
			
			df.Rtwos[,s,b] <- df.Rtwos[,s,b]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
		}
		
	}
	return(df.Rtwos)
}

@


\chapter{Examples}

In the following section the Bayesian LMG implementation is presented on two examples. The first examples simulates data, the second examples uses empirical data.

\section{Simulated Data}

We assume a simple model for the first example: 

\begin{align*} 
&Y_{i} \sim \mathcal{N}(\beta_{0}+x_{1} \beta_{1}+x_{2} \beta_{2}+x_{3} \beta_{3}+x_{4} \beta_{4}, \sigma^2),& \\ & \beta_{1} = 0.5, \beta_{2} = 1,  \beta_{3} = 2 , \beta_{4}=0, \sigma^2 = 1 & \\ & \X_{1}, \X_{2},\X_{3},\X_{4} \sim \mathcal{N}(0, 1) 
\end{align*} 


The values of the four predictors are sampled from a standard normal distribution. These values are then multiplied by the regression coefficients to obtain the dependent variable. A standard normal distributed error is added. Fifty observations were sampled.

The following Code was used to simulate the data :

<<'simudata.example'>>=

x1 <- rnorm(50, 0, 1); x2 <- rnorm(50, 0, 1)
x3 <- rnorm(50, 0, 1); x4 <- rnorm(50, 0, 1)
#b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0.8
b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1

y <- b1*x1 + x2*b2 + b3*x3 + b4*x4 + rnorm(50, 0, 1) 


df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)

@


The model is fitted using the \texttt{rstanarm} package \citep{rstanarm} with the default priors for the regression and $\sigma^2$ parameters.  A burn-in periode of 1000, a sample size of 20000 and a thining of 20 were chosen, resulting in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:simdata.postsample2}. 

For each posterior sample of the parameters the $\Rtwo$ value is calculated. The $\Rtwo$ of the submodels is then calculated by the conditional variance formula for each posterior sample. The resulting $\Rtwo$ values of the first few posterior samples is shown in Table~\ref{tab:simdata.postsample3}.  The thinning makes sense in this case to reduce the computational burden and still obtain an appropriate posterior of the $\Rtwo$ values \citep{Link2012}. 

The \texttt{hier.part} package is used to calculate the LMG value for each posterior sample. The LMG posteriors are shown in Table\ref{tbl:nonstochEx1} . The independent component (I) represents the LMG value. The joint contribution (J) represents the difference from the independent component to the explained variance of the predictor-only model (T). Assuming stochastic or nonstochastic regressors has an influence on the uncertainty. First, non-stochastic regressors are assumed. The resulting LMG values and joint contributions with a 95\% credible interval are shown in Table~\ref{tbl:nonstochEx1}. An option to display the resulting LMG distribution is shown in Figure ~ \ref{fig:simdata.LMG.plot}.  Using the default weakly informative priors, the LMG distributions obtained from the Bayesian framework are very similar to the bootstrap confidence intervals assuming non-stochastic predictors of the LMG estimates obtained from the \texttt{relaimpo} package, as shown in Table~\ref{tbl:nonstochEx1relamip}. 



<<'simdata.postsample', include =FALSE>>=
post2 <- stan_glm(y ~ 1 + x1 + x2 + x3 + x4,
                  data = df,
                  chains = 1, cores =1, iter=40000, thin=20)
@

<<'simdata.postsample2', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=

#posterior sample
post.sample <- as.matrix(post2)

dt <- data.frame(post.sample[1:10,2:6])
rownames(dt)<-c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6', 'sample 7', 'sample 8', 'sample 9', 'sample 10')
#example of the first 10 posterior samples
kable(dt, format = "latex", booktabs=TRUE, linesep = c("", "", "", ""), caption = 'Samples from the posterior distributions of the regression parameters') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

#no need for the intercept, last parameter is sigma
post.sample <- post.sample[,-1]
@

<<'simdata.postsample3', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
#data frame with all submodels
df.rtwos <-rtwos(df[,2:5], post.sample)

colnames(df.rtwos)[1:6] <- c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6')
kable(df.rtwos[,1:6], format = 'latex', booktabs=TRUE, linesep = c("", "", "", ""), caption = '$ \\Rtwo$ for all submodels for the first six posterior samples')  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
@


<<'simdata.LMG', cache=FALSE, include=FALSE>>=

# prepare data frame for LMG values

LMG.Vals.I<-matrix(0, 4, dim(df.rtwos)[2])

LMG.Vals.J<-matrix(0, 4, dim(df.rtwos)[2])

LMG.Vals.T<-matrix(0, 4, dim(df.rtwos)[2])


for(i in 1:dim(df.rtwos)[2]){
  
  gofn<-df.rtwos[,i]
  
  obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
  
  LMG.Vals.I[,i]=obj.Gelman$IJ[,1]
  LMG.Vals.J[,i]=obj.Gelman$IJ[,2]
	LMG.Vals.T[,i]=obj.Gelman$IJ[,3]
}

varnames <- row.names(obj.Gelman$IJ)


# posterior LMG distribution of each variable
quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))

quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))

quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))

# some example how it could be displayed
dat <- data.frame(t(LMG.Vals.I))

pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart



# Comparison to relaimpo package

fit <- lm(y~., data=df)

######## compare to relimp package

run<-boot.relimp(fit, fixed=TRUE)

ci <- booteval.relimp(run, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))@mark

ci <- as.matrix(ci)
ci <- matrix(as.numeric(ci), 4 , 3)
colnames(ci) <- c('LMG', 'l.b', 'u.b')
rownames(ci) <- c('X1', 'X2', 'X3', 'X4')

#Code assuming stochastic predictors
run.stochastic<-boot.relimp(fit, fixed=FALSE)

ci.s <- booteval.relimp(run.stochastic, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))@mark

ci.s <- as.matrix(ci.s)
ci.s <- matrix(as.numeric(ci.s), 4 , 3)
colnames(ci.s) <- c('LMG', 'l.b', 'u.b')
rownames(ci.s) <- c('X1', 'X2', 'X3', 'X4')


@


<<'simdata.LMG.plot', cache=TRUE, echo=FALSE, fig.cap='Posterior distribution of LMG values.'>>=
pairs.chart
@

\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[3]} & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1}
\end{table}




\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{ci[1,1]} (\Sexpr{ci[1,2]}, \Sexpr{ci[1,3]})  & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{ci[2,1]} (\Sexpr{ci[2,2]}, \Sexpr{ci[2,3]})  & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})   \\ 
\Sexpr{varnames[3]} & \Sexpr{ci[3,1]} (\Sexpr{ci[3,2]}, \Sexpr{ci[3,3]})  & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{ci[4,1]} (\Sexpr{ci[4,2]}, \Sexpr{ci[4,3]}) & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relamip}
\end{table}

\FloatBarrier


In the example the predictor values were sampled from a normal distribution. It would therefore be more reasonable to assume stochastic predictors. Under the assumption of weak exogeinity and conditional independence the posterior distributions of the regression paramters $\bbeta$ are valid for non-stochastic and stochastic predictors. However, the uncertainty about the LMG values needs to include the uncertainty about the covariance matrix. If we know the distribution of the predictors we can use this information and obtain the posterior distribution of the covariance matrix. The package \texttt{Jags} was used for inference abouth the covariance matrix in a Bayesian way. As an alternative nonparametric bootstrap was used for inference about the covariance matrix. Both approaches resulted in very similar LMG values. Using the default priors further resulted in very similar LMG values as the nonparametric bootstrap option of the relaimpo package. Table~\ref{tbl:nonstochEx1relaimpstoch} shows the LMG values of these approaches. Using the bootstrap samples of the covariance matrix or samples from the posterior covariance matrix resulted in very similar LMG values. Bootstrap seems to be a valuable option for stochastic predictors when the distribution of the predictors is unknown. Even when the distribution is known the difference seems to be tiny. A benefit of going the full Bayesian way is that we can also include prior knowledge of the covariance matrix. For stochastic predictors the uncertainty about the covariance matrix is reflected in the larger credible intervals compared to nonstochastic predictors. Even when we would knew the exact regression parameters, there is alot of uncertainty in the LMG values caused by the uncertainty about the covariance matrix. 

<<'simdata.LMG.boot', include=FALSE, cache =TRUE>>=



#----------------------------------------------------------------------------------------


df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)

n.boot = 10

LMG.Vals.I.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.J.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.T.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.I.boot[,i, b]=obj.Gelman$IJ[,1]
	LMG.Vals.J.boot[,i, b]=obj.Gelman$IJ[,2]
	LMG.Vals.T.boot[,i, b]=obj.Gelman$IJ[,3]

}
	
}

quantile(c(LMG.Vals.I.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[4,1:1000,]), c(0.025, 0.5, 0.975))

#very similar values as in the confidence intervals for stochastic predictors
t1 <- c(LMG.Vals.I.boot[1,,])
t2 <- c(LMG.Vals.I.boot[2,,])
t3 <- c(LMG.Vals.I.boot[3,,])
t4 <- c(LMG.Vals.I.boot[4,,])

dat.b<-data.frame(cbind(t1,t2,t3,t4))
cor(dat.b)


@



<<'simdata.LMG.cov', include=FALSE, cache=TRUE>>=

zy = df[,2:5]

#----------------------------------------------------------------------------
# In the following example we know that the $\X$ are coming from a normal distribution. The covariance matrix of the distribution is estimated in aBayesian way. The package Jags is used.
#----------------------------------------------------------------------------

# Load some functions used below:
# Install the ellipse package if not already:
# Standardize the data:

#zy = apply(y,2,function(yVec){(yVec-mean(yVec))/sd(yVec)})
# Assemble data for sending to JAGS:
dataList = list(
	zy = zy ,
	Ntotal =  nrow(zy) ,
	Nvar = ncol(zy) ,
	# Include original data info for transforming to original scale:
	# For wishart (dwish) prior on inverse covariance matrix:
	zRscal = ncol(zy) ,  # for dwish prior
	zRmat = diag(x=1,nrow=ncol(zy))  # Rmat = diag(apply(y,2,var))
)

# Define the model:
modelString = "
model {
for ( i in 1:Ntotal ) {
zy[i,1:Nvar] ~ dmnorm( zMu[1:Nvar] , zInvCovMat[1:Nvar,1:Nvar] ) 
}
for ( varIdx in 1:Nvar ) { zMu[varIdx] ~ dnorm( 0 , 1/2^2 ) }
zInvCovMat ~ dwish( zRmat[1:Nvar,1:Nvar] , zRscal )
# Convert invCovMat to sd and correlation:
zCovMat <- inverse( zInvCovMat )

}
" # close quote for modelString
writeLines( modelString , con="Jags-MultivariateNormal-model.txt" )

# Run the chains:
nChain = 3
nAdapt = 500
nBurnIn = 500
nThin = 10
nStepToSave = 20000
require(rjags)
jagsModel = jags.model( file="Jags-MultivariateNormal-model.txt" , 
												data=dataList , n.chains=nChain , n.adapt=nAdapt )
update( jagsModel , n.iter=nBurnIn )
codaSamples = coda.samples( jagsModel , 
														variable.names=c('zCovMat') ,
														n.iter=nStepToSave/nChain*nThin , thin=nThin )


# Convergence diagnostics:
parameterNames = varnames(codaSamples) # get all parameter names


# Examine the posterior distribution:
mcmcMat = as.matrix(codaSamples)
chainLength = nrow(mcmcMat)

covMat <- array(NA, c(4,4,chainLength))

for (i in 1:chainLength){
covMat[1:4,1:4,i]<-matrix(mcmcMat[i,], 4, 4)
}

covMat <- covMat[1:4,1:4,sample(1:20000, replace=F)] 


df.rtwos <-rtwos.covm(df[,2:5], post.sample, covMat, 10)


n.boot = 10

LMG.Vals.I.covm<-array(0, c(4,dim(df.rtwos)[2], n.boot))
LMG.Vals.J.covm<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.T.covm<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))


for (b in 1:n.boot){
	
	for(i in 1:dim(df.rtwos)[2]){
		
		gofn<-df.rtwos[,i,b]
		
		obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
		
		LMG.Vals.I.covm[,i, b]=obj.Gelman$IJ[,1]
		LMG.Vals.J.covm[,i, b]=obj.Gelman$IJ[,2]
		LMG.Vals.T.covm[,i, b]=obj.Gelman$IJ[,3]

	}
	
}

quantile(c(LMG.Vals.I.covm[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.covm[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.covm[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.covm[4,1:1000,]), c(0.025, 0.5, 0.975))




@


\begin{table}[h]
\caption{LMG values assuming stochastic predictors with 95\% CI.}
\centering
\begin{tabular}{clll}
   \toprule
  & \multicolumn{1}{c}{\textbf{Relaimpo}} & \multicolumn{2}{c}{\textbf{Bayesian framework}} \\ \cmidrule(r){2-2}\cmidrule(l){3-4}
 \textbf{Variable} &  & \multicolumn{1}{c}{nonparameteric bootstrap}& \multicolumn{1}{c}{covariance inference} \\
 \midrule
\Sexpr{varnames[1]} & \Sexpr{ci.s[1,1]} (\Sexpr{ci.s[1,2]}, \Sexpr{ci.s[1,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[3]}) &  \Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
\Sexpr{varnames[2]} & \Sexpr{ci.s[2,1]} (\Sexpr{ci.s[2,2]}, \Sexpr{ci.s[2,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
\Sexpr{varnames[3]} & \Sexpr{ci.s[3,1]} (\Sexpr{ci.s[3,2]}, \Sexpr{ci.s[3,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
\Sexpr{varnames[4]} & \Sexpr{ci.s[4,1]} (\Sexpr{ci.s[4,2]}, \Sexpr{ci.s[4,3]}) & \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[3]}) & \Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relaimpstoch}
\end{table}


<<'covuncertainty', include=FALSE, cache=FALSE>>=

#How much variance is effectively in the bootstrap matrix when we know the regression parameters.

#fake post sample

x1 <- rnorm(50, 0, 1); x2 <- rnorm(50, 0, 1)
x3 <- rnorm(50, 0, 1); x4 <- rnorm(50, 0, 1)
#b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0
b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1

y <- b1*x1 + x2*b2 + b3*x3 + b4*x4 + rnorm(50, 0, 1) 

df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)

post.sample <- matrix(1, 1000,5)

df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)

n.boot = 10

LMG.Vals.I.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.I.boot[,i, b]=obj.Gelman$IJ[,1]
}
	
}

quantile(c(LMG.Vals.I.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[4,1:1000,]), c(0.025, 0.5, 0.975))



cov(df[,2:5])
@

<<'simdata.LMG.shrink', include=FALSE, cache =TRUE>>=

#Comparison of sample covariance and shrink covariance matrix

cov(df[,2:5])
cov.shrink(df[,2:5])

@


\clearpage

\section{Empirical Data}
In the following section the LMG analysis is applied on a popular empirical subset of a dataset containing test scores of pupils (N = 301) from the ref study. This subset of the data was used in ref(nimon) to present commonality analysis (another variance decomposition technique) in the R-package ref and in ref. Scores from a paragraph comprehension test (paragrap) was predicted by four verbal tests:  general information (general),  sentence comprehension (sentence) ,  word classification (wordc), and  word meaning (wordm) (Table~\ref{table:hs.data}). The aim of the regression analysis example from ref was to determine the strength of association between verbal ability and paragraph comprehension. The LMG metric may provide us some useful information about the importance of each predictor in the model. 
An overview of the data is shown in Figure~\ref{fig:empi.lmg.plot}. The regression results of the paragraph comprehension scores on the verbal test scores are shown Table~\ref{tbl:empirical.reg}). A novice researcher may conclude that the \textit{non-significant} predictors (general information and word classification), are not important in the model. Given the other predictors are already included in the model, the predictors seem not to provide us much information about the expected paragraph comprehension ability. However, we can not conclude out of this regression table, that the association between any of these \textit{non-significant} predictors and the dependent variable is unimportant. We see in Figure~\ref{fig:empi.lmg.plot} that the correlations between the predictors is rather high. The LMG metric may therefor provide us some new information about the importance of each predictor. 

The Bayesian regression model was fitted in \texttt{rstanarm}. The default priors were used for the regression coefficient and the $\sigma^2$ parameter.   A burn-in periode of 20000, a sample size of 20000 and a thining of 20 resulted in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:empirical.data.postsample2}. The resulting $\Rtwo$ of of these posterior samples is shown in Table~\ref{tab:empirical.data.postsample3}. The LMG values were calculated by using \texttt{hier.part}. The resulting independent component (I), joint contribution (J) and total explained variance in a predictor-only model (T) are shown in Table~\ref{tbl:empirical.ijt}. Sentence-comprehension and word-meaning seem to be the most important predictors by the LMG metric. However, none of the predictors seems to be unimportant.  The joint contributions of each predictor is also quite large.

For comparison purposes, the LMG metric was additionaly calculated with the \texttt{relaimpo} package using parametric bootstrapping. The confidence intervals of  \texttt{relaimpo} are almost identical to the credible intervals of the Bayesian framework (Table~tbl:empirical.relaimp.comp). Assuming stochastich or non-stochastic predictors would also result in almost identical uncertainty estimates with such a large sample size (tbl:empirical.relaimp.comp.stoch). 







\begin{table}
\centering
\caption{Variable description}
\begin{tabular}{l l}
  \toprule			
  Variable Name & Description  \\   \midrule  
  paragrap & scores on paragraph comprehension test  \\
  general & scores on general information test \\
  sentence & scores on sentence completion test\\
  wordc & scores on word classification test \\
  wordm & scores on word meaning test \\
  \bottomrule  
\end{tabular}
\label{table:hs.data}
\end{table}

<<results='asis', echo = FALSE, fig.cap='Test scores from Holzinger and Swineford\'s  (1939) Study. N=301'>>=
data('HS.data', package='MBESS')
hs.data <- HS.data[,c('paragrap','general', 'sentence' , 'wordc' , 'wordm')]
pairs.chart <- ggpairs(hs.data, lower = list(continuous = "cor"), upper = list(continuous = "points", combo = "dot")) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart

@


<<'empirical.data.LMG', include=FALSE, cache =TRUE>>=


bayes.hs <- stan_glm(paragrap ~ . ,
                  data = hs.data,
                  chains = 1, cores = 1, iter=40000, thin=20)

# Comparison to relaimpo package



fit <- lm(paragrap~., data= hs.data)


######## compare to relimp package

run<-boot.relimp(fit, fixed=TRUE)

ci <- booteval.relimp(run, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))@mark

ci <- as.matrix(ci)
ci <- matrix(as.numeric(ci), 4 , 3)
colnames(ci) <- c('LMG', 'l.b', 'u.b')
rownames(ci) <- c('X1', 'X2', 'X3', 'X4')

#Code assuming stochastic predictors
run.stochastic<-boot.relimp(fit, fixed=TRUE)

ci.s <- booteval.relimp(run.stochastic, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))@mark

ci.s <- as.matrix(ci.s)
ci.s <- matrix(as.numeric(ci.s), 4 , 3)
colnames(ci.s) <- c('LMG', 'l.b', 'u.b')
rownames(ci.s) <- c('X1', 'X2', 'X3', 'X4')
@

<<'empirical.data.reg', results='asis', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
res.table <- tableRegression(fit, caption='Regression of paragraph comprehension on verbal tests.', caption.placement = "top", label = 'tbl:empirical.reg')
@

<<'empirical.data.postsample2', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
#posterior sample
post.sample <- as.matrix(bayes.hs)

dt <- data.frame(post.sample[1:10,2:6])
rownames(dt)<-c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6', 'sample 7', 'sample 8', 'sample 9', 'sample 10')
#example of the first 10 posterior samples
kable(dt, format = "latex", booktabs=TRUE, linesep = c("", "", "", ""), caption = 'Samples from the posterior distributions of the regression parameters') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

#no need for the intercept, last parameter is sigma
post.sample <- post.sample[,-1]
@

<<'empirical.data.postsample3', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
#data frame with all submodels
df.rtwos <-rtwos(hs.data[,2:5], post.sample)

colnames(df.rtwos)[1:6] <- c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6')
kable(df.rtwos[,1:6], format = 'latex', booktabs=TRUE, linesep = c("", "", "", ""), caption = '$ \\Rtwo$ for all submodels for the first six posterior samples')  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
@

<<'empirical.LMG.values', include=FALSE,  cache=TRUE>>=

LMG.Vals.I<-matrix(0, 4, dim(df.rtwos)[2])

LMG.Vals.J<-matrix(0, 4, dim(df.rtwos)[2])

LMG.Vals.T<-matrix(0, 4, dim(df.rtwos)[2])


for(i in 1:dim(df.rtwos)[2]){
  
  gofn<-df.rtwos[,i]
  
  obj.Gelman<-partition(gofn, pcan = 4, var.names = names(hs.data[,2:5]))
  
  LMG.Vals.I[,i]=obj.Gelman$IJ[,1]
  LMG.Vals.J[,i]=obj.Gelman$IJ[,2]
	LMG.Vals.T[,i]=obj.Gelman$IJ[,3]
}

varnames <- row.names(obj.Gelman$IJ)


# posterior LMG distribution of each variable
quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))

quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))

quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))




@

<<'empi.lmg.plot', echo = FALSE, fig.cap='LMG posterior distribution of different verbal ability tests'>>=
#Visualization
dat <- data.frame(t(LMG.Vals.I))

pairs.chart <- ggpairs(dat, lower = list(list(combo = "density")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart

@


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[3]} & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.ijt}
\end{table}


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{ci[1,1]} (\Sexpr{ci[1,2]}, \Sexpr{ci[1,3]})  & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{ci[2,1]} (\Sexpr{ci[2,2]}, \Sexpr{ci[2,3]})  & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})   \\ 
\Sexpr{varnames[3]} & \Sexpr{ci[3,1]} (\Sexpr{ci[3,2]}, \Sexpr{ci[3,3]})  & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{ci[4,1]} (\Sexpr{ci[4,2]}, \Sexpr{ci[4,3]}) & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp}
\end{table}



<<'empirical.LMG.boot', include=FALSE, cache =TRUE>>=



#----------------------------------------------------------------------------------------


df.rtwos.boot <-rtwos.boot(hs.data[,2:5], post.sample, 10)

n.boot = 10

LMG.Vals.I.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.J.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.T.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.I.boot[,i, b]=obj.Gelman$IJ[,1]
	LMG.Vals.J.boot[,i, b]=obj.Gelman$IJ[,2]
	LMG.Vals.T.boot[,i, b]=obj.Gelman$IJ[,3]

}
	
}

quantile(c(LMG.Vals.I.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[4,1:1000,]), c(0.025, 0.5, 0.975))

#very similar values as in the confidence intervals for stochastic predictors
t1 <- c(LMG.Vals.I.boot[1,,])
t2 <- c(LMG.Vals.I.boot[2,,])
t3 <- c(LMG.Vals.I.boot[3,,])
t4 <- c(LMG.Vals.I.boot[4,,])

dat.b<-data.frame(cbind(t1,t2,t3,t4))
cor(dat.b)


@

\begin{table}[h]
\caption{Variance decomposition for Stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{ci.s[1,1]} (\Sexpr{ci.s[1,2]}, \Sexpr{ci.s[1,3]})  &  \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{ci.s[2,1]} (\Sexpr{ci.s[2,2]}, \Sexpr{ci.s[2,3]})  &  \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[3]})   \\ 
\Sexpr{varnames[3]} & \Sexpr{ci.s[3,1]} (\Sexpr{ci.s[3,2]}, \Sexpr{ci.s[3,3]})  &  \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{ci.s[4,1]} (\Sexpr{ci.s[4,2]}, \Sexpr{ci.s[4,3]}) &  \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp.stoch}
\end{table}