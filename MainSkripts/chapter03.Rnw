% LaTeX file for Chapter 03

<<set-parent03, echo=FALSE, cache=FALSE>>=
set_parent('Main.Rnw')
@

<<'preamble03',include=FALSE>>=
library(knitr); library(hier.part); library(rstanarm);  library(GGally); library(ggplot2); library(relaimpo); library(rjags); library(corpcor); library(brinla); library(stargazer); library(xtable); library(kableExtra); library(MBESS); library(MASS); library(biostatUZH)


opts_chunk$set(
    fig.path='figure/ch03_fig', 
               echo=TRUE, message=FALSE,
               fig.width=7, fig.height=7,  
               out.width='\\textwidth-3cm',
               message=FALSE, fig.align='center',
               background="gray98", tidy=FALSE, #tidy.opts=list(width.cutoff=60),
               cache=TRUE
) 

options(digits =3)

@

<<'LMGfunction',include=FALSE, tidy=TRUE>>=

rtwos<-function(X, post.sample){
  # X: Predictor data as data frame
  # post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
  X <- cov(X) #covariance matrix
  #Prepare data frame and rownames by using the combn() function
  lst <- list()
  pcan <- dim(X)[2]
  n <- (2^pcan)-1
  for (i in 1:pcan){
    lst[[i]] <- combn(pcan,i)
  }
  var.names <- character(length = 0)
  v<- rownames(X)
  for(i in 1:length(lst)){
    for (j in 1:ncol(lst[[i]])){
      cur<- lst[[i]][,j]
      name <- paste0(v[-cur])
      name <- paste(name, collapse = " ")
      var.names <- c(var.names, name)
    }
  }
  var.names<-c(rev(var.names), 'all')
  var.names[1]<-'none'
  size <- nrow(post.sample)  # how many samples
  sig.posi <- ncol(post.sample)
  df.Rtwos<-data.frame(matrix(0,n+1,size))
  row.names(df.Rtwos) <- var.names
  #fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional variance formula.
  v <- 1:dim(X)[2]
  for (s in 1:size){
    sample.s <- post.sample[s,-sig.posi]
    Vtot <- sample.s%*%X%*%sample.s  #total explained variance
    count=n  
    for(i in 1:(length(lst)-1)){
      for (j in 1:ncol(lst[[i]])){
        cur <- lst[[i]][,j]
        set <- v[-cur]
        matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional variance formula
        var.explain<- sample.s[cur]%*%matr%*%sample.s[cur] 
        df.Rtwos[count,s] <- Vtot - var.explain  
        count=count-1
      }
    }
    df.Rtwos[n+1,s] <- Vtot
    df.Rtwos[,s] <- df.Rtwos[,s]/c(Vtot+post.sample[s,sig.posi]^2) 
  }
  return(df.Rtwos)
}

@

<<'LMGfunction.boot',include=FALSE, tidy=TRUE>>=

#helper function to calculate covariance bootstrap sample

bootcov <- function(df, boot.n){
	len <- nrow(df)
	cov.m <- cov(df)
	l <- dim(cov.m)[1]	
	M.boot <- array(NA, c(l,l,boot.n))
	M.boot[,,1] <- cov(df)
	for (i in 2 :boot.n){
		dfs <- df[sample(1:len, replace=T),]
		M.boot[,,i] <- cov(dfs)
	}
	
	return(M.boot)
}


#Function that includes uncertainty about stochastic predictors by bootstrapping using bootcov
# boot.n is the number of bootstrap samples
# takes boot.n times longer to calculate. 

rtwos.boot<-function(df, post.sample, boot.n){
	# df: Predictor data as data frame
	# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
	
	X <- cov(df) #covariance matrix
	
	#Prepare data frame and rownames by using the combn() function
	
	lst <- list()
	
	pcan <- dim(X)[2]
	n <- (2^pcan)-1
	
	for (i in 1:pcan){
		lst[[i]] <- combn(pcan,i)
	}
	
	var.names <- character(length = 0)
	
	v<- rownames(X)
	
	for(i in 1:length(lst))
	{
		
		for (j in 1:ncol(lst[[i]])){
			cur<- lst[[i]][,j]
			name <- paste0(v[-cur])
			name <- paste(name, collapse = " ")
			var.names <- c(var.names, name)
		}
	}
	
	var.names<-c(rev(var.names), 'all')
	
	var.names[1]<-'none'
	
	size <- nrow(post.sample)  # how many samples
	
	sig.posi <- ncol(post.sample)
	
	df.Rtwos<-array(0,c(n+1,size, boot.n))
	
	dimnames(df.Rtwos)[[1]] <- var.names
	
	boot.M <- bootcov(df, boot.n)
	
	v <- 1:dim(X)[2]
	
	for (b in 1:boot.n){
		
		
		X <- boot.M[,,b]
	
	########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional variance formula.
	
	for (s in 1:size){
		
		sample.s <- post.sample[s,-sig.posi]
		
		Vtot <- sample.s%*%X%*%sample.s  #total explained variance
		
		count=n  
		
		for(i in 1:(length(lst)-1))
		{
			
			for (j in 1:ncol(lst[[i]])){
				cur <- lst[[i]][,j]
				set <- v[-cur]
				matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional variance 
				var.explain<- sample.s[cur]%*%matr%*%sample.s[cur] 
				df.Rtwos[count,s,b] <- Vtot - var.explain  
				count=count-1
			}
		}
		
		df.Rtwos[n+1,s,b] <- Vtot
		
		df.Rtwos[,s,b] <- df.Rtwos[,s,b]/c(Vtot+post.sample[s,sig.posi]^2)
	}
	
	}
	return(df.Rtwos)
}

@

<<'LMGfunction.covm',include=FALSE, tidy=TRUE>>=

#Function to include samples of the predictor covariance matrix
#boot.n = number of samples
#covm = posterior sample of covariance matrix

rtwos.covm<-function(df, post.sample, covm, boot.n){
	# df: Predictor data as data frame
	# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
	
	X <- cov(df) #covariance matrix
	
	#Prepare data frame and rownames by using the combn() function
	
	lst <- list()
	
	pcan <- dim(X)[2]
	n <- (2^pcan)-1
	
	for (i in 1:pcan){
		lst[[i]] <- combn(pcan,i)
	}
	
	var.names <- character(length = 0)
	
	v<- rownames(X)
	
	for(i in 1:length(lst))
	{
		
		for (j in 1:ncol(lst[[i]])){
			cur<- lst[[i]][,j]
			name <- paste0(v[-cur])
			name <- paste(name, collapse = " ")
			var.names <- c(var.names, name)
		}
	}
	
	var.names<-c(rev(var.names), 'all')
	
	var.names[1]<-'none'
	
	size <- nrow(post.sample)  # how many samples
	
	sig.posi <- ncol(post.sample)
	
	df.Rtwos<-array(0,c(n+1,size, boot.n))
	
	dimnames(df.Rtwos)[[1]] <- var.names
	
	v <- 1:dim(X)[2]
	
	
	for (b in 1:boot.n){
		
		
		X <- covm[,,b]
		
		########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
		
		for (s in 1:size){
			
			sample.s <- post.sample[s,-sig.posi]
			
			Vtot <- sample.s%*%X%*%sample.s  #total explained variance
			
			count=n  
			
			for(i in 1:(length(lst)-1))
			{
				
				for (j in 1:ncol(lst[[i]])){
					cur <- lst[[i]][,j]
					set <- v[-cur]
					matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional variance 
					var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  
					df.Rtwos[count,s,b] <- Vtot - var.explain  
					count=count-1
				}
			}
			
			df.Rtwos[n+1,s,b] <- Vtot
			
			df.Rtwos[,s,b] <- df.Rtwos[,s,b]/c(Vtot+post.sample[s,sig.posi]^2) 
		}
		
	}
	return(df.Rtwos)
}

@


\chapter{Examples}

The following chapter presents the Bayesian LMG implementation by two examples. Simulated data is used for the first example. Empirical data is used for the second example.

\section{Simulated Data}

We assume a simple model for the first example: 
\begin{align*} 
&Y_{i} \sim \mathcal{N}(\beta_{0}+x_{1} \beta_{1}+x_{2} \beta_{2}+x_{3} \beta_{3}+x_{4} \beta_{4}, \, \sigma^2),& \\ & \beta_{1} = 0.5, , \beta_{2} = 1, \, \beta_{3} = 2 , \, \beta_{4}=0, \, \sigma^2 = 1 & \\ & \X_{1}, \X_{2},\X_{3},\X_{4} \sim \mathcal{N}(0, 1) 
\end{align*} 
The values of the four predictors are sampled from a standard normal distribution. These values are then multiplied by the regression coefficients. A standard normal distributed error is added to obtain the dependent variable. Fifty observations were sampled. The data generating R-code~\ref{r03:simdata.ex1} can be found in the Appendix.

<<'simudata.example', include=FALSE>>=

x1 <- rnorm(50, 0, 1); x2 <- rnorm(50, 0, 1)
x3 <- rnorm(50, 0, 1); x4 <- rnorm(50, 0, 1)
#b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0.8
b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1

y <- b1*x1 + x2*b2 + b3*x3 + b4*x4 + rnorm(50, 0, 1) 

df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)

@


The model is fitted using the \texttt{rstanarm} package \citep{rstanarm} with the default priors for the regression and $\sigma^2$ parameters. The exact command can be found in R-code~\ref{r03:model.ex1} These default priors are called 'weakly informative priors', because they take into account the order of magnitude of the variables by using the variance of the observed data. Information about these priors can be found in \cite{stanM2017}. A burn-in period of 20000, a sample size of 20000, and a thinning of 20 were chosen, resulting in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:simdata.postsample2}. 

For each posterior sample of the parameters, the $\Rtwo$ value was calculated. The $\Rtwo$ of the submodels was then calculated by the conditional variance formula for each posterior sample. The R-code is found in \ref{r03:LMG}. The resulting $\Rtwo$ values of the first few posterior samples are shown in Table~\ref{tab:simdata.postsample3}.  The thinning is reasonable in this case to reduce the computational burden and to still obtain an appropriate posterior of the $\Rtwo$ values \citep{Link2012}. 

The \texttt{hier.part} package was used to calculate the LMG value for each posterior sample. The LMG posteriors are shown in Table~\ref{tbl:nonstochEx1} . The independent component (I) represents the LMG value. The joint contribution (J) represents the difference from the independent component to the explained variance of the model containing only the predictor itself (T). Assuming stochastic or non-stochastic regressors has an influence on the uncertainty of the LMG values. 

At first, non-stochastic regressors were assumed. The resulting LMG values and joint contributions with a 95\% credible interval are shown in Table~\ref{tbl:nonstochEx1}. An option to display the resulting LMG distribution is shown in Figure ~ \ref{fig:simdata.LMG.plot}.  Using the default weakly informative priors, the LMG distributions obtained from the Bayesian framework were very similar to the bootstrap confidence intervals, assuming non-stochastic predictors of the LMG estimates obtained from the \texttt{relaimpo} package, as shown in Table~\ref{tbl:nonstochEx1relamip}. 



<<'simdata.postsample', include =FALSE>>=
post2 <- stan_glm(y ~ 1 + x1 + x2 + x3 + x4,
                  data = df,
                  chains = 1, cores =1, iter=40000, thin=20)


@

<<'simdata.postsample2', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=

#posterior sample
post.sample <- as.matrix(post2)

dt <- data.frame(post.sample[1:10,2:6])
rownames(dt)<-c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6', 'sample 7', 'sample 8', 'sample 9', 'sample 10')
#example of the first 10 posterior samples
kable(dt, format = "latex", booktabs=TRUE, linesep = c("", "", "", ""), caption = 'Samples from the posterior distributions of the regression parameters') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

#no need for the intercept, last parameter is sigma
post.sample <- post.sample[,-1]
@

<<'simdata.postsample3', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
#data frame with all submodels
df.rtwos <-rtwos(df[,2:5], post.sample)

colnames(df.rtwos)[1:6] <- c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6')
kable(df.rtwos[,1:6], format = 'latex', booktabs=TRUE, linesep = c("", "", "", ""), caption = '$ \\Rtwo$ for all submodels for the first six posterior samples')  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
@


<<'simdata.LMG', cache=FALSE, include=FALSE, tidy=TRUE>>=

# prepare data frame for LMG values

LMG.Vals.I<-matrix(0, 4, dim(df.rtwos)[2])

LMG.Vals.J<-matrix(0, 4, dim(df.rtwos)[2])

LMG.Vals.T<-matrix(0, 4, dim(df.rtwos)[2])


for(i in 1:dim(df.rtwos)[2]){
  
  gofn<-df.rtwos[,i]
  
  obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
  
  LMG.Vals.I[,i]=obj.Gelman$IJ[,1]
  LMG.Vals.J[,i]=obj.Gelman$IJ[,2]
	LMG.Vals.T[,i]=obj.Gelman$IJ[,3]
}

varnames <- row.names(obj.Gelman$IJ)


# posterior LMG distribution of each variable
quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))


# some example how it could be displayed
dat <- data.frame(t(LMG.Vals.I))

pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart



# Comparison to relaimpo package

fit <- lm(y~., data=df)

######## compare to relimp package

run<-boot.relimp(fit, fixed=TRUE)

ci <- booteval.relimp(run, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))@mark

ci <- as.matrix(ci)
ci <- matrix(as.numeric(ci), 4 , 3)
colnames(ci) <- c('LMG', 'l.b', 'u.b')
rownames(ci) <- c('X1', 'X2', 'X3', 'X4')


@

<<'simdata.LMG.sto', cache=FALSE, include=FALSE>>=

#Code assuming stochastic predictors
run.stochastic<-boot.relimp(fit, fixed=FALSE)

ci.s <- booteval.relimp(run.stochastic, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))@mark

ci.s <- as.matrix(ci.s)
ci.s <- matrix(as.numeric(ci.s), 4 , 3)
colnames(ci.s) <- c('LMG', 'l.b', 'u.b')
rownames(ci.s) <- c('X1', 'X2', 'X3', 'X4')


@


<<'simdata.LMG.plot', cache=TRUE, echo=FALSE, fig.cap='Posterior distribution of LMG values.'>>=
pairs.chart
@

\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[3]} & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1}
\end{table}




\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{ci[1,1]} (\Sexpr{ci[1,2]}, \Sexpr{ci[1,3]})  & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{ci[2,1]} (\Sexpr{ci[2,2]}, \Sexpr{ci[2,3]})  & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})   \\ 
\Sexpr{varnames[3]} & \Sexpr{ci[3,1]} (\Sexpr{ci[3,2]}, \Sexpr{ci[3,3]})  & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{ci[4,1]} (\Sexpr{ci[4,2]}, \Sexpr{ci[4,3]}) & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relamip}
\end{table}

\FloatBarrier


In this example, we know, that the predictor values were sampled from a normal distribution. It would therefore be more reasonable to assume stochastic predictors. Under the assumption of weak exogeinity and conditional independence, the posterior distributions of the regression parameters $\bbeta$ are valid for non-stochastic and stochastic predictors. However, the uncertainty about the LMG values needs to include the uncertainty about the covariance matrix. If we know the distribution of the predictors we can incorporate this information and obtain the posterior distribution of the covariance matrix. The package \texttt{Jags} was used for inference about the covariance matrix in a Bayesian way. As an alternative, non-parametric bootstrap was used for inference about the covariance matrix. 

Using the bootstrap samples of the covariance matrix or samples from the posterior covariance matrix produced  very similar LMG distributions. Bootstrap seems to be a valuable option for stochastic predictors when the distribution of the predictors is unknown. Even when the distribution is known, the difference seems to be tiny. A benefit of going the full Bayesian way is that we can also include prior knowledge about the covariance matrix. Using the default priors further produced very similar LMG distribution as using the non-parametric bootstrap option of the \texttt{relaimpo} package. Table~\ref{tbl:nonstochEx1relaimpstoch} shows the LMG values of these approaches. For stochastic predictors, in contrast to non-stochastic predictors, the uncertainty about the covariance matrix is reflected in the larger credible intervals. Even when the exact regression parameters were known, there would a lot of uncertainty in the LMG values caused by the uncertainty about the covariance matrix. 

<<'simdata.LMG.boot', include=FALSE, cache =TRUE>>=



#----------------------------------------------------------------------------------------


df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)

n.boot = 10

LMG.Vals.I.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.J.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.T.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.I.boot[,i, b]=obj.Gelman$IJ[,1]
	LMG.Vals.J.boot[,i, b]=obj.Gelman$IJ[,2]
	LMG.Vals.T.boot[,i, b]=obj.Gelman$IJ[,3]

}
	
}

quantile(c(LMG.Vals.I.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[4,1:1000,]), c(0.025, 0.5, 0.975))

#very similar values as in the confidence intervals for stochastic predictors
t1 <- c(LMG.Vals.I.boot[1,,])
t2 <- c(LMG.Vals.I.boot[2,,])
t3 <- c(LMG.Vals.I.boot[3,,])
t4 <- c(LMG.Vals.I.boot[4,,])

dat.b<-data.frame(cbind(t1,t2,t3,t4))
cor(dat.b)


@



<<'simdata.LMG.cov', include=FALSE, cache=TRUE, tidy=TRUE>>=

zy = df[,2:5]

#----------------------------------------------------------------------------
# In the following example we know that the $\X$ are coming from a normal distribution. The covariance matrix of the distribution is estimated in aBayesian way. The package Jags is used.
#----------------------------------------------------------------------------

# Assemble data for sending to JAGS:
dataList = list(
	zy = zy ,
	Ntotal =  nrow(zy) ,
	Nvar = ncol(zy) ,
	zRscal = ncol(zy) ,  # for dwish prior
	zRmat = diag(x=1,nrow=ncol(zy))  
)

# Define the model:
modelString = "
model {
for ( i in 1:Ntotal ) {
zy[i,1:Nvar] ~ dmnorm( zMu[1:Nvar] , zInvCovMat[1:Nvar,1:Nvar] ) 
}
for ( varIdx in 1:Nvar ) { zMu[varIdx] ~ dnorm( 0 , 1/2^2 ) }
zInvCovMat ~ dwish( zRmat[1:Nvar,1:Nvar] , zRscal )
# Convert invCovMat to sd and correlation:
zCovMat <- inverse( zInvCovMat )

}
" # close quote for modelString
writeLines( modelString , con="Jags-MultivariateNormal-model.txt" )

# Run the chains:
nChain = 3
nAdapt = 500
nBurnIn = 500
nThin = 10
nStepToSave = 20000
require(rjags)
jagsModel = jags.model( file="Jags-MultivariateNormal-model.txt" , 
												data=dataList , n.chains=nChain , n.adapt=nAdapt )
update( jagsModel , n.iter=nBurnIn )
codaSamples = coda.samples( jagsModel , 
														variable.names=c('zCovMat') ,
														n.iter=nStepToSave/nChain*nThin , thin=nThin )


# Convergence diagnostics:
parameterNames = varnames(codaSamples) # get all parameter names


# Examine the posterior distribution:
mcmcMat = as.matrix(codaSamples)
chainLength = nrow(mcmcMat)

covMat <- array(NA, c(4,4,chainLength))

for (i in 1:chainLength){
covMat[1:4,1:4,i]<-matrix(mcmcMat[i,], 4, 4)
}

covMat <- covMat[1:4,1:4,sample(1:20000, replace=F)] 


df.rtwos <-rtwos.covm(df[,2:5], post.sample, covMat, 10)


n.boot = 10

LMG.Vals.I.covm<-array(0, c(4,dim(df.rtwos)[2], n.boot))
LMG.Vals.J.covm<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.T.covm<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))


for (b in 1:n.boot){
	
	for(i in 1:dim(df.rtwos)[2]){
		
		gofn<-df.rtwos[,i,b]
		
		obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
		
		LMG.Vals.I.covm[,i, b]=obj.Gelman$IJ[,1]
		LMG.Vals.J.covm[,i, b]=obj.Gelman$IJ[,2]
		LMG.Vals.T.covm[,i, b]=obj.Gelman$IJ[,3]

	}
	
}

quantile(c(LMG.Vals.I.covm[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.covm[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.covm[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.covm[4,1:1000,]), c(0.025, 0.5, 0.975))




@


\begin{table}[h]
\caption{LMG values assuming stochastic predictors with 95\% CI.}
\centering
\begin{tabular}{clll}
   \toprule
  & \multicolumn{1}{c}{\textbf{Relaimpo}} & \multicolumn{2}{c}{\textbf{Bayesian framework}} \\ \cmidrule(r){2-2}\cmidrule(l){3-4}
 \textbf{Variable} &  & \multicolumn{1}{c}{nonparameteric bootstrap}& \multicolumn{1}{c}{covariance inference} \\
 \midrule
\Sexpr{varnames[1]} & \Sexpr{ci.s[1,1]} (\Sexpr{ci.s[1,2]}, \Sexpr{ci.s[1,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[3]}) &  \Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
\Sexpr{varnames[2]} & \Sexpr{ci.s[2,1]} (\Sexpr{ci.s[2,2]}, \Sexpr{ci.s[2,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
\Sexpr{varnames[3]} & \Sexpr{ci.s[3,1]} (\Sexpr{ci.s[3,2]}, \Sexpr{ci.s[3,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
\Sexpr{varnames[4]} & \Sexpr{ci.s[4,1]} (\Sexpr{ci.s[4,2]}, \Sexpr{ci.s[4,3]}) & \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[3]}) & \Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
   \bottomrule
\end{tabular}
\label{tbl:nonstochEx1relaimpstoch}
\end{table}


<<'covuncertainty', include=FALSE, cache=FALSE>>=

#How much variance is effectively in the bootstrap matrix when we know the regression parameters.

#fake post sample

x1 <- rnorm(50, 0, 1); x2 <- rnorm(50, 0, 1)
x3 <- rnorm(50, 0, 1); x4 <- rnorm(50, 0, 1)
#b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0
b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1

y <- b1*x1 + x2*b2 + b3*x3 + b4*x4 + rnorm(50, 0, 1) 

df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)

post.sample <- matrix(1, 1000,5)

df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)

n.boot = 10

LMG.Vals.I.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.I.boot[,i, b]=obj.Gelman$IJ[,1]
}
	
}

quantile(c(LMG.Vals.I.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[4,1:1000,]), c(0.025, 0.5, 0.975))



cov(df[,2:5])
@

<<'simdata.LMG.shrink', include=FALSE, cache =TRUE>>=

#Comparison of sample covariance and shrink covariance matrix

cov(df[,2:5])
cov.shrink(df[,2:5])

@


\clearpage

\section{Empirical Data}
In the following section, the Bayesian LMG implementation is applied on an empirical dataset containing test scores of pupils (N = 301) from a study by \cite{Holzinger1939} available in the R package \texttt{MBESS} \citep{MBESS}. This dataset was used in \cite{Nimon2008} to present commonality analysis, which is another variance decomposition technique. Scores from a paragraph comprehension test (paragrap) were predicted by four verbal tests:  general-information (general),  sentence-comprehension (sentence) ,  word-classification (wordc), and  word-meaning (wordm) (Table~\ref{table:hs.data}). 

The aim of the regression analysis was to determine the  association between verbal ability and paragraph comprehension.  
An overview of the data is shown in Figure~\ref{fig:empi.lmg.plot}. The regression results are shown in Table~\ref{tbl:empirical.reg}). A novice researcher may wrongly conclude, that there is little association between the "non-significant" predictors (general information and word-classification) and paragraph comprehension. Given the other predictors are already included in the model, the predictors seem not to provide  much information about the expected paragraph comprehension ability. However, it should not be concluded from this regression table, that the association between any of these "non-significant" predictors and the dependent variable is unimportant. As shown in Figure~\ref{fig:empi.lmg.plot}, the correlations between the predictors are rather high. The LMG metric may therefore provide new information about the importance of each predictor. 

The Bayesian regression model was fitted in \texttt{rstanarm}. The default priors were used for the regression coefficients and the $\sigma^2$ parameter. A burn-in period of 20000, a sample size of 20000, and a thinning of 20 resulted in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:empirical.data.postsample2}. The resulting $\Rtwo$ of these posterior samples are shown in Table~\ref{tab:empirical.data.postsample3}. The LMG values were calculated by using \texttt{hier.part}. The  independent component (I), joint contribution (J), and total explained variance in a one-predictor model (T) are shown in Table~\ref{tbl:empirical.ijt}. Sentence-comprehension and word-meaning seem to be the most important predictors by applying the LMG metric. However, none of the predictors seem to be unimportant.  The joint contributions of each predictor were quite large.

For comparison purposes, the LMG metric was additionally calculated with the \texttt{relaimpo} package using parametric bootstrapping. The confidence intervals of  \texttt{relaimpo} were almost identical to the credible intervals of the Bayesian framework (Table~\ref{tbl:empirical.relaimp.comp}). Assuming stochastic or non-stochastic predictors resulted also in almost identical uncertainty estimates with such a large sample size (Table\ref{tbl:empirical.relaimp.comp.stoch}). 







\begin{table}
\centering
\caption{Variable description}
\begin{tabular}{l l}
  \toprule			
  Variable Name & Description  \\   \midrule  
  paragrap & scores on paragraph comprehension test  \\
  general & scores on general information test \\
  sentence & scores on sentence completion test\\
  wordc & scores on word classification test \\
  wordm & scores on word meaning test \\
  \bottomrule  
\end{tabular}
\label{table:hs.data}
\end{table}

<<results='asis', echo = FALSE, fig.cap='Test scores from Holzinger and Swineford\'s  (1939) Study. N=301'>>=
data('HS.data', package='MBESS')
hs.data <- HS.data[,c('paragrap','general', 'sentence' , 'wordc' , 'wordm')]
pairs.chart <- ggpairs(hs.data, lower = list(continuous = "cor"), upper = list(continuous = "points", combo = "dot")) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart

@


<<'empirical.data.LMG', include=FALSE, cache =TRUE>>=


bayes.hs <- stan_glm(paragrap ~ . ,
                  data = hs.data,
                  chains = 1, cores = 1, iter=40000, thin=20)

# Comparison to relaimpo package



fit <- lm(paragrap~., data= hs.data)


######## compare to relimp package

run<-boot.relimp(fit, fixed=TRUE)

ci <- booteval.relimp(run, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))@mark

ci <- as.matrix(ci)
ci <- matrix(as.numeric(ci), 4 , 3)
colnames(ci) <- c('LMG', 'l.b', 'u.b')
rownames(ci) <- c('X1', 'X2', 'X3', 'X4')

#Code assuming stochastic predictors
run.stochastic<-boot.relimp(fit, fixed=TRUE)

ci.s <- booteval.relimp(run.stochastic, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))@mark

ci.s <- as.matrix(ci.s)
ci.s <- matrix(as.numeric(ci.s), 4 , 3)
colnames(ci.s) <- c('LMG', 'l.b', 'u.b')
rownames(ci.s) <- c('X1', 'X2', 'X3', 'X4')
@

<<'empirical.data.reg', results='asis', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
res.table <- tableRegression(fit, caption='Regression of paragraph comprehension on verbal tests.', caption.placement = "top", label = 'tbl:empirical.reg')
@

<<'empirical.data.postsample2', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
#posterior sample
post.sample <- as.matrix(bayes.hs)

dt <- data.frame(post.sample[1:10,2:6])
rownames(dt)<-c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6', 'sample 7', 'sample 8', 'sample 9', 'sample 10')
#example of the first 10 posterior samples
kable(dt, format = "latex", booktabs=TRUE, linesep = c("", "", "", ""), caption = 'Samples from the posterior distributions of the regression parameters') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

#no need for the intercept, last parameter is sigma
post.sample <- post.sample[,-1]
@

<<'empirical.data.postsample3', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
#data frame with all submodels
df.rtwos <-rtwos(hs.data[,2:5], post.sample)

colnames(df.rtwos)[1:6] <- c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6')
kable(df.rtwos[,1:6], format = 'latex', booktabs=TRUE, linesep = c("", "", "", ""), caption = '$ \\Rtwo$ for all submodels for the first six posterior samples')  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
@

<<'empirical.LMG.values', include=FALSE,  cache=TRUE>>=

LMG.Vals.I<-matrix(0, 4, dim(df.rtwos)[2])

LMG.Vals.J<-matrix(0, 4, dim(df.rtwos)[2])

LMG.Vals.T<-matrix(0, 4, dim(df.rtwos)[2])


for(i in 1:dim(df.rtwos)[2]){
  
  gofn<-df.rtwos[,i]
  
  obj.Gelman<-partition(gofn, pcan = 4, var.names = names(hs.data[,2:5]))
  
  LMG.Vals.I[,i]=obj.Gelman$IJ[,1]
  LMG.Vals.J[,i]=obj.Gelman$IJ[,2]
	LMG.Vals.T[,i]=obj.Gelman$IJ[,3]
}

varnames <- row.names(obj.Gelman$IJ)


# posterior LMG distribution of each variable
quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))

quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))

quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))




@

<<'empi.lmg.plot', echo = FALSE, fig.cap='LMG posterior distribution of different verbal ability tests'>>=
#Visualization
dat <- data.frame(t(LMG.Vals.I))

pairs.chart <- ggpairs(dat, lower = list(list(combo = "density")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart

@


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{clll}
  \toprule
  \multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[3]} & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.ijt}
\end{table}


\begin{table}[h]
\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{ci[1,1]} (\Sexpr{ci[1,2]}, \Sexpr{ci[1,3]})  & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{ci[2,1]} (\Sexpr{ci[2,2]}, \Sexpr{ci[2,3]})  & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})   \\ 
\Sexpr{varnames[3]} & \Sexpr{ci[3,1]} (\Sexpr{ci[3,2]}, \Sexpr{ci[3,3]})  & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{ci[4,1]} (\Sexpr{ci[4,2]}, \Sexpr{ci[4,3]}) & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp}
\end{table}



<<'empirical.LMG.boot', include=FALSE, cache =TRUE>>=



#----------------------------------------------------------------------------------------


df.rtwos.boot <-rtwos.boot(hs.data[,2:5], post.sample, 10)

n.boot = 10

LMG.Vals.I.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.J.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
LMG.Vals.T.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.I.boot[,i, b]=obj.Gelman$IJ[,1]
	LMG.Vals.J.boot[,i, b]=obj.Gelman$IJ[,2]
	LMG.Vals.T.boot[,i, b]=obj.Gelman$IJ[,3]

}
	
}

quantile(c(LMG.Vals.I.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.I.boot[4,1:1000,]), c(0.025, 0.5, 0.975))

#very similar values as in the confidence intervals for stochastic predictors
t1 <- c(LMG.Vals.I.boot[1,,])
t2 <- c(LMG.Vals.I.boot[2,,])
t3 <- c(LMG.Vals.I.boot[3,,])
t4 <- c(LMG.Vals.I.boot[4,,])

dat.b<-data.frame(cbind(t1,t2,t3,t4))
cor(dat.b)


@

\begin{table}[h]
\caption{Variance decomposition for Stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
\centering
\begin{tabular}{cll}
   \toprule
   &\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
 \textbf{Variable} & \multicolumn{1}{c}{Relaimpo} & \textbf{Bayesian framework}  \\
  \hline
\Sexpr{varnames[1]} & \Sexpr{ci.s[1,1]} (\Sexpr{ci.s[1,2]}, \Sexpr{ci.s[1,3]})  &  \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[2]} & \Sexpr{ci.s[2,1]} (\Sexpr{ci.s[2,2]}, \Sexpr{ci.s[2,3]})  &  \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[3]})   \\ 
\Sexpr{varnames[3]} & \Sexpr{ci.s[3,1]} (\Sexpr{ci.s[3,2]}, \Sexpr{ci.s[3,3]})  &  \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
\Sexpr{varnames[4]} & \Sexpr{ci.s[4,1]} (\Sexpr{ci.s[4,2]}, \Sexpr{ci.s[4,3]}) &  \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
   \bottomrule
\end{tabular}
\label{tbl:empirical.relaimp.comp.stoch}
\end{table}




