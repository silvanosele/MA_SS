% LaTeX file for Chapter 03

<<set-parent03, echo=FALSE, cache=FALSE>>=
set_parent('Main.Rnw')
@

<<'preamble03',include=FALSE>>=
library(knitr); library(hier.part); library(rstanarm);  library(GGally); library(ggplot2); library(relaimpo); library(rjags); library(corpcor); library(brinla); library(stargazer)

opts_chunk$set(
    fig.path='figure/ch03_fig', 
    self.contained=FALSE,
    cache=FALSE
) 


#function to calculate R2 data frame
rtwos<-function(X, post.sample){
  # X: Predictor data as data frame
  # post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
  
  X <- cov(X) #covariance matrix
  
  #Prepare data frame and rownames by using the combn() function
  
  lst <- list()
  
  pcan <- dim(X)[2]
  n <- (2^pcan)-1
  
  for (i in 1:pcan){
    lst[[i]] <- combn(pcan,i)
  }
  
  var.names <- character(length = 0)
  
  v<- 1:length(lst)
  
  for(i in 1:length(lst))
  {
    
    for (j in 1:ncol(lst[[i]])){
      cur<- lst[[i]][,j]
      name <- paste0('x',v[-cur])
      name <- paste(name, collapse = " ")
      var.names <- c(var.names, name)
    }
  }
  
  var.names<-c(rev(var.names), 'all')
  
  var.names[1]<-'none'
  
  size <- nrow(post.sample)  # how many samples
  
  sig.posi <- ncol(post.sample)
  
  df.Rtwos<-data.frame(matrix(0,n+1,size))
  
  row.names(df.Rtwos) <- var.names
  
  ########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
  
  for (s in 1:size){
    
    sample.s <- post.sample[s,-sig.posi]
    
    Vtot <- sample.s%*%X%*%sample.s  #total explained variance
    
    count=n  
    
    for(i in 1:(length(lst)-1))
    {
      
      for (j in 1:ncol(lst[[i]])){
        cur <- lst[[i]][,j]
        set <- v[-cur]
        matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
        var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
        df.Rtwos[count,s] <- Vtot - var.explain  
        count=count-1
      }
    }
    
    df.Rtwos[n+1,s] <- Vtot
    
    df.Rtwos[,s] <- df.Rtwos[,s]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
  }
  
  return(df.Rtwos)
}



bootcov <- function(df, boot.n){
	len <- nrow(df)
	cov.m <- cov(df)
	l <- dim(cov.m)[1]	
	M.boot <- array(NA, c(l,l,boot.n))
	M.boot[,,1] <- cov(df)
	for (i in 2 :boot.n){
		dfs <- df[sample(1:len, replace=T),]
		M.boot[,,i] <- cov(dfs)
	}
	
	return(M.boot)
}


#Function that includes uncertainty about stochastic predictors by bootstrapping using bootcov
#n is the number of bootstrap samples it should calculate
# takes boot.n times longer to calculate. 
rtwos.boot<-function(df, post.sample, boot.n){
	# X: Predictor data as data frame
	# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
	
	X <- cov(df) #covariance matrix
	
	#Prepare data frame and rownames by using the combn() function
	
	lst <- list()
	
	pcan <- dim(X)[2]
	n <- (2^pcan)-1
	
	for (i in 1:pcan){
		lst[[i]] <- combn(pcan,i)
	}
	
	var.names <- character(length = 0)
	
	v<- 1:length(lst)
	
	for(i in 1:length(lst))
	{
		
		for (j in 1:ncol(lst[[i]])){
			cur<- lst[[i]][,j]
			name <- paste0('x',v[-cur])
			name <- paste(name, collapse = " ")
			var.names <- c(var.names, name)
		}
	}
	
	var.names<-c(rev(var.names), 'all')
	
	var.names[1]<-'none'
	
	size <- nrow(post.sample)  # how many samples
	
	sig.posi <- ncol(post.sample)
	
	df.Rtwos<-array(0,c(n+1,size, boot.n))
	
	dimnames(df.Rtwos)[[1]] <- var.names
	
	boot.M <- bootcov(df, boot.n)
	
	for (b in 1:boot.n){
		
		
		X <- boot.M[,,b]
	
	########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
	
	for (s in 1:size){
		
		sample.s <- post.sample[s,-sig.posi]
		
		Vtot <- sample.s%*%X%*%sample.s  #total explained variance
		
		count=n  
		
		for(i in 1:(length(lst)-1))
		{
			
			for (j in 1:ncol(lst[[i]])){
				cur <- lst[[i]][,j]
				set <- v[-cur]
				matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
				var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
				df.Rtwos[count,s,b] <- Vtot - var.explain  
				count=count-1
			}
		}
		
		df.Rtwos[n+1,s,b] <- Vtot
		
		df.Rtwos[,s,b] <- df.Rtwos[,s,b]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
	}
	
	}
	return(df.Rtwos)
}


#Function to include samples of the predictor covariance matrix.

rtwos.covm<-function(df, post.sample, covm, boot.n){
	# X: Predictor data as data frame
	# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
	
	X <- cov(df) #covariance matrix
	
	#Prepare data frame and rownames by using the combn() function
	
	lst <- list()
	
	pcan <- dim(X)[2]
	n <- (2^pcan)-1
	
	for (i in 1:pcan){
		lst[[i]] <- combn(pcan,i)
	}
	
	var.names <- character(length = 0)
	
	v<- 1:length(lst)
	
	for(i in 1:length(lst))
	{
		
		for (j in 1:ncol(lst[[i]])){
			cur<- lst[[i]][,j]
			name <- paste0('x',v[-cur])
			name <- paste(name, collapse = " ")
			var.names <- c(var.names, name)
		}
	}
	
	var.names<-c(rev(var.names), 'all')
	
	var.names[1]<-'none'
	
	size <- nrow(post.sample)  # how many samples
	
	sig.posi <- ncol(post.sample)
	
	df.Rtwos<-array(0,c(n+1,size, boot.n))
	
	dimnames(df.Rtwos)[[1]] <- var.names
	
	for (b in 1:boot.n){
		
		
		X <- covm[,,b]
		
		########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
		
		for (s in 1:size){
			
			sample.s <- post.sample[s,-sig.posi]
			
			Vtot <- sample.s%*%X%*%sample.s  #total explained variance
			
			count=n  
			
			for(i in 1:(length(lst)-1))
			{
				
				for (j in 1:ncol(lst[[i]])){
					cur <- lst[[i]][,j]
					set <- v[-cur]
					matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
					var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
					df.Rtwos[count,s,b] <- Vtot - var.explain  
					count=count-1
				}
			}
			
			df.Rtwos[n+1,s,b] <- Vtot
			
			df.Rtwos[,s,b] <- df.Rtwos[,s,b]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
		}
		
	}
	return(df.Rtwos)
}



@


\chapter{Examples}

In the following section the Bayesian LMG implementation is presented on two examples. The first examples simulates data, the second examples uses real data.

\section{Simulated Data}

Lets assume a simple model: 

\begin{align} 
&Y_{i} \sim \mathcal{N}(\beta_{0}+x_{1} \beta_{1}+x_{2} \beta_{2}+x_{3} \beta_{3}+x_{4} \beta_{4}, \sigma^2),
\end{align} 


The values of the four predictors are sampled from a standard normal distribution. These values are then multiplied by the regression coefficients to obatin the dependent variable. A standard normal distributed error is added. Fifty observations were sampled.

The following Code was used to simulate the data :

<<'simudata.example'>>=

x1 <- rnorm(50, 0, 1); x2 <- rnorm(50, 0, 1)
x3 <- rnorm(50, 0, 1); x4 <- rnorm(50, 0, 1)
#b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0
b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1

y <- b1*x1 + x2*b2 + b3*x3 + b4*x4 + rnorm(50, 0, 1) 

df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)

@


The model is fitted using the rstanarm package with the default priors for the regression and $\sigma^2$ parameter. For computational reasons a small burning periode of 1000 and a sample size of 1000 were chosen. For each posterior sample of the parameters the $\Rtwo$ value is calculated. The $\Rtwo$ of the submodels is then calculated by the conditional variance formula for each posterior sample.


<<'simdata.postsample', cache=TRUE>>=
post2 <- stan_glm(y ~ 1 + x1 + x2 + x3 + x4,
                  data = df,
                  chains = 2, cores = 1)

#posterior sample
post.sample <- as.matrix(post2)

#example of the first 10 posterior samples
post.sample[1:10,]

#no need for the intercept, last parameter is sigma
post.sample <- post.sample[,-1]


#data frame with all submodels
df.rtwos <-rtwos(df[,2:5], post.sample)

df.rtwos[,1:5]
@

After the $\Rtwo$ for each posterior sample and their corresponding submodels is calculated, the package hier.part is used to calculate the LMG value for each posterior sample.

<<'simdata.LMG', cache=TRUE>>=

# prepare data frame for LMG values

LMG.Vals<-matrix(0, 4, dim(df.rtwos)[2])

for(i in 1:dim(df.rtwos)[2]){
  
  gofn<-df.rtwos[,i]
  
  obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
  
  LMG.Vals[,i]=obj.Gelman$IJ[,1]
}


# posterior LMG distribution of each variable
quantile(LMG.Vals[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[4,], c(0.025, 0.5, 0.975))

# some example how it could be displayed
dat <- data.frame(t(LMG.Vals))

pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart



# Comparison to relaimpo package

fit <- lm(y~., data=df)

######## compare to relimp package

run<-boot.relimp(fit, fixed=TRUE)

booteval.relimp(run, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))

@


Using the default uninformative priors, the LMG distributions obtained from the Bayesian framework are very similar to the bootstrap confidence intervals of the LMG estimates obtained from the relaimpo package. In both cases fixed regressors are assumed. In the example above the predictors were sampled from a normal distribution. It would therefore be more reasonable to assume stochastic predictors. As noted in ... under the assumption of weak exogeinity and conditional independence the posterior distributions of the regression paramters $\bbeta$ are valid for fixed and stochastic predictors. Inference about the covariance matrix can be seen as an independent problem. G recommends in most cases to use the non fixed regressor option when calculating bootstrap confidence intervals. The confidence intervals will then in general be a bit larger. If we want to include this uncertainty in the Bayesian framework, we would need some ideas about the distribution of the predictor variables $\X$. It is then possible to obtain poserior distributions of their corresponding covariance matrix. As a practical solution nonparametric bootstrap may be used to include the uncertainty of the covariance matrix. The following code includes the uncertainty of the stochastic predictors. 


<<'simdata.LMG.boot',cache =TRUE>>=

#Code assuming stochastic predictors
run.stochastic<-boot.relimp(fit, fixed=FALSE)

booteval.relimp(run.stochastic, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))


#----------------------------------------------------------------------------------------


df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)

n.boot = 10

LMG.Vals.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.boot[,i, b]=obj.Gelman$IJ[,1]
}
	
}

quantile(c(LMG.Vals.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[4,1:1000,]), c(0.025, 0.5, 0.975))

#very similar values as in the confidence intervals for stochastic predictors


#what if we have prior knowledge
my_prior <- normal(location = c(1, 1,1,1), scale = c(0.01, 0.01,0.01,0.01), autoscale = FALSE)

post2 <- stan_glm(y ~ 1 + x1 + x2 + x3 + x4,
                  data = df, prior = my_prior,
                  chains = 2, cores = 1)

#posterior sample
post.sample <- as.matrix(post2)

#example of the first 10 posterior samples
post.sample[1:10,]

#no need for the intercept, last parameter is sigma
post.sample <- post.sample[,-1]


#data frame with all submodels


df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)

n.boot = 10

LMG.Vals.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.boot[,i, b]=obj.Gelman$IJ[,1]
}
	
}

quantile(c(LMG.Vals.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[4,1:1000,]), c(0.025, 0.5, 0.975))


@

In the following example we know that the $\X$ are coming from a normal distribution. The covariance matrix of the distribution is estimated in a Bayesian way. The package Jags is used.

<<'simdata.LMG.cov', cache=TRUE>>=

zy = df[,2:5]

#----------------------------------------------------------------------------
# The rest can remain unchanged, except for the specification of difference of
# correlations at the very end.
#----------------------------------------------------------------------------

# Load some functions used below:
# Install the ellipse package if not already:
# Standardize the data:

#zy = apply(y,2,function(yVec){(yVec-mean(yVec))/sd(yVec)})
# Assemble data for sending to JAGS:
dataList = list(
	zy = zy ,
	Ntotal =  nrow(zy) ,
	Nvar = ncol(zy) ,
	# Include original data info for transforming to original scale:
	# For wishart (dwish) prior on inverse covariance matrix:
	zRscal = ncol(zy) ,  # for dwish prior
	zRmat = diag(x=1,nrow=ncol(zy))  # Rmat = diag(apply(y,2,var))
)

# Define the model:
modelString = "
model {
for ( i in 1:Ntotal ) {
zy[i,1:Nvar] ~ dmnorm( zMu[1:Nvar] , zInvCovMat[1:Nvar,1:Nvar] ) 
}
for ( varIdx in 1:Nvar ) { zMu[varIdx] ~ dnorm( 0 , 1/2^2 ) }
zInvCovMat ~ dwish( zRmat[1:Nvar,1:Nvar] , zRscal )
# Convert invCovMat to sd and correlation:
zCovMat <- inverse( zInvCovMat )

}
" # close quote for modelString
writeLines( modelString , con="Jags-MultivariateNormal-model.txt" )

# Run the chains:
nChain = 3
nAdapt = 500
nBurnIn = 500
nThin = 10
nStepToSave = 20000
require(rjags)
jagsModel = jags.model( file="Jags-MultivariateNormal-model.txt" , 
												data=dataList , n.chains=nChain , n.adapt=nAdapt )
update( jagsModel , n.iter=nBurnIn )
codaSamples = coda.samples( jagsModel , 
														variable.names=c('zCovMat') ,
														n.iter=nStepToSave/nChain*nThin , thin=nThin )


# Convergence diagnostics:
parameterNames = varnames(codaSamples) # get all parameter names


# Examine the posterior distribution:
mcmcMat = as.matrix(codaSamples)
chainLength = nrow(mcmcMat)

covMat <- array(NA, c(4,4,chainLength))

for (i in 1:chainLength){
covMat[1:4,1:4,i]<-matrix(mcmcMat[i,], 4, 4)
}

covMat <- covMat[1:4,1:4,sample(1:20000, replace=F)] 


df.rtwos <-rtwos.covm(df[,2:5], post.sample, covMat, 10)


n.boot = 10

LMG.Vals<-array(0, c(4,dim(df.rtwos)[2], n.boot))

for (b in 1:n.boot){
	
	for(i in 1:dim(df.rtwos)[2]){
		
		gofn<-df.rtwos[,i,b]
		
		obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
		
		LMG.Vals[,i, b]=obj.Gelman$IJ[,1]
	}
	
}

quantile(c(LMG.Vals[1,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals[2,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals[3,1:1000,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals[4,1:1000,]), c(0.025, 0.5, 0.975))





@

Using the bootstrap samples of the covariance matrix or samples from the posterior covariance matrix resulted in very similar LMG values. Bootstrap seems to be a valuable option for stochastic predictors when the distribution of the predictors is unknown. Even when the distribution is known the difference seems to be tiny. A benefit of going the full Bayesian way is that we can also include prior knowledge of the covariance matrix. For stochastic predictors the uncertainty about the covariance matrix is reflected in the large credible intervals. Even when we would knew the exact regression parameters, there is alot of uncertainty in the LMG values caused by the uncertainty about the covariance matrix. Code xx shows the uncertainty about the LMG values caused by the uncertainty about the covariance matrix. 

Another interesting option in some cases would be to use the shrinkage estimate of the covariance matrix. 


<<'covuncertainty', cache=FALSE>>=

#How much variance is effectively in the bootstrap matrix when we know the regression parameters.

#fake post sample

x1 <- rnorm(50, 0, 1); x2 <- rnorm(50, 0, 1)
x3 <- rnorm(50, 0, 1); x4 <- rnorm(50, 0, 1)
#b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0
b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1

y <- b1*x1 + x2*b2 + b3*x3 + b4*x4 + rnorm(50, 0, 1) 

df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)

post.sample <- matrix(1, 1,5)

df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 1000)

n.boot = 1000

LMG.Vals.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))

for (b in 1:n.boot){
	
for(i in 1:dim(df.rtwos.boot)[2]){
	
	gofn<-df.rtwos.boot[,i,b]
	
	obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
	
	LMG.Vals.boot[,i, b]=obj.Gelman$IJ[,1]
}
	
}

quantile(c(LMG.Vals.boot[1,1,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[2,1,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[3,1,]), c(0.025, 0.5, 0.975))
quantile(c(LMG.Vals.boot[4,1,]), c(0.025, 0.5, 0.975))

dat <- data.frame(t(LMG.Vals.boot[1:4,1,]))
pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart

cov(df[,2:5])
@

<<'simdata.LMG.shrink',cache =TRUE>>=

#Comparison of sample covariance and shrink covariance matrix

cov(df[,2:5])
cov.shrink(df[,2:5])

@

\section{Real World Data}


The following example data are taken from the book Bayesian Regression Modeling with INLA. The data were about air pollution in 41 cities in the United States originally published in Everitt (2006). The data consits of the SO2 level as the dependent variable and six explanatory variables 
Two of the explanatory variables are are related to human ecology (pop, manuf) and four others are related to climate (negtemp, wind, precip, days).


\begin{table}
\caption{Variable description of air pollution data}
\begin{tabularx}{\textwidth}{|l|X|l|}
  \hline			
  Variable Name & Description & Codes/Values \\   \hline  
  SO2 & sulfur dioxide content of air & microgrames per cubic meter \\
  negtemp & negative value of average & fahrenheit\\
  manuf & number of manufacturing enterprises employing 20 or more workers & integers \\
  pop & population size in thousands (1970 census) & numbers \\
  wind & average wind speed & miles per hour \\
  precip & average annual percipitation & inches \\
  days & average munber of days with precipitation per year & integers \\
  \hline  
\end{tabularx}
\label{table:airpollutiondata}
\end{table}

<<results='asis', echo = FALSE>>=
data('usair', package='brinla')
pairs.chart <- ggpairs(usair[,-1], lower = list(continuous = "cor"), upper = list(continuous = "points", combo = "dot")) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart

@

A simple linear regression model including the dependent variable SO2 and the six explanatory variables is fitted with the lm command in R. The $R^2$ of the full model is 0.670. 

<<'real.data.LMG',cache =TRUE>>=

usair.lm <- lm(SO2~., data = usair)


#Code assuming stochastic predictors
run<-boot.relimp(usair.lm, fixed=TRUE)

booteval.relimp(run, bty = "perc", level = 0.95,
                sort = FALSE, norank = TRUE, nodiff = TRUE,
                typesel = c("lmg"))


bayes.usair <- stan_glm(SO2 ~ . ,
                  data = usair,
                  chains = 4, cores = 4)

#posterior sample
post.sample <- as.matrix(bayes.usair)

#example of the first 10 posterior samples
post.sample[1:10,]

#no need for the intercept, last parameter is sigma
post.sample <- post.sample[,-1]

#data frame with all submodels
df.rtwos <-rtwos(usair[,2:7], post.sample)

df.rtwos[,1:3]


# prepare data frame for LMG values

LMG.Vals<-matrix(0, 6, dim(df.rtwos)[2])

for(i in 1:dim(df.rtwos)[2]){
  
  gofn<-df.rtwos[,i]
  
  obj.Gelman<-partition(gofn, pcan = 6, var.names = names(usair[,2:7]))
  
  LMG.Vals[,i]=obj.Gelman$IJ[,1]
}

# posterior LMG distribution of each variable
quantile(LMG.Vals[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[4,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[5,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[6,], c(0.025, 0.5, 0.975))


#Visualization
dat <- data.frame(t(LMG.Vals))

pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart

#use prior knowledge
my_prior <- normal(location = c(1, 0.05,-0.03,-3, 0, 0), scale = c(0.1, 0.01,0.01,0.1, 0.1, 0.1), autoscale = FALSE)


bayes.usair <- stan_glm(SO2 ~ . ,
                  data = usair, prior = my_prior,
                  chains = 4, cores = 4)

#posterior sample
post.sample <- as.matrix(bayes.usair)

#example of the first 10 posterior samples
post.sample[1:10,]

#no need for the intercept, last parameter is sigma
post.sample <- post.sample[,-1]

#data frame with all submodels
df.rtwos <-rtwos(usair[,2:7], post.sample)


# prepare data frame for LMG values

LMG.Vals<-matrix(0, 6, dim(df.rtwos)[2])

for(i in 1:dim(df.rtwos)[2]){
  
  gofn<-df.rtwos[,i]
  
  obj.Gelman<-partition(gofn, pcan = 6, var.names = names(usair[,2:7]))
  
  LMG.Vals[,i]=obj.Gelman$IJ[,1]
}

# posterior LMG distribution of each variable
quantile(LMG.Vals[1,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[2,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[3,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[4,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[5,], c(0.025, 0.5, 0.975))
quantile(LMG.Vals[6,], c(0.025, 0.5, 0.975))

#Visualization
dat <- data.frame(t(LMG.Vals))

pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
pairs.chart

@








