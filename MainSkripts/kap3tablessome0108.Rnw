% LaTeX file for Chapter 03

<<set-parent03, echo=FALSE, cache=FALSE>>=
	set_parent('Main.Rnw')
@
	
	<<'preamble03',include=FALSE>>=
		library(knitr); library(hier.part); library(rstanarm);  library(GGally); library(ggplot2); library(relaimpo); library(rjags); library(corpcor); library(brinla); library(stargazer); library(xtable); library(kableExtra)
	
	opts_chunk$set(
		fig.path='figure/ch03_fig', 
		self.contained=FALSE,
		inlcude=FALSE,
		cache=FALSE
	) 
	
	options(digits =3)
	#function to calculate R2 data frame
	rtwos<-function(X, post.sample){
		# X: Predictor data as data frame
		# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
		
		X <- cov(X) #covariance matrix
		
		#Prepare data frame and rownames by using the combn() function
		
		lst <- list()
		
		pcan <- dim(X)[2]
		n <- (2^pcan)-1
		
		for (i in 1:pcan){
			lst[[i]] <- combn(pcan,i)
		}
		
		var.names <- character(length = 0)
		
		v<- 1:length(lst)
		
		for(i in 1:length(lst))
		{
			
			for (j in 1:ncol(lst[[i]])){
				cur<- lst[[i]][,j]
				name <- paste0('x',v[-cur])
				name <- paste(name, collapse = " ")
				var.names <- c(var.names, name)
			}
		}
		
		var.names<-c(rev(var.names), 'all')
		
		var.names[1]<-'none'
		
		size <- nrow(post.sample)  # how many samples
		
		sig.posi <- ncol(post.sample)
		
		df.Rtwos<-data.frame(matrix(0,n+1,size))
		
		row.names(df.Rtwos) <- var.names
		
		########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
		
		for (s in 1:size){
			
			sample.s <- post.sample[s,-sig.posi]
			
			Vtot <- sample.s%*%X%*%sample.s  #total explained variance
			
			count=n  
			
			for(i in 1:(length(lst)-1))
			{
				
				for (j in 1:ncol(lst[[i]])){
					cur <- lst[[i]][,j]
					set <- v[-cur]
					matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
					var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
					df.Rtwos[count,s] <- Vtot - var.explain  
					count=count-1
				}
			}
			
			df.Rtwos[n+1,s] <- Vtot
			
			df.Rtwos[,s] <- df.Rtwos[,s]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
		}
		
		return(df.Rtwos)
	}
	
	
	
	bootcov <- function(df, boot.n){
		len <- nrow(df)
		cov.m <- cov(df)
		l <- dim(cov.m)[1]	
		M.boot <- array(NA, c(l,l,boot.n))
		M.boot[,,1] <- cov(df)
		for (i in 2 :boot.n){
			dfs <- df[sample(1:len, replace=T),]
			M.boot[,,i] <- cov(dfs)
		}
		
		return(M.boot)
	}
	
	
	#Function that includes uncertainty about stochastic predictors by bootstrapping using bootcov
	#n is the number of bootstrap samples it should calculate
	# takes boot.n times longer to calculate. 
	rtwos.boot<-function(df, post.sample, boot.n){
		# X: Predictor data as data frame
		# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
		
		X <- cov(df) #covariance matrix
		
		#Prepare data frame and rownames by using the combn() function
		
		lst <- list()
		
		pcan <- dim(X)[2]
		n <- (2^pcan)-1
		
		for (i in 1:pcan){
			lst[[i]] <- combn(pcan,i)
		}
		
		var.names <- character(length = 0)
		
		v<- 1:length(lst)
		
		for(i in 1:length(lst))
		{
			
			for (j in 1:ncol(lst[[i]])){
				cur<- lst[[i]][,j]
				name <- paste0('x',v[-cur])
				name <- paste(name, collapse = " ")
				var.names <- c(var.names, name)
			}
		}
		
		var.names<-c(rev(var.names), 'all')
		
		var.names[1]<-'none'
		
		size <- nrow(post.sample)  # how many samples
		
		sig.posi <- ncol(post.sample)
		
		df.Rtwos<-array(0,c(n+1,size, boot.n))
		
		dimnames(df.Rtwos)[[1]] <- var.names
		
		boot.M <- bootcov(df, boot.n)
		
		for (b in 1:boot.n){
			
			
			X <- boot.M[,,b]
			
			########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
			
			for (s in 1:size){
				
				sample.s <- post.sample[s,-sig.posi]
				
				Vtot <- sample.s%*%X%*%sample.s  #total explained variance
				
				count=n  
				
				for(i in 1:(length(lst)-1))
				{
					
					for (j in 1:ncol(lst[[i]])){
						cur <- lst[[i]][,j]
						set <- v[-cur]
						matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
						var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
						df.Rtwos[count,s,b] <- Vtot - var.explain  
						count=count-1
					}
				}
				
				df.Rtwos[n+1,s,b] <- Vtot
				
				df.Rtwos[,s,b] <- df.Rtwos[,s,b]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
			}
			
		}
		return(df.Rtwos)
	}
	
	
	#Function to include samples of the predictor covariance matrix.
	
	rtwos.covm<-function(df, post.sample, covm, boot.n){
		# X: Predictor data as data frame
		# post.sample: posterior sample as matrix (M[sample_i,]), last position should be sigma paramater. 
		
		X <- cov(df) #covariance matrix
		
		#Prepare data frame and rownames by using the combn() function
		
		lst <- list()
		
		pcan <- dim(X)[2]
		n <- (2^pcan)-1
		
		for (i in 1:pcan){
			lst[[i]] <- combn(pcan,i)
		}
		
		var.names <- character(length = 0)
		
		v<- 1:length(lst)
		
		for(i in 1:length(lst))
		{
			
			for (j in 1:ncol(lst[[i]])){
				cur<- lst[[i]][,j]
				name <- paste0('x',v[-cur])
				name <- paste(name, collapse = " ")
				var.names <- c(var.names, name)
			}
		}
		
		var.names<-c(rev(var.names), 'all')
		
		var.names[1]<-'none'
		
		size <- nrow(post.sample)  # how many samples
		
		sig.posi <- ncol(post.sample)
		
		df.Rtwos<-array(0,c(n+1,size, boot.n))
		
		dimnames(df.Rtwos)[[1]] <- var.names
		
		for (b in 1:boot.n){
			
			
			X <- covm[,,b]
			
			########### fill in R^2 matrix, use posterior samples and calculate submodels according to the conditional expectation formula.
			
			for (s in 1:size){
				
				sample.s <- post.sample[s,-sig.posi]
				
				Vtot <- sample.s%*%X%*%sample.s  #total explained variance
				
				count=n  
				
				for(i in 1:(length(lst)-1))
				{
					
					for (j in 1:ncol(lst[[i]])){
						cur <- lst[[i]][,j]
						set <- v[-cur]
						matr <- X[cur, cur] - X[cur, set]%*%solve(X[set, set])%*%X[set, cur]  #conditional expectation formula, how much variance is explained by the predictors that are not included in the model and that is not explained by other predictors
						var.explain<- sample.s[cur]%*%matr%*%sample.s[cur]  #multiply by parameter sample
						df.Rtwos[count,s,b] <- Vtot - var.explain  
						count=count-1
					}
				}
				
				df.Rtwos[n+1,s,b] <- Vtot
				
				df.Rtwos[,s,b] <- df.Rtwos[,s,b]/c(Vtot+post.sample[s,sig.posi]^2) #total variance + sigma^2 (post sample[s])
			}
			
		}
		return(df.Rtwos)
	}
	
	@
		
		
		\chapter{Examples}
	
	In the following section the Bayesian LMG implementation is presented on two examples. The first examples simulates data, the second examples uses empirical data.
	
	\section{Simulated Data}
	
	We assume a simple model for the first example: 
		
		\begin{align*} 
	&Y_{i} \sim \mathcal{N}(\beta_{0}+x_{1} \beta_{1}+x_{2} \beta_{2}+x_{3} \beta_{3}+x_{4} \beta_{4}, \sigma^2),& \\ & \beta_{1} = 0.5, \beta_{2} = 1,  \beta_{3} = 2 , \beta_{4}=0, \sigma^2 = 1 & \\ & \X_{1}, \X_{2},\X_{3},\X_{4} \sim \mathcal{N}(0, 1) 
	\end{align*} 
	
	
	The values of the four predictors are sampled from a standard normal distribution. These values are then multiplied by the regression coefficients to obtain the dependent variable. A standard normal distributed error is added. Fifty observations were sampled.
	
	The following Code was used to simulate the data :
		
		<<'simudata.example'>>=
		
		x1 <- rnorm(50, 0, 1); x2 <- rnorm(50, 0, 1)
	x3 <- rnorm(50, 0, 1); x4 <- rnorm(50, 0, 1)
	b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0.8
	#b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1
	
	y <- b1*x1 + x2*b2 + b3*x3 + b4*x4 + rnorm(50, 0, 1) 
	
	df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)
	
	@
		
		
		The model is fitted using the \texttt{rstanarm} package \citep{rstanarm} with the default priors for the regression and $\sigma^2$ parameters.  A burn-in periode of 1000, a sample size of 20000 and a thining of 20 were chosen, resulting in a posterior sample size of 1000. The first few posterior samples are shown in Table~\ref{tab:simdata.postsample2}. 
	
	For each posterior sample of the parameters the $\Rtwo$ value is calculated. The $\Rtwo$ of the submodels is then calculated by the conditional variance formula for each posterior sample. The resulting $\Rtwo$ values of the first few posterior samples is shown in Table~\ref{tab:simdata.postsample3}.  The thinning makes sense in this case to reduce the computational burden and still obtain an appropriate posterior of the $\Rtwo$ values \citep{Link2012}. 
	
	The \texttt{hier.part} package is used to calculate the LMG value for each posterior sample. The LMG posteriors are shown in Table\ref{tbl:nonstochEx1} . The independent component (I) represents the LMG value. The joint contribution (J) represents the difference from the independent component to the explained variance of the predictor-only model (T). Assuming stochastic or nonstochastic regressors has an influence on the uncertainty. First, non-stochastic regressors are assumed. The resulting LMG values and joint contributions with a 95\% credible interval are shown in Table~\ref{tbl:nonstochEx1}. An option to display the resulting LMG distribution is shown in Figure ~ \ref{fig:simdata.LMG.plot}.  Using the default weakly informative priors, the LMG distributions obtained from the Bayesian framework are very similar to the bootstrap confidence intervals assuming non-stochastic predictors of the LMG estimates obtained from the \texttt{relaimpo} package, as shown in Table~\ref{tbl:nonstochEx1relamip}. 
	
	In the example the predictor values were sampled from a normal distribution. It would therefore be more reasonable to assume stochastic predictors. Under the assumption of weak exogeinity and conditional independence the posterior distributions of the regression paramters $\bbeta$ are valid for non-stochastic and stochastic predictors. However, the uncertainty about the LMG values needs to include the uncertainty about the covariance matrix. If we know the distribution of the predictors we can use this information and obtain the posterior distribution of the covariance matrix. The package \texttt{Jags} was used for inference abouth the covariance matrix in a Bayesian way. As an alternative nonparametric bootstrap was used for inference about the covariance matrix. Both approaches resulted in very similar LMG values. Using the default priors further resulted in very similar LMG values as the nonparametric bootstrap option of the relaimpo package. Table~\ref{tbl:nonstochEx1relamipstoch} shows the LMG values of these approaches. Using the bootstrap samples of the covariance matrix or samples from the posterior covariance matrix resulted in very similar LMG values. Bootstrap seems to be a valuable option for stochastic predictors when the distribution of the predictors is unknown. Even when the distribution is known the difference seems to be tiny. A benefit of going the full Bayesian way is that we can also include prior knowledge of the covariance matrix. For stochastic predictors the uncertainty about the covariance matrix is reflected in the larger credible intervals compared to nonstochastic predictors. Even when we would knew the exact regression parameters, there is alot of uncertainty in the LMG values caused by the uncertainty about the covariance matrix. 
	
	<<'simdata.postsample', include =FALSE>>=
		post2 <- stan_glm(y ~ 1 + x1 + x2 + x3 + x4,
											data = df,
											chains = 1, cores =1, iter=40000, thin=20)
	@
		
		<<'simdata.postsample2', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
		
		#posterior sample
		post.sample <- as.matrix(post2)
	
	dt <- data.frame(post.sample[1:10,2:6])
	rownames(dt)<-c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6', 'sample 7', 'sample 8', 'sample 9', 'sample 10')
	#example of the first 10 posterior samples
	kable(dt, format = "latex", booktabs=TRUE, linesep = c("", "", "", ""), caption = 'Samples from the posterior distributions of the regression parameters') %>%
		kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
	
	#no need for the intercept, last parameter is sigma
	post.sample <- post.sample[,-1]
	@
		
		<<'simdata.postsample3', echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE>>=
		#data frame with all submodels
		df.rtwos <-rtwos(df[,2:5], post.sample)
	
	colnames(df.rtwos)[1:6] <- c('sample 1', 'sample 2', 'sample 3', 'sample 4', 'sample 5', 'sample 6')
	kable(df.rtwos[,1:6], format = 'latex', booktabs=TRUE, linesep = c("", "", "", ""), caption = '$ \\Rtwo$ for all submodels for the first six posterior samples')  %>%
		kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
	@
		
		
		<<'simdata.LMG', cache=FALSE, include=FALSE>>=
		
		# prepare data frame for LMG values
		
		LMG.Vals.I<-matrix(0, 4, dim(df.rtwos)[2])
	
	LMG.Vals.J<-matrix(0, 4, dim(df.rtwos)[2])
	
	LMG.Vals.T<-matrix(0, 4, dim(df.rtwos)[2])
	
	
	for(i in 1:dim(df.rtwos)[2]){
		
		gofn<-df.rtwos[,i]
		
		obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
		
		LMG.Vals.I[,i]=obj.Gelman$IJ[,1]
		LMG.Vals.J[,i]=obj.Gelman$IJ[,2]
		LMG.Vals.T[,i]=obj.Gelman$IJ[,3]
	}
	
	varnames <- row.names(obj.Gelman$IJ)
	
	
	# posterior LMG distribution of each variable
	quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))
	
	quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))
	
	quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))
	
	# some example how it could be displayed
	dat <- data.frame(t(LMG.Vals.I))
	
	pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
	pairs.chart
	
	
	
	# Comparison to relaimpo package
	
	fit <- lm(y~., data=df)
	
	######## compare to relimp package
	
	run<-boot.relimp(fit, fixed=TRUE)
	
	ci <- booteval.relimp(run, bty = "perc", level = 0.95,
												sort = FALSE, norank = TRUE, nodiff = TRUE,
												typesel = c("lmg"))@mark
	
	ci <- as.matrix(ci)
	ci <- matrix(as.numeric(ci), 4 , 3)
	colnames(ci) <- c('LMG', 'l.b', 'u.b')
	rownames(ci) <- c('X1', 'X2', 'X3', 'X4')
	
	#Code assuming stochastic predictors
	run.stochastic<-boot.relimp(fit, fixed=FALSE)
	
	ci.s <- booteval.relimp(run.stochastic, bty = "perc", level = 0.95,
													sort = FALSE, norank = TRUE, nodiff = TRUE,
													typesel = c("lmg"))@mark
	
	ci.s <- as.matrix(ci.s)
	ci.s <- matrix(as.numeric(ci.s), 4 , 3)
	colnames(ci.s) <- c('LMG', 'l.b', 'u.b')
	rownames(ci.s) <- c('X1', 'X2', 'X3', 'X4')
	
	
	@
		
		
		<<'simdata.LMG.plot', cache=TRUE, echo=FALSE, fig.cap='Posterior distribution of LMG values.'>>=
		pairs.chart
	@
		
		\begin{table}[h]
	\centering
	\begin{tabular}{clll}
	\hline
	\multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
	\hline
	\Sexpr{varnames[1]} & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[1,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[2]} & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[2,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[2,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[3]} & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[3,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[4]} & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J[4,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
	
	\hline
	\end{tabular}
	\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
	\label{tbl:nonstochEx1}
	\end{table}
	
	
	
	
	\begin{table}[h]
	\centering
	\begin{tabular}{cll}
	\toprule
	&\multicolumn{2}{c}{\textbf{LMG value (95\%-CI)}} \\
	\textbf{Variable} & \multicolumn{1}{c}{maximum likelihood} & \textbf{Bayesian framwork}  \\
	\hline
	\Sexpr{varnames[1]} & \Sexpr{ci[1,1]} (\Sexpr{ci[1,2]}, \Sexpr{ci[1,3]})  & \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[2]} & \Sexpr{ci[2,1]} (\Sexpr{ci[2,2]}, \Sexpr{ci[2,3]})  & \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))[3]})   \\ 
	\Sexpr{varnames[3]} & \Sexpr{ci[3,1]} (\Sexpr{ci[3,2]}, \Sexpr{ci[3,3]})  & \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[4]} & \Sexpr{ci[4,1]} (\Sexpr{ci[4,2]}, \Sexpr{ci[4,3]}) & \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\bottomrule
	\end{tabular}
	\caption{Variance decomposition for non-stochastic predictors. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
	\label{tbl:nonstochEx1relamip}
	\end{table}
	
	
	
	
	<<'simdata.LMG.boot', include=FALSE, cache =TRUE>>=
		
		
		
		#----------------------------------------------------------------------------------------
	
	
	df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)
	
	n.boot = 10
	
	LMG.Vals.I.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
	LMG.Vals.J.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
	LMG.Vals.T.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
	
	for (b in 1:n.boot){
		
		for(i in 1:dim(df.rtwos.boot)[2]){
			
			gofn<-df.rtwos.boot[,i,b]
			
			obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
			
			LMG.Vals.I.boot[,i, b]=obj.Gelman$IJ[,1]
			LMG.Vals.J.boot[,i, b]=obj.Gelman$IJ[,2]
			LMG.Vals.T.boot[,i, b]=obj.Gelman$IJ[,3]
			
		}
		
	}
	
	quantile(c(LMG.Vals.I.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot[4,1:1000,]), c(0.025, 0.5, 0.975))
	
	#very similar values as in the confidence intervals for stochastic predictors
	
	
	#what if we have prior knowledge
	my_prior <- normal(location = c(1, 1,1,1), scale = c(0.01, 0.01,0.01,0.01), autoscale = FALSE)
	
	post2 <- stan_glm(y ~ 1 + x1 + x2 + x3 + x4,
										data = df, prior = my_prior,
										chains = 2, cores = 1)
	
	#posterior sample
	post.sample <- as.matrix(post2)
	
	#example of the first 10 posterior samples
	post.sample[1:10,]
	
	#no need for the intercept, last parameter is sigma
	post.sample <- post.sample[,-1]
	
	
	#data frame with all submodels
	
	
	df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)
	
	n.boot = 10
	
	LMG.Vals.I.boot.p<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
	
	for (b in 1:n.boot){
		
		for(i in 1:dim(df.rtwos.boot)[2]){
			
			gofn<-df.rtwos.boot[,i,b]
			
			obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
			
			LMG.Vals.I.boot.p[,i, b]=obj.Gelman$IJ[,1]
		}
		
	}
	
	quantile(c(LMG.Vals.I.boot.p[1,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot.p[2,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot.p[3,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot.p[4,1:1000,]), c(0.025, 0.5, 0.975))
	
	
	@
		
		
		
		\begin{table}[h]
	\caption{Variance decomposition. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
	\centering
	\begin{tabular}{clll}
	\hline
	\multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
	\hline
	\Sexpr{varnames[1]} & \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J.boot[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J.boot[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J.boot[1,1:1000,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T.boot[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T.boot[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T.boot[1,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[2]} & \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J.boot[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J.boot[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J.boot[2,1:1000,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T.boot[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T.boot[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T.boot[2,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[3]} & \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J.boot[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J.boot[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J.boot[3,1:1000,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T.boot[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T.boot[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T.boot[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[4]} & \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J.boot[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J.boot[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J.boot[4,1:1000,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T.boot[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T.boot[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T.boot[4,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\hline
	\end{tabular}
	\label{tbl:fundus.to.SLO2}
	\end{table}
	
	
	
	
	
	
	<<'simdata.LMG.cov', include=FALSE, cache=TRUE>>=
		
		zy = df[,2:5]
	
	#----------------------------------------------------------------------------
	# In the following example we know that the $\X$ are coming from a normal distribution. The covariance matrix of the distribution is estimated in aBayesian way. The package Jags is used.
	#----------------------------------------------------------------------------
	
	# Load some functions used below:
	# Install the ellipse package if not already:
	# Standardize the data:
	
	#zy = apply(y,2,function(yVec){(yVec-mean(yVec))/sd(yVec)})
	# Assemble data for sending to JAGS:
	dataList = list(
		zy = zy ,
		Ntotal =  nrow(zy) ,
		Nvar = ncol(zy) ,
		# Include original data info for transforming to original scale:
		# For wishart (dwish) prior on inverse covariance matrix:
		zRscal = ncol(zy) ,  # for dwish prior
		zRmat = diag(x=1,nrow=ncol(zy))  # Rmat = diag(apply(y,2,var))
	)
	
	# Define the model:
	modelString = "
	model {
	for ( i in 1:Ntotal ) {
	zy[i,1:Nvar] ~ dmnorm( zMu[1:Nvar] , zInvCovMat[1:Nvar,1:Nvar] ) 
	}
	for ( varIdx in 1:Nvar ) { zMu[varIdx] ~ dnorm( 0 , 1/2^2 ) }
	zInvCovMat ~ dwish( zRmat[1:Nvar,1:Nvar] , zRscal )
	# Convert invCovMat to sd and correlation:
	zCovMat <- inverse( zInvCovMat )
	
	}
	" # close quote for modelString
	writeLines( modelString , con="Jags-MultivariateNormal-model.txt" )
	
	# Run the chains:
	nChain = 3
	nAdapt = 500
	nBurnIn = 500
	nThin = 10
	nStepToSave = 20000
	require(rjags)
	jagsModel = jags.model( file="Jags-MultivariateNormal-model.txt" , 
													data=dataList , n.chains=nChain , n.adapt=nAdapt )
	update( jagsModel , n.iter=nBurnIn )
	codaSamples = coda.samples( jagsModel , 
															variable.names=c('zCovMat') ,
															n.iter=nStepToSave/nChain*nThin , thin=nThin )
	
	
	# Convergence diagnostics:
	parameterNames = varnames(codaSamples) # get all parameter names
	
	
	# Examine the posterior distribution:
	mcmcMat = as.matrix(codaSamples)
	chainLength = nrow(mcmcMat)
	
	covMat <- array(NA, c(4,4,chainLength))
	
	for (i in 1:chainLength){
		covMat[1:4,1:4,i]<-matrix(mcmcMat[i,], 4, 4)
	}
	
	covMat <- covMat[1:4,1:4,sample(1:20000, replace=F)] 
	
	
	df.rtwos <-rtwos.covm(df[,2:5], post.sample, covMat, 10)
	
	
	n.boot = 10
	
	LMG.Vals.I.covm<-array(0, c(4,dim(df.rtwos)[2], n.boot))
	LMG.Vals.J.covm<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
	LMG.Vals.T.covm<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
	
	
	for (b in 1:n.boot){
		
		for(i in 1:dim(df.rtwos)[2]){
			
			gofn<-df.rtwos[,i,b]
			
			obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
			
			LMG.Vals.I.covm[,i, b]=obj.Gelman$IJ[,1]
			LMG.Vals.J.covm[,i, b]=obj.Gelman$IJ[,2]
			LMG.Vals.T.covm[,i, b]=obj.Gelman$IJ[,3]
			
		}
		
	}
	
	quantile(c(LMG.Vals.I.covm[1,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.covm[2,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.covm[3,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.covm[4,1:1000,]), c(0.025, 0.5, 0.975))
	
	
	
	
	
	@
		
		
		
		\begin{table}[h]
	\caption{LMG values assuming stochastic predictors with 95\% CI.}
	\centering
	\begin{tabular}{clll}
	\toprule
	& \multicolumn{1}{c}{\textbf{Relaimpo}} & \multicolumn{2}{c}{\textbf{Bayesian framework}} \\ \cmidrule(r){2-2}\cmidrule(l){3-4}
	\textbf{Variable} &  & \multicolumn{1}{c}{nonparameteric bootstrap}& \multicolumn{1}{c}{covariance inference} \\
	\midrule
	\Sexpr{varnames[1]} & \Sexpr{ci.s[1,1]} (\Sexpr{ci.s[1,2]}, \Sexpr{ci.s[1,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[1,1:1000,], c(0.025, 0.5, 0.975))[3]}) &  \Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
	\Sexpr{varnames[2]} & \Sexpr{ci.s[2,1]} (\Sexpr{ci.s[2,2]}, \Sexpr{ci.s[2,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[2,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
	\Sexpr{varnames[3]} & \Sexpr{ci.s[3,1]} (\Sexpr{ci.s[3,2]}, \Sexpr{ci.s[3,3]})  & \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
	\Sexpr{varnames[4]} & \Sexpr{ci.s[4,1]} (\Sexpr{ci.s[4,2]}, \Sexpr{ci.s[4,3]}) & \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.boot[4,1:1000,], c(0.025, 0.5, 0.975))[3]}) & \Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[3]}) \\ 
	\bottomrule
	\end{tabular}
	\label{tbl:nonstochEx1relamipstoch}
	\end{table}
	
	
	
	
	\begin{table}[h]
	\centering
	\begin{tabular}{clll}
	\hline
	\multicolumn{1}{c}{\textbf{Variable}} & \multicolumn{1}{c}{\textbf{I}} &\multicolumn{1}{c}{\textbf{J}} & \multicolumn{1}{c}{\textbf{Total}} \\
	\hline
	\Sexpr{varnames[1]} & \Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[1,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J.covm[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J.covm[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J.covm[1,1:1000,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T.covm[1,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T.covm[1,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T.covm[1,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[2]} & \Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[2,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J.covm[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J.covm[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J.covm[2,1:1000,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T.covm[2,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T.covm[2,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T.covm[2,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[3]} & \Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J.covm[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J.covm[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J.covm[3,1:1000,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T.covm[3,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T.covm[3,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T.covm[3,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
	\Sexpr{varnames[4]} & \Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.I.covm[4,1:1000,], c(0.025, 0.5, 0.975))[3]})  & \Sexpr{quantile(LMG.Vals.J.covm[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.J.covm[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.J.covm[4,1:1000,], c(0.025, 0.5, 0.975))[3]})   & \Sexpr{quantile(LMG.Vals.T.covm[4,1:1000,], c(0.025, 0.5, 0.975))[2]} (\Sexpr{quantile(LMG.Vals.T.covm[4,1:1000,], c(0.025, 0.5, 0.975))[1]}, \Sexpr{quantile(LMG.Vals.T.covm[4,1:1000,], c(0.025, 0.5, 0.975))[3]})  \\ 
	
	\hline
	\end{tabular}
	\caption{Variance decomposition. I = LMG values, J = joint contribution, Total = total explained variance in one-predictor only model}
	\label{tbl:stochex1covm}
	\end{table}
	
	Code xx shows the uncertainty about the LMG values caused by the uncertainty about the covariance matrix. 
	
	Another interesting option in some cases would be to use the shrinkage estimate of the covariance matrix. 
	
	
	<<'covuncertainty', include=FALSE, cache=FALSE>>=
		
		#How much variance is effectively in the bootstrap matrix when we know the regression parameters.
		
		#fake post sample
		
		x1 <- rnorm(50, 0, 1); x2 <- rnorm(50, 0, 1)
	x3 <- rnorm(50, 0, 1); x4 <- rnorm(50, 0, 1)
	#b1 <- 0.5; b2 <- 1; b3 <- 2; b4 <- 0
	b1 <- 1; b2 <- 1; b3 <- 1; b4 <- 1
	
	y <- b1*x1 + x2*b2 + b3*x3 + b4*x4 + rnorm(50, 0, 1) 
	
	df <- data.frame(y = y, x1 = x1, x2 = x2, x3 = x3, x4 = x4)
	
	post.sample <- matrix(1, 1000,5)
	
	df.rtwos.boot <-rtwos.boot(df[,2:5], post.sample, 10)
	
	n.boot = 10
	
	LMG.Vals.I.boot<-array(0, c(4,dim(df.rtwos.boot)[2], n.boot))
	
	for (b in 1:n.boot){
		
		for(i in 1:dim(df.rtwos.boot)[2]){
			
			gofn<-df.rtwos.boot[,i,b]
			
			obj.Gelman<-partition(gofn, pcan = 4, var.names = names(df[,2:5]))
			
			LMG.Vals.I.boot[,i, b]=obj.Gelman$IJ[,1]
		}
		
	}
	
	quantile(c(LMG.Vals.I.boot[1,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot[2,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot[3,1:1000,]), c(0.025, 0.5, 0.975))
	quantile(c(LMG.Vals.I.boot[4,1:1000,]), c(0.025, 0.5, 0.975))
	
	
	
	cov(df[,2:5])
	@
		
		<<'simdata.LMG.shrink', include=FALSE, cache =TRUE>>=
		
		#Comparison of sample covariance and shrink covariance matrix
		
		cov(df[,2:5])
	cov.shrink(df[,2:5])
	
	@
		
		\section{Empirical Data}
	
	
	The following example data are taken from the book Bayesian Regression Modeling with INLA. The data were about air pollution in 41 cities in the United States originally published in Everitt (2006). The data consits of the SO2 level as the dependent variable and six explanatory variables 
	Two of the explanatory variables are are related to human ecology (pop, manuf) and four others are related to climate (negtemp, wind, precip, days).
	
	
	\begin{table}
	\caption{Variable description of air pollution data}
	\begin{tabularx}{\textwidth}{|l|X|l|}
	\hline			
	Variable Name & Description & Codes/Values \\   \hline  
	SO2 & sulfur dioxide content of air & microgrames per cubic meter \\
	negtemp & negative value of average & fahrenheit\\
	manuf & number of manufacturing enterprises employing 20 or more workers & integers \\
	pop & population size in thousands (1970 census) & numbers \\
	wind & average wind speed & miles per hour \\
	precip & average annual percipitation & inches \\
	days & average munber of days with precipitation per year & integers \\
	\hline  
	\end{tabularx}
	\label{table:airpollutiondata}
	\end{table}
	
	<<results='asis', echo = FALSE>>=
		data('usair', package='brinla')
	pairs.chart <- ggpairs(usair[,], lower = list(continuous = "cor"), upper = list(continuous = "points", combo = "dot")) + ggplot2::theme(axis.text = element_text(size = 6)) 
	pairs.chart
	
	
	
	@
		
		A simple linear regression model including the dependent variable SO2 and the six explanatory variables is fitted with the lm command in R. The $R^2$ of the full model is 0.670. 
	
	<<'real.data.LMG',cache =TRUE>>=
		
		usair.lm <- lm(SO2~., data = usair)
	
	
	#Code assuming stochastic predictors
	run<-boot.relimp(usair.lm, fixed=TRUE)
	
	booteval.relimp(run, bty = "perc", level = 0.95,
									sort = FALSE, norank = TRUE, nodiff = TRUE,
									typesel = c("lmg"))
	
	
	bayes.usair <- stan_glm(SO2 ~ . ,
													data = usair,
													chains = 4, cores = 4)
	
	#posterior sample
	post.sample <- as.matrix(bayes.usair)
	
	#example of the first 10 posterior samples
	post.sample[1:10,]
	
	#no need for the intercept, last parameter is sigma
	post.sample <- post.sample[,-1]
	
	#data frame with all submodels
	df.rtwos <-rtwos(usair[,2:7], post.sample)
	
	df.rtwos[,1:3]
	
	
	# prepare data frame for LMG values
	
	LMG.Vals.I<-matrix(0, 6, dim(df.rtwos)[2])
	
	for(i in 1:dim(df.rtwos)[2]){
		
		gofn<-df.rtwos[,i]
		
		obj.Gelman<-partition(gofn, pcan = 6, var.names = names(usair[,2:7]))
		
		LMG.Vals.I[,i]=obj.Gelman$IJ[,1]
	}
	
	# posterior LMG distribution of each variable
	quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[5,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[6,], c(0.025, 0.5, 0.975))
	
	
	#Visualization
	dat <- data.frame(t(LMG.Vals.I))
	
	pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
	pairs.chart
	
	#use prior knowledge
	my_prior <- normal(location = c(1, 0.05,-0.03,-3, 0, 0), scale = c(0.1, 0.01,0.01,0.1, 0.1, 0.1), autoscale = FALSE)
	
	
	bayes.usair <- stan_glm(SO2 ~ . ,
													data = usair, prior = my_prior,
													chains = 4, cores = 4)
	
	#posterior sample
	post.sample <- as.matrix(bayes.usair)
	
	#example of the first 10 posterior samples
	post.sample[1:10,]
	
	#no need for the intercept, last parameter is sigma
	post.sample <- post.sample[,-1]
	
	#data frame with all submodels
	df.rtwos <-rtwos(usair[,2:7], post.sample)
	
	
	# prepare data frame for LMG values
	
	LMG.Vals.I<-matrix(0, 6, dim(df.rtwos)[2])
	
	for(i in 1:dim(df.rtwos)[2]){
		
		gofn<-df.rtwos[,i]
		
		obj.Gelman<-partition(gofn, pcan = 6, var.names = names(usair[,2:7]))
		
		LMG.Vals.I[,i]=obj.Gelman$IJ[,1]
	}
	
	# posterior LMG distribution of each variable
	quantile(LMG.Vals.I[1,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[2,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[3,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[4,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[5,], c(0.025, 0.5, 0.975))
	quantile(LMG.Vals.I[6,], c(0.025, 0.5, 0.975))
	
	#Visualization
	dat <- data.frame(t(LMG.Vals.I))
	
	pairs.chart <- ggpairs(dat, lower = list(list(combo = "facetdensity")), upper = list(list(continuous = "cor"))) + ggplot2::theme(axis.text = element_text(size = 6)) 
	pairs.chart
	
	@
		
		
		
		
		
		
		
		
		